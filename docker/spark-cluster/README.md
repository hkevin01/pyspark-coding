# ğŸ³ Spark Cluster Simulation with Docker

Simulate a real Spark cluster locally with 1 Master + 3 Workers!

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK CLUSTER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              SPARK MASTER (Driver)                      â”‚   â”‚
â”‚   â”‚              spark-master:7077                          â”‚   â”‚
â”‚   â”‚              Web UI: localhost:8080                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                    â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚            â”‚               â”‚               â”‚                    â”‚
â”‚            â–¼               â–¼               â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚  WORKER 1   â”‚  â”‚  WORKER 2   â”‚  â”‚  WORKER 3   â”‚            â”‚
â”‚   â”‚  2 cores    â”‚  â”‚  2 cores    â”‚  â”‚  2 cores    â”‚            â”‚
â”‚   â”‚  2GB RAM    â”‚  â”‚  2GB RAM    â”‚  â”‚  2GB RAM    â”‚            â”‚
â”‚   â”‚  :8081      â”‚  â”‚  :8082      â”‚  â”‚  :8083      â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Start the Cluster
```bash
docker-compose up -d
```

### 2. Check Cluster Status
```bash
# View running containers
docker-compose ps

# View logs
docker-compose logs -f spark-master
```

### 3. Open Spark Web UI
- **Master UI**: http://localhost:8080
- **Worker 1**: http://localhost:8081
- **Worker 2**: http://localhost:8082
- **Worker 3**: http://localhost:8083

### 4. Submit a Job
```bash
docker exec spark-master spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/example_job.py
```

### 5. Interactive PySpark Shell
```bash
docker exec -it spark-master pyspark \
    --master spark://spark-master:7077
```

### 6. Stop the Cluster
```bash
docker-compose down
```

## ğŸ“ Directory Structure

```
spark-cluster/
â”œâ”€â”€ docker-compose.yml     # Cluster configuration
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ apps/                  # Your Spark applications
â”‚   â””â”€â”€ example_job.py     # Sample distributed job
â””â”€â”€ data/                  # Shared data directory
```

## ğŸ”§ Configuration

### Adjust Worker Resources
Edit `docker-compose.yml`:
```yaml
environment:
  - SPARK_WORKER_MEMORY=4G    # Increase memory
  - SPARK_WORKER_CORES=4      # Increase cores
```

### Add More Workers
Copy a worker block and change:
- `container_name`
- `hostname`
- `ports` (use unique port like 8084:8081)

### Scale Workers Dynamically
```bash
docker-compose up -d --scale spark-worker-1=1 --scale spark-worker-2=1 --scale spark-worker-3=3
```

## ğŸ’¡ What This Demonstrates

1. **Driver Node**: `spark-master` container runs the driver program
2. **Worker Nodes**: `spark-worker-1/2/3` execute tasks in parallel
3. **Cluster Manager**: Spark Standalone scheduler allocates resources
4. **Task Distribution**: Work is split across partitions on different workers
5. **Communication**: Containers communicate over `spark-network`

## ğŸ“Š Monitoring

### View Registered Workers
Open http://localhost:8080 â†’ Shows all workers with:
- Cores available
- Memory allocated
- Running applications

### View Job Execution
After submitting a job:
1. Go to http://localhost:8080
2. Click on your application
3. View stages, tasks, and which executor ran each task

### View Task Distribution
In the Spark UI:
- **Stages** tab shows task distribution
- **Executors** tab shows per-worker metrics
- Each task shows which worker executed it

## ğŸ§ª Example Output

When you run `example_job.py`, you'll see:
```
ğŸ–¥ï¸  WORKER NODE EXECUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Partition Distribution Across Workers:
  spark-worker-1: 4 partitions, 333,333 records
  spark-worker-2: 4 partitions, 333,333 records
  spark-worker-3: 4 partitions, 333,334 records
```

This proves the work is distributed across all three workers!

## ğŸ”— Relates To

See `src/cluster_computing/quick_info.md` for conceptual overview of:
- Driver vs Worker nodes
- How jobs are scheduled
- How code is distributed
