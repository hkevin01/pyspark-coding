# ðŸŒ Spark UI Live Demo Guide

## ðŸŽ¯ Quick Start

### Step 1: Open Browser First
```bash
# Open these URLs in separate browser tabs:
firefox http://localhost:4040 &  # Application UI (will show "not found" until job starts)
firefox http://localhost:9080 &  # Master UI (always available)
```

### Step 2: Run the Long Demo Job
```bash
cd /home/kevin/Projects/pyspark-coding/docker/spark-cluster

docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/long_running_demo.py
```

### Step 3: Watch the UI Update in Real-Time! ðŸŽ¬

## ðŸ“Š What You'll See (60+ second job)

### Timeline

**0-5 seconds**: Initial setup
- Application UI becomes active at http://localhost:4040
- See "UI Demo - Long Running Job" in Jobs tab

**5-15 seconds**: Stage 1 - Data Creation
- 10 million records across 24 partitions
- Watch task timeline in Stages tab
- See data cached in Storage tab

**15-25 seconds**: Stage 2 - Aggregations
- GroupBy operations across 100 categories
- Monitor shuffle metrics in Executors tab

**25-35 seconds**: Stage 3 - Window Functions
- Complex window operations
- Check SQL tab for physical plan

**35-45 seconds**: Stage 4 - Joins
- Join operations with shuffle
- Executors tab shows shuffle read/write

**45-55 seconds**: Stages 5-7 - Filters, Sorts, Stats
- Multiple transformation stages
- See task distribution across workers

**55-70 seconds**: Stage 8 - Final Aggregation
- Complex multi-operation aggregation
- UI stays active for exploration

## ðŸ” What to Look For in Each Tab

### 1. Jobs Tab
âœ“ 8+ jobs listed (one per major operation)
âœ“ Click any job to see its stages
âœ“ View DAG visualization
âœ“ Check job duration and status

### 2. Stages Tab
âœ“ 24 tasks per stage (one per partition)
âœ“ Task timeline showing parallel execution
âœ“ Color-coded task status (running/completed)
âœ“ Input/Output/Shuffle metrics
âœ“ Locality: PROCESS_LOCAL is best

### 3. Storage Tab
âœ“ Cached DataFrame visible
âœ“ Memory usage: ~XX MB
âœ“ 24 partitions cached
âœ“ Fraction cached: 100%

### 4. Environment Tab
âœ“ Spark properties
âœ“ System properties
âœ“ Classpath entries
âœ“ Verify configurations

### 5. Executors Tab
âœ“ 3 executors + 1 driver
âœ“ Each executor: 2 cores, 2GB RAM
âœ“ Task time distribution
âœ“ Shuffle read/write metrics
âœ“ GC time monitoring
âœ“ Active/Complete/Failed tasks

### 6. SQL Tab
âœ“ DataFrame operations listed
âœ“ Physical plan visualization
âœ“ Click "Details" to expand
âœ“ See Catalyst optimizer work

## ðŸŽ¨ Color Coding

- **Blue**: Running tasks
- **Green**: Completed successfully
- **Red**: Failed tasks
- **Orange**: Skipped tasks
- **Grey**: Pending tasks

## ðŸ“ˆ Key Metrics to Monitor

### Per-Stage Metrics
- Duration
- Input/Output size
- Shuffle read/write
- Records processed
- Task execution time

### Per-Executor Metrics
- Total tasks run
- Active/Failed/Completed tasks
- Storage memory used
- Shuffle read/write
- GC time

## ðŸš€ Alternative: Interactive Shell (UI Stays Open Indefinitely)

```bash
# Start PySpark shell
docker exec -it spark-master /opt/spark/bin/pyspark \
    --master spark://spark-master:7077

# Then run commands interactively:
>>> df = spark.range(10_000_000, numPartitions=24)
>>> df.count()
>>> df.groupBy((df.id % 100).alias("cat")).count().show()

# UI at http://localhost:4040 stays active until you exit
# Type exit() to quit
```

## ðŸ’¡ Pro Tips

1. **Keep Jobs Tab Open**: Refresh to see new jobs as they complete
2. **Stage Detail View**: Click stage ID to see task-level details
3. **Timeline View**: Shows exact task execution on each executor
4. **Event Timeline**: See when tasks started/finished on each worker
5. **Executor Threads**: Watch real-time thread activity
6. **Failed Tasks**: Click to see error details and stack traces

## ðŸ“Š Comparison: Quick vs Long Demo

| Feature | example_job.py | long_running_demo.py |
|---------|----------------|----------------------|
| Duration | <1 second | ~60 seconds |
| Records | 1 million | 10 million |
| Partitions | 12 | 24 |
| Stages | 3 | 8+ |
| Operations | Basic | Complex (joins, windows, etc) |
| UI Time | Brief | Extended |
| Best For | Quick test | UI exploration |

## ðŸ”„ Running Multiple Times

```bash
# Run 3 times in a row to see history
for i in {1..3}; do
    echo "Run $i of 3"
    docker exec spark-master /opt/spark/bin/spark-submit \
        --master spark://spark-master:7077 \
        /opt/spark-apps/long_running_demo.py
    sleep 2
done

# Then check Master UI at http://localhost:9080
# See "Completed Applications" section with all runs
```

## ðŸ“¸ Screenshot Checklist

While the job runs, capture:
- [ ] Jobs tab showing multiple jobs
- [ ] Stages tab with task timeline
- [ ] Storage tab with cached data
- [ ] Executors tab showing 3 workers
- [ ] SQL tab with query plans
- [ ] Stage detail page with tasks
- [ ] DAG visualization
- [ ] Event timeline

## ðŸŽ“ Learning Exercise

1. Run the long demo
2. Watch Stages tab during execution
3. Note which stages take longest
4. Check why (look at shuffle metrics)
5. Compare executor workloads
6. Identify bottlenecks
7. Think about optimizations

## âš ï¸ Troubleshooting

**UI shows "not found"**
â†’ Job not running yet, refresh when terminal shows "Open Spark UI now"

**UI disappeared**
â†’ Job completed, check Master UI for history

**Slow performance**
â†’ Normal for first run (JVM warmup), subsequent runs faster

**Port already in use**
â†’ Another job running, wait for completion or use different port

---

**Happy UI Exploring! ðŸŽ‰**
