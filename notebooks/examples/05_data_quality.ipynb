{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Data Quality Checks\n",
    "\n",
    "Perform data quality checks and cleaning operations.\n",
    "\n",
    "This demonstrates:\n",
    "- Detecting nulls and duplicates\n",
    "- Data validation\n",
    "- Data profiling\n",
    "- Cleaning operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataQuality\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", 28, 75000),\n",
    "    (2, \"Jane Smith\", None, 35, 85000),\n",
    "    (3, \"Bob Johnson\", \"bob@email.com\", None, 65000),\n",
    "    (1, \"John Doe\", \"john@email.com\", 28, 75000),  # Duplicate\n",
    "    (4, \"Alice Brown\", \"alice@invalid\", -5, 95000),  # Invalid age and email\n",
    "    (5, None, \"charlie@email.com\", 42, None),  # Missing name and salary\n",
    "    (6, \"  Emma Davis  \", \"emma@email.com\", 30, 70000),  # Extra spaces\n",
    "    (7, \"Michael Wilson\", \"MICHAEL@EMAIL.COM\", 45, 80000),  # Uppercase email\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"email\", \"age\", \"salary\"])\n",
    "\n",
    "print(\"Original Messy Data:\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data dimensions\n",
    "print(f\"\\nTotal rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull Value Counts:\")\n",
    "null_counts = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null percentages\n",
    "total_rows = df.count()\n",
    "print(\"\\nNull Percentages:\")\n",
    "for col_name in df.columns:\n",
    "    null_count = df.filter(F.col(col_name).isNull()).count()\n",
    "    null_pct = (null_count / total_rows) * 100\n",
    "    print(f\"{col_name}: {null_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "print(f\"\\nDuplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Find duplicate records\n",
    "duplicates = df.groupBy(df.columns).count().filter(F.col(\"count\") > 1)\n",
    "print(\"\\nDuplicate records:\")\n",
    "duplicates.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data profiling - unique values per column\n",
    "print(\"\\nUnique Value Counts:\")\n",
    "for col_name in df.columns:\n",
    "    unique_count = df.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning - Step by step\n",
    "\n",
    "# 1. Remove exact duplicates\n",
    "df_cleaned = df.dropDuplicates()\n",
    "print(f\"\\nAfter removing duplicates: {df_cleaned.count()} rows\")\n",
    "\n",
    "# 2. Trim whitespace from name\n",
    "df_cleaned = df_cleaned.withColumn(\"name\", F.trim(F.col(\"name\")))\n",
    "\n",
    "# 3. Standardize email to lowercase\n",
    "df_cleaned = df_cleaned.withColumn(\"email\", F.lower(F.col(\"email\")))\n",
    "\n",
    "# 4. Validate email format (simple check)\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"valid_email\",\n",
    "    F.when(F.col(\"email\").rlike(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"), True)\n",
    "     .otherwise(False)\n",
    ")\n",
    "\n",
    "# 5. Validate age (must be between 18 and 100)\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"valid_age\",\n",
    "    F.when((F.col(\"age\") >= 18) & (F.col(\"age\") <= 100), True)\n",
    "     .otherwise(False)\n",
    ")\n",
    "\n",
    "print(\"\\nCleaned data with validation flags:\")\n",
    "df_cleaned.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only valid records\n",
    "df_valid = df_cleaned.filter(\n",
    "    (F.col(\"name\").isNotNull()) &\n",
    "    (F.col(\"email\").isNotNull()) &\n",
    "    (F.col(\"valid_email\") == True) &\n",
    "    (F.col(\"age\").isNotNull()) &\n",
    "    (F.col(\"valid_age\") == True) &\n",
    "    (F.col(\"salary\").isNotNull())\n",
    ").drop(\"valid_email\", \"valid_age\")\n",
    "\n",
    "print(\"\\nFinal valid records:\")\n",
    "df_valid.show(truncate=False)\n",
    "print(f\"Valid records: {df_valid.count()} out of {df.count()} original rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, fill missing values instead of dropping\n",
    "df_filled = df_cleaned.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"email\": \"no-email@example.com\",\n",
    "    \"age\": 0,\n",
    "    \"salary\": 0\n",
    "})\n",
    "\n",
    "print(\"\\nData with filled null values:\")\n",
    "df_filled.drop(\"valid_email\", \"valid_age\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
