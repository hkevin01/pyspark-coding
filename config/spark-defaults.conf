#
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                                                                              â•‘
# â•‘                    SPARK DEFAULTS CONFIGURATION FILE                         â•‘
# â•‘                    spark-defaults.conf                                       â•‘
# â•‘                                                                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# ğŸ“‹ PURPOSE:
#    Default configuration for all PySpark applications in this project
#    Optimized for local development with support for:
#    â€¢ Hive integration
#    â€¢ Kafka streaming
#    â€¢ Delta Lake
#    â€¢ Parquet/ORC optimization
#    â€¢ Memory management
#
# ğŸ¯ USAGE:
#    Option 1 (Automatic): Place in $SPARK_HOME/conf/spark-defaults.conf
#    Option 2 (Manual): spark-submit --properties-file spark-defaults.conf
#    Option 3 (Code): spark.config("spark.key", "value")
#
# ğŸ“– READING GUIDE:
#    Each section is clearly marked with visual separators
#    Properties are grouped by function
#    Every setting includes explanation and recommended values
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 1: APPLICATION BASICS                                               â•‘
# â•‘  Core settings for Spark application identity and behavior                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Default application name (override in code for specific apps)
spark.app.name                      PySpark-Practice-App

# Master URL (local mode for development)
# local[*] = use all available cores
# local[4] = use 4 cores
# spark://host:7077 = standalone cluster
spark.master                        local[*]

# Submit deploy mode
# client = driver runs on submit machine (development)
# cluster = driver runs on cluster (production)
spark.submit.deployMode             client


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 2: MEMORY CONFIGURATION                                             â•‘
# â•‘  Optimize memory allocation for executors and driver                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Driver memory (machine running your code)
# Development: 2g-4g | Production: 4g-8g
spark.driver.memory                 4g

# Executor memory (workers processing data)
# Development: 2g-4g | Production: 4g-16g
spark.executor.memory               4g

# Memory overhead for non-heap memory
# Should be ~10% of executor memory
spark.executor.memoryOverhead       512m
spark.driver.memoryOverhead         512m

# Memory fraction for execution (joins, aggregations)
# Default: 0.6 (60% of heap)
spark.memory.fraction               0.6

# Memory fraction for storage (caching, broadcasting)
# Default: 0.5 (50% of execution memory)
spark.memory.storageFraction        0.5


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 3: EXECUTOR CONFIGURATION                                           â•‘
# â•‘  Control parallelism and resource allocation                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Number of executor instances
# Local mode: automatically determined
# Cluster: set explicitly (e.g., 10 executors)
# spark.executor.instances          2

# CPU cores per executor
# Development: 2-4 | Production: 4-8
spark.executor.cores                2

# Default parallelism (number of partitions for RDDs)
# Rule of thumb: 2-4x number of CPU cores
spark.default.parallelism           8

# SQL shuffle partitions (for DataFrame operations)
# Development: 20-200 | Production: 200-2000
# Adjust based on data size
spark.sql.shuffle.partitions        200


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 4: HIVE INTEGRATION                                                 â•‘
# â•‘  Enable Hive metastore and SQL compatibility                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable Hive support
spark.sql.catalogImplementation     hive

# Hive metastore warehouse directory
spark.sql.warehouse.dir             /tmp/spark-warehouse

# Path to hive-site.xml (for metastore connection)
spark.sql.hive.metastore.version    3.1.2
spark.sql.hive.metastore.jars       builtin

# Hive dynamic partition mode
spark.sql.sources.partitionOverwriteMode    dynamic

# Enable Hive statistics
spark.sql.statistics.fallBackToHdfs false


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 5: KAFKA STREAMING                                                  â•‘
# â•‘  Configuration for real-time streaming with Kafka                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Kafka packages (automatically download)
# Version: 3.5.0 for Spark 3.5.x, Scala 2.12
spark.jars.packages                 org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\
                                    io.delta:delta-core_2.12:2.4.0,\
                                    org.apache.spark:spark-avro_2.12:3.5.0

# Kafka consumer settings
# spark.kafka.consumer.cache.enabled    true
# spark.kafka.consumer.cache.timeout    300s


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 6: FILE FORMATS & COMPRESSION                                       â•‘
# â•‘  Optimize reading/writing of Parquet, ORC, CSV, JSON                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Parquet settings
spark.sql.parquet.compression.codec     snappy
spark.sql.parquet.mergeSchema           false
spark.sql.parquet.filterPushdown        true
spark.sql.parquet.writeLegacyFormat     false

# ORC settings  
spark.sql.orc.compression.codec         snappy
spark.sql.orc.filterPushdown            true
spark.sql.orc.enabled                   true

# CSV settings
spark.sql.csv.parser.columnPruning.enabled  true

# General compression
# Options: snappy, gzip, lzo, zstd, none
spark.io.compression.codec              snappy


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 7: ADAPTIVE QUERY EXECUTION (AQE)                                   â•‘
# â•‘  Enable dynamic optimization during query execution                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable AQE (Spark 3.0+)
# Dynamically optimizes queries at runtime
spark.sql.adaptive.enabled              true

# Coalesce shuffle partitions
spark.sql.adaptive.coalescePartitions.enabled       true
spark.sql.adaptive.coalescePartitions.minPartitionSize  1MB

# Optimize skew joins
spark.sql.adaptive.skewJoin.enabled     true
spark.sql.adaptive.skewJoin.skewedPartitionFactor   5
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes  256MB

# Optimize join strategies
spark.sql.adaptive.localShuffleReader.enabled       true


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 8: OPTIMIZATION & PERFORMANCE                                       â•‘
# â•‘  General performance tuning settings                                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Broadcast join threshold
# Join smaller tables if under this size (default: 10MB)
spark.sql.autoBroadcastJoinThreshold    10485760

# Enable cost-based optimization
spark.sql.cbo.enabled                   true
spark.sql.cbo.joinReorder.enabled       true

# Enable predicate pushdown
spark.sql.parquet.filterPushdown        true
spark.sql.orc.filterPushdown            true

# Optimize null handling
spark.sql.optimizer.excludedRules       

# Whole stage code generation
spark.sql.codegen.wholeStage            true
spark.sql.codegen.maxFields             100

# Dynamic allocation (for clusters)
# spark.dynamicAllocation.enabled       false
# spark.dynamicAllocation.minExecutors  1
# spark.dynamicAllocation.maxExecutors  10


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 9: SERIALIZATION                                                    â•‘
# â•‘  Configure how data is serialized for shuffles and caching                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Use Kryo serializer (faster than Java serialization)
spark.serializer                        org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired         false
spark.kryoserializer.buffer.max         512m


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 10: SHUFFLE & NETWORK                                               â•‘
# â•‘  Optimize data shuffling and network transfers                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Shuffle service (for clusters)
# spark.shuffle.service.enabled         false

# Shuffle file buffer
spark.shuffle.file.buffer               32k

# Shuffle compress
spark.shuffle.compress                  true
spark.shuffle.spill.compress            true

# Network timeout
spark.network.timeout                   120s


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 11: LOGGING & MONITORING                                            â•‘
# â•‘  Configure application logging and event tracking                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Event log (for Spark UI and history server)
spark.eventLog.enabled                  true
spark.eventLog.dir                      /tmp/spark-events

# UI settings
spark.ui.enabled                        true
spark.ui.port                           4040
spark.ui.retainedJobs                   1000
spark.ui.retainedStages                 1000

# SQL UI settings
spark.sql.ui.retainedExecutions         1000


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 12: DELTA LAKE                                                      â•‘
# â•‘  Enable Delta Lake for ACID transactions and time travel                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Delta Lake extensions
spark.sql.extensions                    io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog         org.apache.spark.sql.delta.catalog.DeltaCatalog


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 13: DEVELOPMENT & DEBUGGING                                         â•‘
# â•‘  Settings useful during development                                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Show detailed errors
spark.sql.execution.pyspark.udf.simplifiedTraceback     false

# Python worker reuse (faster for multiple jobs)
spark.python.worker.reuse               true

# Python profile
# spark.python.profile                  false

# Explain plans (set to "extended" for detailed plans)
# spark.sql.planChangeLog.level         WARN


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SECTION 14: SECURITY                                                        â•‘
# â•‘  Authentication and encryption (mainly for production)                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Authentication (disabled for local development)
spark.authenticate                      false

# Encryption (disabled for local development)
# spark.network.crypto.enabled          false
# spark.io.encryption.enabled           false


#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF CONFIGURATION
#
# ğŸ“š QUICK REFERENCE SUMMARY:
#    Section 1:  Application basics (master, deploy mode)
#    Section 2:  Memory (4g driver, 4g executor)
#    Section 3:  Executors (2 cores, 200 shuffle partitions)
#    Section 4:  Hive (enabled, /tmp/spark-warehouse)
#    Section 5:  Kafka (streaming packages included)
#    Section 6:  File formats (Parquet/ORC with snappy)
#    Section 7:  AQE (enabled for dynamic optimization)
#    Section 8:  Performance (broadcast join, CBO, codegen)
#    Section 9:  Serialization (Kryo)
#    Section 10: Shuffle (compressed, 120s timeout)
#    Section 11: Logging (event logs to /tmp/spark-events)
#    Section 12: Delta Lake (extensions enabled)
#    Section 13: Development (detailed errors, worker reuse)
#    Section 14: Security (disabled for local dev)
#
# ğŸ’¡ CUSTOMIZATION TIPS:
#    â€¢ Local machine: Keep these settings
#    â€¢ Production cluster: Increase memory (8g-16g) and executors
#    â€¢ Large datasets: Increase shuffle.partitions (500-2000)
#    â€¢ Streaming: Tune Kafka consumer settings
#    â€¢ Memory issues: Decrease driver/executor memory or increase overhead
#
# ğŸš€ USAGE EXAMPLES:
#
#    # In your PySpark code (override defaults):
#    spark = SparkSession.builder \
#        .appName("MyApp") \
#        .config("spark.sql.shuffle.partitions", "500") \
#        .config("spark.executor.memory", "8g") \
#        .getOrCreate()
#
#    # Or use this file with spark-submit:
#    spark-submit --properties-file config/spark-defaults.conf my_script.py
#
# ğŸ“– DOCUMENTATION:
#    Official docs: https://spark.apache.org/docs/latest/configuration.html
#    See also: docs/pyspark_overview.md for learning path
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
