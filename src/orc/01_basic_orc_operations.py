#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
ORC BASIC OPERATIONS - Reading and Writing ORC Files
================================================================================

ğŸ“– OVERVIEW:
Apache ORC (Optimized Row Columnar) is a columnar storage format optimized for
Hive and big data analytics. It provides excellent compression and performance.

ğŸ¯ KEY FEATURES:
â€¢ Columnar storage (like Parquet)
â€¢ Built-in indexes (row groups, bloom filters)
â€¢ ACID transaction support
â€¢ Superior compression
â€¢ Optimized for Hive ecosystem

ğŸš€ RUN:
spark-submit 01_basic_orc_operations.py

ğŸ“¦ BENEFITS:
â€¢ 75% smaller than text formats
â€¢ 3-5x faster than Parquet for some workloads
â€¢ Best integration with Hive
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime, timedelta
import random


def create_spark_session():
    """Create Spark session optimized for ORC."""
    print("=" * 80)
    print("ğŸš€ CREATING SPARK SESSION WITH ORC SUPPORT")
    print("=" * 80)
    
    spark = SparkSession.builder \
        .appName("ORC Basic Operations") \
        .config("spark.sql.orc.impl", "native") \
        .config("spark.sql.orc.enableVectorizedReader", "true") \
        .config("spark.sql.orc.filterPushdown", "true") \
        .config("spark.sql.orc.char.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("âœ… Spark session created")
    print(f"âœ… ORC implementation: native")
    print(f"âœ… Vectorized reader: enabled")
    print(f"âœ… Filter pushdown: enabled")
    print()
    
    return spark


def generate_sample_data(spark):
    """Generate sample dataset."""
    print("=" * 80)
    print("ğŸ“Š GENERATING SAMPLE DATA")
    print("=" * 80)
    
    num_records = 10000
    
    data = []
    products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse']
    regions = ['North', 'South', 'East', 'West', 'Central']
    statuses = ['Completed', 'Pending', 'Cancelled', 'Shipped']
    
    for i in range(num_records):
        record = {
            'transaction_id': i + 1,
            'customer_id': random.randint(1000, 5000),
            'product': random.choice(products),
            'quantity': random.randint(1, 20),
            'price': round(random.uniform(50.0, 2000.0), 2),
            'region': random.choice(regions),
            'status': random.choice(statuses),
            'transaction_date': (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d'),
            'is_premium': random.random() > 0.7,
            'discount_rate': round(random.uniform(0.0, 0.3), 2)
        }
        record['total_amount'] = round(record['quantity'] * record['price'] * (1 - record['discount_rate']), 2)
        data.append(record)
    
    df = spark.createDataFrame(data)
    
    print(f"âœ… Generated {num_records:,} transactions")
    print(f"âœ… Columns: {', '.join(df.columns)}")
    print()
    
    return df


def example_1_write_orc(spark, df):
    """
    Example 1: Write DataFrame to ORC format
    
    ğŸ“ Demonstrates:
    â€¢ Basic write operation
    â€¢ Compression codecs
    â€¢ Write modes
    """
    print("=" * 80)
    print("EXAMPLE 1: Write DataFrame to ORC")
    print("=" * 80)
    
    output_path = "/tmp/pyspark_examples/orc/transactions_basic"
    
    print("ğŸ“ Writing DataFrame to ORC...")
    
    df.write \
        .format("orc") \
        .mode("overwrite") \
        .save(output_path)
    
    print(f"âœ… Data written to: {output_path}")
    print(f"âœ… Format: ORC (columnar)")
    print(f"âœ… Compression: zlib (default)")
    print()
    
    # Show file statistics
    import os
    total_size = 0
    file_count = 0
    for root, dirs, files in os.walk(output_path):
        for file in files:
            if file.endswith('.orc') or file.startswith('part-'):
                file_path = os.path.join(root, file)
                size = os.path.getsize(file_path)
                total_size += size
                file_count += 1
    
    print(f"ğŸ“Š File Statistics:")
    print(f"   Total size: {total_size / (1024*1024):.2f} MB")
    print(f"   Number of files: {file_count}")
    if file_count > 0:
        print(f"   Average file size: {total_size / (1024*1024*file_count):.2f} MB")
    print()


def example_2_read_orc(spark):
    """
    Example 2: Read ORC files
    
    ğŸ“ Demonstrates:
    â€¢ Basic read operation
    â€¢ Automatic schema inference
    â€¢ Built-in statistics
    """
    print("=" * 80)
    print("EXAMPLE 2: Read ORC Files")
    print("=" * 80)
    
    input_path = "/tmp/pyspark_examples/orc/transactions_basic"
    
    print(f"ğŸ“– Reading ORC from: {input_path}")
    
    df = spark.read \
        .format("orc") \
        .load(input_path)
    
    print("âœ… Data loaded successfully")
    print()
    
    print("ğŸ“‹ Schema Information:")
    df.printSchema()
    
    print("ğŸ“Š Sample Data (first 10 rows):")
    df.show(10)
    
    print(f"ğŸ“ˆ Total records: {df.count():,}")
    print()


def example_3_compression_comparison(spark, df):
    """
    Example 3: Compression Codec Comparison
    
    ğŸ“ Demonstrates:
    â€¢ zlib (default, good compression)
    â€¢ snappy (fast)
    â€¢ lzo (fast, moderate compression)
    â€¢ none (uncompressed)
    """
    print("=" * 80)
    print("EXAMPLE 3: Compression Codecs")
    print("=" * 80)
    
    codecs = ['none', 'snappy', 'zlib']
    results = []
    
    import time
    
    for codec in codecs:
        output_path = f"/tmp/pyspark_examples/orc/transactions_{codec}"
        
        print(f"ğŸ“ Writing with {codec} compression...")
        
        start_time = time.time()
        
        df.write \
            .format("orc") \
            .mode("overwrite") \
            .option("compression", codec) \
            .save(output_path)
        
        write_time = time.time() - start_time
        
        # Calculate size
        import os
        total_size = 0
        for root, dirs, files in os.walk(output_path):
            for file in files:
                if file.startswith('part-'):
                    total_size += os.path.getsize(os.path.join(root, file))
        
        results.append({
            'codec': codec,
            'size_mb': total_size / (1024*1024),
            'write_time': write_time
        })
        
        print(f"   âœ“ Size: {total_size / (1024*1024):.2f} MB")
        print(f"   âœ“ Write time: {write_time:.2f} seconds")
        print()
    
    print("ğŸ“Š Compression Comparison:")
    print("-" * 60)
    print(f"{'Codec':<15} {'Size (MB)':<15} {'Write Time (s)':<15} {'Ratio'}")
    print("-" * 60)
    
    base_size = results[0]['size_mb']
    for r in results:
        ratio = base_size / r['size_mb']
        print(f"{r['codec']:<15} {r['size_mb']:<15.2f} {r['write_time']:<15.2f} {ratio:.2f}x")
    
    print("-" * 60)
    print()
    
    print("ğŸ’¡ Codec Recommendations:")
    print("   â€¢ zlib: Best compression (default, recommended)")
    print("   â€¢ snappy: Fast compression, good for hot data")
    print("   â€¢ lzo: Balance of speed and compression")
    print()


def example_4_predicate_pushdown(spark):
    """
    Example 4: Predicate Pushdown with ORC Statistics
    
    ğŸ“ Demonstrates:
    â€¢ Column statistics (min/max/count)
    â€¢ Predicate pushdown optimization
    â€¢ Stripe-level filtering
    """
    print("=" * 80)
    print("EXAMPLE 4: Predicate Pushdown")
    print("=" * 80)
    
    input_path = "/tmp/pyspark_examples/orc/transactions_basic"
    
    print("ğŸ” Applying filter: product = 'Laptop' AND total_amount > 1000")
    
    df = spark.read \
        .format("orc") \
        .load(input_path) \
        .filter((col('product') == 'Laptop') & (col('total_amount') > 1000))
    
    print("âœ… Predicate pushdown enabled (ORC stripe-level filtering)")
    print()
    
    print("ğŸ“Š Filtered Results:")
    df.show(20)
    
    print(f"ğŸ“ˆ Matching records: {df.count():,}")
    print()
    
    print("ğŸ’¡ ORC Predicate Pushdown:")
    print("   â€¢ Min/max statistics for each stripe")
    print("   â€¢ Skip entire stripes that don't match")
    print("   â€¢ Bloom filters for precise filtering")
    print("   â€¢ Can skip 90%+ of data")
    print()


def example_5_partitioned_orc(spark, df):
    """
    Example 5: Partitioned ORC Files
    
    ğŸ“ Demonstrates:
    â€¢ Partitioning by columns
    â€¢ Partition pruning
    â€¢ Hive-style partitions
    """
    print("=" * 80)
    print("EXAMPLE 5: Partitioned ORC")
    print("=" * 80)
    
    output_path = "/tmp/pyspark_examples/orc/transactions_partitioned"
    
    print("ğŸ“ Writing partitioned by region and status...")
    
    df.write \
        .format("orc") \
        .mode("overwrite") \
        .partitionBy("region", "status") \
        .save(output_path)
    
    print(f"âœ… Data partitioned and written to: {output_path}")
    print()
    
    print("ğŸ“‚ Partition Structure (sample):")
    import os
    partitions = []
    for root, dirs, files in os.walk(output_path):
        if any(f.startswith('part-') for f in files):
            rel_path = os.path.relpath(root, output_path)
            if rel_path != '.':
                partitions.append(rel_path)
    
    for i, partition in enumerate(sorted(partitions)[:15], 1):
        print(f"   {i}. {partition}")
    
    if len(partitions) > 15:
        print(f"   ... and {len(partitions) - 15} more partitions")
    
    print()
    
    # Demonstrate partition pruning
    print("ğŸ” Reading only North region + Completed status...")
    
    df_filtered = spark.read \
        .format("orc") \
        .load(output_path) \
        .filter((col('region') == 'North') & (col('status') == 'Completed'))
    
    print(f"âœ… Only scanned North/Completed partition")
    print(f"ğŸ“ˆ Records: {df_filtered.count():,}")
    print()


def example_6_orc_with_indexes(spark):
    """
    Example 6: ORC Built-in Indexes
    
    ğŸ“ Demonstrates:
    â€¢ Row group indexes
    â€¢ Bloom filters
    â€¢ Column statistics
    """
    print("=" * 80)
    print("EXAMPLE 6: ORC Built-in Indexes")
    print("=" * 80)
    
    # Create dataset with patterns
    data = []
    for i in range(100000):
        data.append({
            'id': i,
            'category': f"cat_{i % 100}",
            'value': random.uniform(0, 10000),
            'status': random.choice(['A', 'B', 'C', 'D'])
        })
    
    df = spark.createDataFrame(data)
    
    path = "/tmp/pyspark_examples/orc/with_indexes"
    
    print("ğŸ“ Writing ORC with automatic indexes...")
    
    df.write \
        .format("orc") \
        .mode("overwrite") \
        .option("orc.bloom.filter.columns", "category,status") \
        .option("orc.bloom.filter.fpp", "0.05") \
        .save(path)
    
    print("âœ… ORC written with bloom filters on 'category' and 'status'")
    print()
    
    print("ğŸ” Testing bloom filter efficiency...")
    print("   Query: category = 'cat_50'")
    
    import time
    start = time.time()
    
    result = spark.read \
        .format("orc") \
        .load(path) \
        .filter(col('category') == 'cat_50') \
        .count()
    
    query_time = time.time() - start
    
    print(f"   âœ“ Found {result:,} records in {query_time:.2f}s")
    print()
    
    print("ğŸ’¡ ORC Indexes:")
    print("   â€¢ Row group indexes (automatic)")
    print("   â€¢ Bloom filters (configurable)")
    print("   â€¢ Column statistics (automatic)")
    print("   â€¢ Stripe-level metadata")
    print()


def example_7_acid_transactions(spark):
    """
    Example 7: ACID Transaction Support
    
    ğŸ“ Demonstrates:
    â€¢ ORC's ACID capabilities (when used with Hive)
    â€¢ Append mode
    â€¢ Overwrite mode
    """
    print("=" * 80)
    print("EXAMPLE 7: ACID Transaction Support")
    print("=" * 80)
    
    path = "/tmp/pyspark_examples/orc/acid_demo"
    
    # Initial data
    data_v1 = [
        {'id': 1, 'name': 'Alice', 'balance': 1000},
        {'id': 2, 'name': 'Bob', 'balance': 1500}
    ]
    
    df_v1 = spark.createDataFrame(data_v1)
    
    print("ğŸ“ Writing initial data...")
    df_v1.write.format("orc").mode("overwrite").save(path)
    df_v1.show()
    
    # Append more data
    data_v2 = [
        {'id': 3, 'name': 'Charlie', 'balance': 2000},
        {'id': 4, 'name': 'Diana', 'balance': 2500}
    ]
    
    df_v2 = spark.createDataFrame(data_v2)
    
    print("ğŸ“ Appending new data...")
    df_v2.write.format("orc").mode("append").save(path)
    
    # Read all
    df_all = spark.read.format("orc").load(path)
    
    print("âœ… All data after append:")
    df_all.orderBy('id').show()
    
    print(f"ğŸ“ˆ Total records: {df_all.count()}")
    print()
    
    print("ğŸ’¡ ACID in ORC:")
    print("   â€¢ Supports Atomicity, Consistency, Isolation, Durability")
    print("   â€¢ Best with Hive metastore")
    print("   â€¢ Enables INSERT, UPDATE, DELETE in Hive")
    print("   â€¢ ORC is only format supporting ACID in Hive")
    print()


def example_8_complex_types(spark):
    """
    Example 8: Complex and Nested Data Types
    
    ğŸ“ Demonstrates:
    â€¢ Struct, Array, Map types
    â€¢ Nested structures
    """
    print("=" * 80)
    print("EXAMPLE 8: Complex Data Types")
    print("=" * 80)
    
    data = [
        {
            'customer_id': 1,
            'profile': {'name': 'Alice', 'age': 30, 'city': 'NYC'},
            'purchases': [
                {'item': 'Laptop', 'price': 1200},
                {'item': 'Mouse', 'price': 25}
            ],
            'preferences': {'theme': 'dark', 'language': 'en'}
        },
        {
            'customer_id': 2,
            'profile': {'name': 'Bob', 'age': 25, 'city': 'LA'},
            'purchases': [
                {'item': 'Phone', 'price': 800}
            ],
            'preferences': {'theme': 'light', 'language': 'es'}
        }
    ]
    
    df = spark.createDataFrame(data)
    
    print("ğŸ“‹ Schema with complex types:")
    df.printSchema()
    
    path = "/tmp/pyspark_examples/orc/complex_types"
    
    print("\nğŸ“ Writing complex nested data to ORC...")
    df.write.format("orc").mode("overwrite").save(path)
    
    print("âœ… Complex types preserved")
    print()
    
    print("ğŸ“– Reading and querying nested data...")
    df_read = spark.read.format("orc").load(path)
    
    df_read.select(
        col('customer_id'),
        col('profile.name').alias('name'),
        col('profile.city').alias('city'),
        size(col('purchases')).alias('purchase_count'),
        col('preferences.theme').alias('theme')
    ).show()
    
    print("ğŸ’¡ Complex Types in ORC:")
    print("   â€¢ Full support for nested structures")
    print("   â€¢ Efficient columnar storage even for nested data")
    print("   â€¢ Good performance with complex schemas")
    print()


def main():
    """Main execution function."""
    print("\n" + "ğŸ”¥ " * 40)
    print("ORC BASIC OPERATIONS - COMPREHENSIVE GUIDE")
    print("ğŸ”¥ " * 40)
    print()
    
    spark = create_spark_session()
    
    try:
        # Generate sample data
        df = generate_sample_data(spark)
        
        # Run examples
        example_1_write_orc(spark, df)
        example_2_read_orc(spark)
        example_3_compression_comparison(spark, df)
        example_4_predicate_pushdown(spark)
        example_5_partitioned_orc(spark, df)
        example_6_orc_with_indexes(spark)
        example_7_acid_transactions(spark)
        example_8_complex_types(spark)
        
        print("=" * 80)
        print("âœ… ALL EXAMPLES COMPLETED SUCCESSFULLY")
        print("=" * 80)
        print()
        
        print("""
ğŸ“š Summary - ORC Best Practices:

1. âœ… Use zlib compression (default, best ratio)
2. âœ… Enable vectorized reader
3. âœ… Use bloom filters for high-cardinality columns
4. âœ… Partition large datasets
5. âœ… Leverage built-in statistics
6. âœ… Best for Hive ecosystem
7. âœ… ACID support when needed

ğŸ¯ When to Use ORC:
   â€¢ Hive data warehouse
   â€¢ ACID transactions required
   â€¢ Heavy read workloads
   â€¢ Analytical queries (OLAP)
   â€¢ When you need bloom filters
   â€¢ Best compression needed

ğŸš« When NOT to Use ORC:
   â€¢ Streaming workloads (use Avro)
   â€¢ Non-Hive environments (use Parquet)
   â€¢ Small datasets
   â€¢ Frequent schema changes
        """)
    
    finally:
        spark.stop()
        print("âœ… Spark session stopped")


if __name__ == "__main__":
    main()
