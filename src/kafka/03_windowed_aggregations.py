#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
WINDOWED AGGREGATIONS - Time-Based Analytics on Kafka Streams
================================================================================

ğŸ“– OVERVIEW:
Demonstrates time-window based aggregations on Kafka streaming data:
â€¢ Tumbling windows (non-overlapping)
â€¢ Sliding windows (overlapping)
â€¢ Session windows (activity-based)
â€¢ Watermarks for late data handling

ğŸ¯ USE CASES:
â€¢ Real-time dashboards (metrics per minute/hour)
â€¢ Alert systems (threshold detection in time windows)
â€¢ Trend analysis (comparing window to window)
â€¢ Session analytics (user activity sessions)

ğŸš€ RUN:
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  03_windowed_aggregations.py
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, window, count, sum as _sum, avg, max as _max, min as _min,
    current_timestamp, expr, session_window
)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType


def create_spark_session():
    """Create SparkSession for windowed streaming."""
    print("=" * 80)
    print("ğŸš€ WINDOWED AGGREGATIONS - SPARK SESSION")
    print("=" * 80)
    
    spark = SparkSession.builder \
        .appName("KafkaWindowedAggregations") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/windowed_checkpoint") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Spark {spark.version} ready for windowed streaming")
    print()
    
    return spark


def example_1_tumbling_windows(spark):
    """
    Example 1: Tumbling Windows - Non-overlapping time buckets.
    
    Use Case: Page views per 5-minute intervals
    """
    print("=" * 80)
    print("ğŸ“Š EXAMPLE 1: TUMBLING WINDOWS")
    print("=" * 80)
    
    print("""
ğŸ• Tumbling Windows (Non-Overlapping):

Timeline:
â”œâ”€â”€â”€â”€ 10:00 â”€â”€â”€â”€â”¼â”€â”€â”€â”€ 10:05 â”€â”€â”€â”€â”¼â”€â”€â”€â”€ 10:10 â”€â”€â”€â”€â”¼â”€â”€â”€â”€ 10:15 â”€â”€â”€â”€â”¤
â”‚  Window 1     â”‚  Window 2     â”‚  Window 3     â”‚  Window 4     â”‚
â”‚  (5 min)      â”‚  (5 min)      â”‚  (5 min)      â”‚  (5 min)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each event belongs to exactly ONE window.
Perfect for: Metrics dashboards, hourly/daily reports

Example: Count page views per 5-minute bucket
    """)
    
    # Define user event schema
    event_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("event_type", StringType(), True),
        StructField("page_url", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("country", StringType(), True)
    ])
    
    try:
        # Read from Kafka
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "user-events") \
            .option("startingOffsets", "latest") \
            .load()
        
        # Parse JSON
        events = kafka_stream.select(
            from_json(col("value").cast("string"), event_schema).alias("data")
        ).select("data.*")
        
        # Apply watermark and tumbling window
        windowed_counts = events \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "5 minutes"),  # Tumbling 5-min windows
                col("country")
            ) \
            .agg(
                count("*").alias("event_count"),
                count(expr("CASE WHEN event_type = 'page_view' THEN 1 END")).alias("page_views"),
                count(expr("CASE WHEN event_type = 'button_click' THEN 1 END")).alias("clicks")
            )
        
        print("""
âœ… Tumbling Window Configuration:
   â€¢ Window size: 5 minutes
   â€¢ Watermark: 10 minutes (handle late data up to 10 min)
   â€¢ Group by: country
   â€¢ Metrics: Total events, page views, clicks per window

ğŸ“Š Output Schema:
   window: {start: timestamp, end: timestamp}
   country: string
   event_count: long
   page_views: long
   clicks: long

ğŸ’¡ Watermark Explanation:
   "10 minutes" = Keep state for events up to 10 minutes late
   
   Example timeline:
   Current time: 10:15
   Processing window: 10:05-10:10
   Accept events timestamped >= 10:05 (10:15 - 10min watermark)
   Drop events timestamped < 10:05 (too late)
        """)
        
        print("\nâœ… Example 1 setup complete (ready to start query)")
        
    except Exception as e:
        print(f"âš ï¸  Note: Requires running Kafka. Error: {e}")


def example_2_sliding_windows(spark):
    """
    Example 2: Sliding Windows - Overlapping time buckets.
    
    Use Case: Moving average of sensor readings
    """
    print("\n" + "=" * 80)
    print("ï¿½ï¿½ EXAMPLE 2: SLIDING WINDOWS")
    print("=" * 80)
    
    print("""
ğŸ• Sliding Windows (Overlapping):

Timeline:
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        Window 1 (10:00 - 10:10)
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    Window 2 (10:05 - 10:15)
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    Window 3 (10:10 - 10:20)
10:00   10:05   10:10   10:15   10:20

Window: 10 minutes, Slide: 5 minutes
Each event may belong to MULTIPLE windows (overlap).
Perfect for: Moving averages, trend detection

Example: Average temperature per 10-min window, sliding every 5 min
    """)
    
    # IoT sensor schema
    sensor_schema = StructType([
        StructField("sensor_id", StringType(), True),
        StructField("sensor_type", StringType(), True),
        StructField("value", DoubleType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("location", StringType(), True)
    ])
    
    try:
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "iot-sensors") \
            .option("startingOffsets", "latest") \
            .load()
        
        sensors = kafka_stream.select(
            from_json(col("value").cast("string"), sensor_schema).alias("data")
        ).select("data.*")
        
        # Sliding window: 10-min window, 5-min slide
        sliding_avg = sensors \
            .filter(col("sensor_type") == "temperature") \
            .withWatermark("timestamp", "15 minutes") \
            .groupBy(
                window(col("timestamp"), "10 minutes", "5 minutes"),  # Sliding window
                col("location")
            ) \
            .agg(
                avg("value").alias("avg_temperature"),
                _min("value").alias("min_temperature"),
                _max("value").alias("max_temperature"),
                count("*").alias("reading_count")
            )
        
        print("""
âœ… Sliding Window Configuration:
   â€¢ Window duration: 10 minutes
   â€¢ Slide duration: 5 minutes
   â€¢ Watermark: 15 minutes
   â€¢ Metrics: avg, min, max temperature per location

ğŸ“Š Example Output (Temperature readings):
   
   Window: [2024-12-15 10:00:00, 2024-12-15 10:10:00]
   Location: warehouse_a
   Avg Temperature: 22.5Â°C, Min: 20.1Â°C, Max: 24.3Â°C
   
   Window: [2024-12-15 10:05:00, 2024-12-15 10:15:00]
   Location: warehouse_a
   Avg Temperature: 23.1Â°C, Min: 21.2Â°C, Max: 25.0Â°C

ğŸ’¡ When to Use Sliding Windows:
   âœ“ Smoothing noisy data (moving average)
   âœ“ Detecting gradual trends
   âœ“ Real-time monitoring with overlap for context
   âœ“ Alert systems (threshold breaches in rolling window)
        """)
        
        print("\nâœ… Example 2 setup complete")
        
    except Exception as e:
        print(f"âš ï¸  Note: Requires running Kafka. Error: {e}")


def example_3_session_windows(spark):
    """
    Example 3: Session Windows - Activity-based windows.
    
    Use Case: User session analytics (gap-based grouping)
    """
    print("\n" + "=" * 80)
    print("ğŸ‘¤ EXAMPLE 3: SESSION WINDOWS")
    print("=" * 80)
    
    print("""
ğŸ• Session Windows (Gap-Based):

User Activity Timeline:
Event1  Event2  Event3  â”€â”€â”€ GAP (>5 min) â”€â”€â”€  Event4  Event5
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ Session 1  â”‚                                 â”‚Session 2â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gap threshold: 5 minutes of inactivity = end session
Perfect for: User behavior analysis, session metrics

Example: Session duration and event count per user
    """)
    
    event_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("event_type", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("page_url", StringType(), True)
    ])
    
    try:
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "user-events") \
            .option("startingOffsets", "latest") \
            .load()
        
        events = kafka_stream.select(
            from_json(col("value").cast("string"), event_schema).alias("data")
        ).select("data.*")
        
        # Session window: 5-minute gap threshold
        user_sessions = events \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                col("user_id"),
                session_window(col("timestamp"), "5 minutes")  # Session window
            ) \
            .agg(
                count("*").alias("events_in_session"),
                _min("timestamp").alias("session_start"),
                _max("timestamp").alias("session_end"),
                expr("(unix_timestamp(max(timestamp)) - unix_timestamp(min(timestamp))) / 60").alias("session_duration_minutes"),
                count(expr("CASE WHEN event_type = 'purchase' THEN 1 END")).alias("conversions")
            )
        
        print("""
âœ… Session Window Configuration:
   â€¢ Gap threshold: 5 minutes
   â€¢ Group by: user_id
   â€¢ Watermark: 10 minutes
   â€¢ Metrics: event count, session duration, conversions

ğŸ“Š Example Output (User Sessions):
   
   User: user_1234
   Session: [2024-12-15 10:00:00, 2024-12-15 10:12:00]
   Duration: 12 minutes
   Events: 8 (7 page views, 1 purchase)
   Conversions: 1

ğŸ’¡ Use Cases for Session Windows:
   âœ“ User engagement metrics (session length, events per session)
   âœ“ Conversion funnels (events leading to purchase)
   âœ“ Bounce rate (single-event sessions)
   âœ“ Session timeout detection (inactive users)
   âœ“ Web analytics (time on site, pages per session)
        """)
        
        print("\nâœ… Example 3 setup complete")
        
    except Exception as e:
        print(f"âš ï¸  Note: Requires running Kafka. Error: {e}")


def example_4_advanced_patterns(spark):
    """
    Example 4: Advanced patterns - Combining windows with other operations.
    """
    print("\n" + "=" * 80)
    print("ğŸ¯ EXAMPLE 4: ADVANCED WINDOWING PATTERNS")
    print("=" * 80)
    
    print("""
ğŸ“š Advanced Windowing Techniques:

1. Multiple Aggregations in Same Window:
   â””â”€ Group by window + multiple dimensions
   â””â”€ Compute various metrics simultaneously

2. Window-to-Window Comparison:
   â””â”€ Compare current window to previous
   â””â”€ Detect anomalies or trends

3. Nested Windows:
   â””â”€ Minute windows aggregated into hour windows
   â””â”€ Multi-level time hierarchies

4. Conditional Aggregations:
   â””â”€ Different logic per event type
   â””â”€ Filter within aggregation

Example: E-commerce metrics per 10-minute window
    """)
    
    purchase_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("category", StringType(), True),
        StructField("total_amount", DoubleType(), True),
        StructField("payment_method", StringType(), True),
        StructField("timestamp", TimestampType(), True)
    ])
    
    try:
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "payment-events") \
            .option("startingOffsets", "latest") \
            .load()
        
        purchases = kafka_stream.select(
            from_json(col("value").cast("string"), purchase_schema).alias("data")
        ).select("data.*")
        
        # Complex aggregations
        ecommerce_metrics = purchases \
            .withWatermark("timestamp", "30 minutes") \
            .groupBy(
                window(col("timestamp"), "10 minutes"),
                col("category")
            ) \
            .agg(
                # Revenue metrics
                _sum("total_amount").alias("total_revenue"),
                avg("total_amount").alias("avg_order_value"),
                _max("total_amount").alias("largest_order"),
                
                # Volume metrics
                count("*").alias("order_count"),
                expr("count(distinct user_id)").alias("unique_customers"),
                
                # Payment method breakdown
                count(expr("CASE WHEN payment_method = 'credit_card' THEN 1 END")).alias("cc_orders"),
                count(expr("CASE WHEN payment_method = 'paypal' THEN 1 END")).alias("paypal_orders"),
                
                # Performance indicators
                expr("sum(total_amount) / count(*)").alias("revenue_per_order"),
                expr("sum(total_amount) / count(distinct user_id)").alias("revenue_per_customer")
            )
        
        print("""
âœ… Advanced Metrics Computed:
   Revenue: total, average, largest order
   Volume: orders, unique customers
   Payment: credit card vs PayPal orders
   KPIs: revenue per order, revenue per customer

ğŸ“Š Sample Dashboard Output:

   Window: 10:00 - 10:10 | Category: Electronics
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Total Revenue:        $12,450.00
   Orders:               45
   Unique Customers:     38
   Avg Order Value:      $276.67
   Largest Order:        $999.99
   Revenue/Customer:     $327.63
   Payment Mix:          30 CC, 15 PayPal

ğŸ’¡ Best Practices:
   âœ“ Combine multiple aggregations in one query (efficient)
   âœ“ Use CASE WHEN for conditional counting
   âœ“ Calculate ratios with expressions
   âœ“ Group by window + dimensions for drill-down
   âœ“ Monitor state size (watermark is critical!)
        """)
        
        print("\nâœ… Example 4 setup complete")
        
    except Exception as e:
        print(f"âš ï¸  Note: Requires running Kafka. Error: {e}")


def main():
    """Main execution function."""
    print("\n" + "ğŸ”¥ " * 40)
    print("WINDOWED AGGREGATIONS ON KAFKA STREAMS")
    print("ğŸ”¥ " * 40)
    print()
    
    spark = create_spark_session()
    
    example_1_tumbling_windows(spark)
    example_2_sliding_windows(spark)
    example_3_session_windows(spark)
    example_4_advanced_patterns(spark)
    
    print("\n" + "=" * 80)
    print("âœ… ALL WINDOWING EXAMPLES COMPLETE")
    print("=" * 80)
    
    print("""
ğŸ“š Summary - Windowing Strategies:

1. Tumbling Windows:
   âœ“ Non-overlapping time buckets
   âœ“ Use: Hourly/daily reports, simple dashboards
   âœ“ Example: .groupBy(window(col("timestamp"), "1 hour"))

2. Sliding Windows:
   âœ“ Overlapping time buckets
   âœ“ Use: Moving averages, trend detection
   âœ“ Example: .groupBy(window(col("timestamp"), "10 minutes", "5 minutes"))

3. Session Windows:
   âœ“ Activity-based (gap threshold)
   âœ“ Use: User sessions, behavior analysis
   âœ“ Example: .groupBy(session_window(col("timestamp"), "5 minutes"))

4. Watermarks (CRITICAL):
   âœ“ Define "how late is too late"
   âœ“ Enable state cleanup (prevent memory issues)
   âœ“ Example: .withWatermark("timestamp", "10 minutes")

ğŸ¯ Choosing the Right Window:

Question: "How many events per hour?"
â†’ Tumbling window (1 hour)

Question: "What's the moving average over last 30 minutes?"
â†’ Sliding window (30 minutes, 5-minute slide)

Question: "How long do users stay active?"
â†’ Session window (5-10 minute gap)

âš ï¸  Common Pitfalls:
   âœ— Forgetting watermark = unbounded state growth
   âœ— Watermark too short = valid late data dropped
   âœ— Watermark too long = excessive memory usage
   âœ— Window too small = too many small windows
   âœ— Window too large = delayed insights

ï¿½ï¿½ Related Files:
   â€¢ 01_basic_kafka_consumer.py - Reading from Kafka
   â€¢ 04_stream_joins.py - Joining windowed streams
   â€¢ ../streaming/03_kafka_streaming.py - Complete guide
    """)
    
    spark.stop()


if __name__ == "__main__":
    main()
