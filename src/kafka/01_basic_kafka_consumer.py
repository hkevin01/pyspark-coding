#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
BASIC KAFKA CONSUMER - Reading from Kafka Streams
================================================================================

ğŸ“– OVERVIEW:
This example demonstrates the fundamentals of consuming messages from Apache
Kafka using PySpark Structured Streaming. Learn how to:

â€¢ Connect to Kafka brokers
â€¢ Subscribe to topics
â€¢ Read streaming data
â€¢ Parse binary messages
â€¢ Display results to console

ğŸ¯ USE CASE:
Real-time log aggregation, event processing, or any scenario where you need
to consume and process messages from Kafka topics as they arrive.

ğŸ“‹ PREREQUISITES:
1. Kafka cluster running (local or remote)
2. Topic created with test data
3. spark-sql-kafka package installed

ğŸš€ RUN:
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  01_basic_kafka_consumer.py
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_json, struct, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import time


def create_spark_session():
    """
    Create SparkSession with Kafka support.
    """
    print("=" * 80)
    print("ğŸš€ CREATING SPARK SESSION WITH KAFKA SUPPORT")
    print("=" * 80)
    
    spark = SparkSession.builder \
        .appName("BasicKafkaConsumer") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/kafka_checkpoint") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Spark {spark.version} initialized")
    print(f"âœ… Checkpoint location: /tmp/kafka_checkpoint")
    print()
    
    return spark


def example_1_read_kafka_raw(spark):
    """
    Example 1: Read raw Kafka messages and display schema.
    
    Shows the default Kafka DataFrame structure with binary key/value.
    """
    print("=" * 80)
    print("ğŸ“¥ EXAMPLE 1: READ RAW KAFKA MESSAGES")
    print("=" * 80)
    
    print("""
ğŸ”Œ Connecting to Kafka:
   â€¢ Bootstrap servers: localhost:9092
   â€¢ Topic: user-events
   â€¢ Starting from: earliest offset
   â€¢ Fail on data loss: false (for demo resilience)

ğŸ“Š Kafka Message Structure:
   Kafka messages arrive as binary data with metadata:
   - key: Binary message key (optional, for partitioning)
   - value: Binary message payload (your actual data)
   - topic: Source topic name
   - partition: Partition number (0 to N-1)
   - offset: Unique message ID within partition
   - timestamp: Message creation or append time
   - timestampType: CreateTime (0) or LogAppendTime (1)
    """)
    
    try:
        # Read from Kafka
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "user-events") \
            .option("startingOffsets", "earliest") \
            .option("failOnDataLoss", "false") \
            .load()
        
        print("âœ… Successfully connected to Kafka topic: user-events")
        print("\nğŸ“‹ Kafka DataFrame Schema:")
        kafka_stream.printSchema()
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHEMA EXPLANATION:                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ key             : binary (nullable)                             â”‚
â”‚   â†’ Message key, used for partitioning (e.g., user_id)          â”‚
â”‚                                                                 â”‚
â”‚ value           : binary (non-nullable)                         â”‚
â”‚   â†’ Your actual message payload (JSON, Avro, etc.)              â”‚
â”‚                                                                 â”‚
â”‚ topic           : string (non-nullable)                         â”‚
â”‚   â†’ Topic name where message came from                          â”‚
â”‚                                                                 â”‚
â”‚ partition       : integer (non-nullable)                        â”‚
â”‚   â†’ Kafka partition number (determines parallelism)             â”‚
â”‚                                                                 â”‚
â”‚ offset          : long (non-nullable)                           â”‚
â”‚   â†’ Unique sequential ID per partition                          â”‚
â”‚                                                                 â”‚
â”‚ timestamp       : timestamp (non-nullable)                      â”‚
â”‚   â†’ When message was created or appended to log                 â”‚
â”‚                                                                 â”‚
â”‚ timestampType   : integer (non-nullable)                        â”‚
â”‚   â†’ 0 = CreateTime, 1 = LogAppendTime                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸  IMPORTANT: key and value are BINARY!
   You must cast them to string or parse them to use the data.
        """)
        
        # Convert binary to string for display
        readable_stream = kafka_stream.select(
            col("key").cast("string").alias("key"),
            col("value").cast("string").alias("value"),
            col("topic"),
            col("partition"),
            col("offset"),
            col("timestamp")
        )
        
        print("\nâ–¶ï¸  Starting streaming query (simulated - requires running Kafka)...")
        print("   Output: First 20 messages from the stream\n")
        
        # In production, you would use:
        # query = readable_stream.writeStream \
        #     .format("console") \
        #     .option("truncate", "false") \
        #     .outputMode("append") \
        #     .trigger(processingTime="5 seconds") \
        #     .start()
        # query.awaitTermination(30)
        
        print("âœ… Example 1 complete - raw Kafka message structure demonstrated")
        
    except Exception as e:
        print(f"âš ï¸  Note: This example requires a running Kafka cluster.")
        print(f"   Error: {str(e)}")
        print(f"   See README.md for Kafka setup instructions.")


def example_2_parse_json_messages(spark):
    """
    Example 2: Parse JSON messages from Kafka.
    
    Demonstrates extracting structured data from JSON payloads.
    """
    print("\n" + "=" * 80)
    print("ğŸ” EXAMPLE 2: PARSE JSON MESSAGES FROM KAFKA")
    print("=" * 80)
    
    print("""
ğŸ“– Scenario: User Events Stream
   Your application sends JSON events to Kafka:
   
   Sample Message Value:
   {
     "user_id": "user_12345",
     "event_type": "page_view",
     "page_url": "/products/laptop",
     "timestamp": "2024-12-15T10:30:00Z",
     "session_id": "sess_abc123",
     "device": "mobile",
     "country": "US",
     "referrer": "google.com"
   }
   
   Goal: Parse this JSON into structured columns
    """)
    
    # Define the JSON schema
    event_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("event_type", StringType(), True),
        StructField("page_url", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("session_id", StringType(), True),
        StructField("device", StringType(), True),
        StructField("country", StringType(), True),
        StructField("referrer", StringType(), True)
    ])
    
    print("\nğŸ“‹ Defined JSON Schema:")
    for field in event_schema.fields:
        print(f"   â€¢ {field.name}: {field.dataType.simpleString()}")
    
    try:
        # Read from Kafka
        kafka_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "user-events") \
            .option("startingOffsets", "latest") \
            .load()
        
        # Parse JSON from value column
        parsed_stream = kafka_stream.select(
            col("key").cast("string").alias("message_key"),
            from_json(col("value").cast("string"), event_schema).alias("event_data"),
            col("timestamp").alias("kafka_timestamp"),
            col("partition"),
            col("offset")
        ).select(
            "message_key",
            "event_data.*",
            "kafka_timestamp",
            "partition",
            "offset"
        )
        
        print("\nâœ… Parsed DataFrame Schema:")
        parsed_stream.printSchema()
        
        print("""
ğŸ¯ Benefits of Schema Definition:
   âœ“ Type safety - each field has correct data type
   âœ“ Null handling - schema defines nullable fields
   âœ“ Performance - Spark optimizes based on schema
   âœ“ Validation - malformed JSON handled gracefully
   âœ“ Predicate pushdown - filter on specific fields

ğŸ’¡ Best Practices:
   1. Always define schema explicitly (don't use inferSchema in streaming)
   2. Handle malformed JSON with try-catch or corrupt record column
   3. Use appropriate data types (TimestampType for dates, DoubleType for numbers)
   4. Extract only needed fields to reduce processing overhead
        """)
        
        # Filter example
        page_views = parsed_stream.filter(col("event_type") == "page_view")
        
        print("\nğŸ“Š Filtered Stream (page_view events only):")
        print("   Ready to process page view analytics...")
        
        print("\nâœ… Example 2 complete - JSON parsing demonstrated")
        
    except Exception as e:
        print(f"âš ï¸  Note: This example requires a running Kafka cluster.")
        print(f"   Error: {str(e)}")


def example_3_multiple_topics(spark):
    """
    Example 3: Subscribe to multiple Kafka topics.
    
    Shows how to consume from multiple topics simultaneously.
    """
    print("\n" + "=" * 80)
    print("ğŸ“š EXAMPLE 3: CONSUME FROM MULTIPLE TOPICS")
    print("=" * 80)
    
    print("""
ğŸ¯ Use Case: Multi-Source Event Processing
   
   Your system has different event types in separate topics:
   â€¢ "user-events" - User actions (clicks, views, searches)
   â€¢ "payment-events" - Transaction data
   â€¢ "system-logs" - Application logs
   
   You want to process all of them in a single streaming job.

ğŸ“‹ Three Ways to Subscribe:

1. Specific Topics (Comma-Separated):
   .option("subscribe", "topic1,topic2,topic3")
   
2. Topic Pattern (Regex):
   .option("subscribePattern", "events-.*")
   
3. Assign Specific Partitions:
   .option("assign", '{"topic1":[0,1],"topic2":[0]}')
    """)
    
    try:
        # Method 1: Multiple specific topics
        print("\nğŸ”¹ Method 1: Subscribe to specific topics")
        multi_topic_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "user-events,payment-events,system-logs") \
            .option("startingOffsets", "latest") \
            .load()
        
        print("   âœ… Subscribed to: user-events, payment-events, system-logs")
        
        # Method 2: Pattern-based subscription
        print("\nğŸ”¹ Method 2: Subscribe using pattern")
        pattern_stream = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribePattern", "events-.*") \
            .option("startingOffsets", "latest") \
            .load()
        
        print("   âœ… Subscribed to all topics matching: events-.*")
        
        # Route messages based on topic
        print("\nğŸ”€ Route Processing by Topic:")
        routed_stream = multi_topic_stream.select(
            col("topic"),
            col("key").cast("string").alias("key"),
            col("value").cast("string").alias("value"),
            col("partition"),
            col("offset")
        )
        
        print("""
   
   # Separate processing per topic:
   user_events = routed_stream.filter(col("topic") == "user-events")
   payments = routed_stream.filter(col("topic") == "payment-events")
   logs = routed_stream.filter(col("topic") == "system-logs")
   
   # Then apply topic-specific transformations
        """)
        
        print("""
ğŸ’¡ When to Use Each Method:

Multiple Topics (subscribe):
   âœ“ Known, fixed set of topics
   âœ“ Different schemas per topic
   âœ“ Explicit control

Pattern (subscribePattern):
   âœ“ Dynamic topics (new topics added over time)
   âœ“ Consistent schema across topics
   âœ“ Namespace-based organization (e.g., "prod-*")

Specific Partitions (assign):
   âœ“ Need exact partition control
   âœ“ Manual partition assignment
   âœ“ Advanced use cases only
        """)
        
        print("\nâœ… Example 3 complete - multi-topic consumption demonstrated")
        
    except Exception as e:
        print(f"âš ï¸  Note: This example requires a running Kafka cluster.")
        print(f"   Error: {str(e)}")


def example_4_kafka_options(spark):
    """
    Example 4: Important Kafka consumer options.
    
    Demonstrates key configuration options for production use.
    """
    print("\n" + "=" * 80)
    print("âš™ï¸  EXAMPLE 4: KAFKA CONSUMER OPTIONS")
    print("=" * 80)
    
    print("""
ğŸ”§ Essential Kafka Options for PySpark Streaming:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTION                          â”‚ PURPOSE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ kafka.bootstrap.servers         â”‚ Kafka broker addresses         â”‚
â”‚ subscribe / subscribePattern    â”‚ Topics to consume              â”‚
â”‚ startingOffsets                 â”‚ Where to start reading         â”‚
â”‚ failOnDataLoss                  â”‚ Handle missing offsets         â”‚
â”‚ maxOffsetsPerTrigger            â”‚ Rate limiting                  â”‚
â”‚ minPartitions                   â”‚ Spark parallelism              â”‚
â”‚ kafka.group.id                  â”‚ Consumer group ID              â”‚
â”‚ kafka.session.timeout.ms        â”‚ Session timeout                â”‚
â”‚ kafka.request.timeout.ms        â”‚ Request timeout                â”‚
â”‚ kafka.max.poll.records          â”‚ Records per poll               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # Example with common options
    print("\nğŸ“ Example Configuration:")
    print("""
kafka_stream = spark.readStream \\
    .format("kafka") \\
    .option("kafka.bootstrap.servers", "broker1:9092,broker2:9092,broker3:9092") \\
    .option("subscribe", "high-volume-topic") \\
    .option("startingOffsets", "latest") \\
    .option("failOnDataLoss", "false") \\
    .option("maxOffsetsPerTrigger", "10000") \\
    .option("minPartitions", "10") \\
    .option("kafka.group.id", "pyspark-consumer-group-1") \\
    .option("kafka.session.timeout.ms", "30000") \\
    .option("kafka.request.timeout.ms", "40000") \\
    .option("kafka.max.poll.records", "500") \\
    .option("kafka.isolation.level", "read_committed") \\
    .load()
    """)
    
    print("""
ğŸ“– Option Details:

1. startingOffsets:
   â€¢ "earliest" - Read from beginning of topic (all historical data)
   â€¢ "latest" - Read only new messages (from now on)
   â€¢ '{"topic1":{"0":23,"1":-1}}' - Start from specific offsets
   
   Production: Use "latest" to avoid reprocessing old data

2. failOnDataLoss:
   â€¢ true (default) - Fail if data lost (strict mode)
   â€¢ false - Continue if offsets missing (resilient mode)
   
   Production: Set to "false" for long-running jobs

3. maxOffsetsPerTrigger:
   â€¢ Limit records per micro-batch
   â€¢ Prevents overwhelming cluster
   â€¢ Enables backpressure handling
   
   Example: 10000 = max 10,000 records per batch

4. minPartitions:
   â€¢ Minimum Spark partitions (overrides Kafka partitions)
   â€¢ Increases parallelism
   
   Rule of thumb: 2-3x number of Kafka partitions

5. kafka.group.id:
   â€¢ Consumer group identifier
   â€¢ Required for offset tracking
   â€¢ Multiple apps with same group.id = load balancing
   
   Production: Use unique, descriptive names

6. kafka.isolation.level:
   â€¢ "read_uncommitted" (default) - Read all messages
   â€¢ "read_committed" - Only read committed transactions
   
   Use "read_committed" for exactly-once semantics
    """)
    
    print("""
âš¡ Performance Tuning:

High Throughput:
   âœ“ Increase maxOffsetsPerTrigger (10000-100000)
   âœ“ Increase minPartitions
   âœ“ Use larger trigger intervals
   âœ“ Enable compression

Low Latency:
   âœ“ Decrease maxOffsetsPerTrigger (100-1000)
   âœ“ Use continuous triggers
   âœ“ Reduce kafka.session.timeout.ms
   âœ“ More Kafka partitions

Resource-Constrained:
   âœ“ Lower maxOffsetsPerTrigger
   âœ“ Increase trigger interval
   âœ“ Reduce minPartitions
   âœ“ Enable data skipping
    """)
    
    print("\nâœ… Example 4 complete - Kafka options explained")


def example_5_error_handling(spark):
    """
    Example 5: Error handling and resilience patterns.
    """
    print("\n" + "=" * 80)
    print("ğŸ›¡ï¸  EXAMPLE 5: ERROR HANDLING & RESILIENCE")
    print("=" * 80)
    
    print("""
ğŸš¨ Common Kafka Streaming Errors and Solutions:

1. CONNECTION ERRORS:
   Error: "Failed to resolve Kafka bootstrap servers"
   Cause: Kafka brokers unreachable
   Solution: âœ“ Verify kafka.bootstrap.servers
            âœ“ Check network connectivity
            âœ“ Ensure Kafka is running

2. OFFSET OUT OF RANGE:
   Error: "Offset out of range"
   Cause: Requested offset no longer exists (retention policy)
   Solution: âœ“ Set failOnDataLoss=false
            âœ“ Use startingOffsets="latest"
            âœ“ Adjust Kafka retention settings

3. DESERIALIZATION ERRORS:
   Error: "Failed to parse JSON" / "Cast error"
   Cause: Malformed messages or schema mismatch
   Solution: âœ“ Add corrupt record column
            âœ“ Use try-catch in UDFs
            âœ“ Validate upstream producers

4. CONSUMER LAG:
   Error: Processing falls behind production
   Cause: Insufficient throughput
   Solution: âœ“ Increase parallelism (minPartitions)
            âœ“ Optimize transformations
            âœ“ Add more executors
            âœ“ Use maxOffsetsPerTrigger

5. CHECKPOINT CORRUPTION:
   Error: "Incompatible checkpoint"
   Cause: Code changed with existing checkpoint
   Solution: âœ“ Use new checkpoint location
            âœ“ Version checkpoints
            âœ“ Plan for schema evolution
    """)
    
    print("""
ğŸ’¡ Best Practices for Resilient Streaming:

1. Checkpointing:
   .option("checkpointLocation", "/reliable/storage/checkpoints/app1")
   
   âœ“ Use HDFS, S3, or other reliable storage
   âœ“ Never use local disk in production
   âœ“ Include app version in path

2. Malformed Data Handling:
   
   from pyspark.sql.functions import expr
   
   # Add corrupt record column
   schema_with_corrupt = StructType([
       StructField("_corrupt_record", StringType(), True),
       # ... other fields
   ])
   
   parsed = kafka_stream.select(
       from_json(col("value").cast("string"), schema_with_corrupt).alias("data")
   )
   
   # Separate good and bad records
   good_records = parsed.filter(col("data._corrupt_record").isNull())
   bad_records = parsed.filter(col("data._corrupt_record").isNotNull())
   
   # Write bad records to dead letter queue
   bad_records.writeStream \\
       .format("kafka") \\
       .option("topic", "dead-letter-queue") \\
       .start()

3. Monitoring:
   
   query = stream.writeStream.start()
   
   # Monitor progress
   while query.isActive:
       progress = query.lastProgress
       if progress:
           print(f"Input rate: {progress.get('inputRowsPerSecond', 0)}")
           print(f"Process rate: {progress.get('processedRowsPerSecond', 0)}")
           print(f"Batch duration: {progress['durationMs']['triggerExecution']}ms")
       time.sleep(10)

4. Graceful Shutdown:
   
   import signal
   
   def signal_handler(sig, frame):
       print("Shutting down gracefully...")
       query.stop()
       spark.stop()
       sys.exit(0)
   
   signal.signal(signal.SIGINT, signal_handler)
   signal.signal(signal.SIGTERM, signal_handler)
    """)
    
    print("\nâœ… Example 5 complete - error handling covered")


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ”¥ " * 40)
    print("KAFKA CONSUMER EXAMPLES - COMPREHENSIVE GUIDE")
    print("ğŸ”¥ " * 40)
    print()
    
    spark = create_spark_session()
    
    # Run all examples
    example_1_read_kafka_raw(spark)
    example_2_parse_json_messages(spark)
    example_3_multiple_topics(spark)
    example_4_kafka_options(spark)
    example_5_error_handling(spark)
    
    print("\n" + "=" * 80)
    print("âœ… ALL EXAMPLES COMPLETE")
    print("=" * 80)
    
    print("""
ğŸ“š Summary - What You Learned:

1. Basic Kafka Reading:
   âœ“ Connect to Kafka brokers
   âœ“ Subscribe to topics
   âœ“ Understand Kafka message structure (key, value, metadata)

2. JSON Parsing:
   âœ“ Define schemas for structured data
   âœ“ Use from_json() to parse messages
   âœ“ Extract nested fields

3. Multi-Topic Consumption:
   âœ“ Subscribe to multiple topics
   âœ“ Use patterns for dynamic topics
   âœ“ Route processing by topic

4. Configuration Options:
   âœ“ Essential Kafka options
   âœ“ Performance tuning parameters
   âœ“ Production-ready settings

5. Error Handling:
   âœ“ Common errors and solutions
   âœ“ Resilience patterns
   âœ“ Monitoring and alerting

ğŸ¯ Next Steps:
   1. Set up Kafka locally (see README.md)
   2. Create test topics and produce sample data
   3. Run these examples with real Kafka
   4. Experiment with different options
   5. Move to advanced examples (windowing, joins, aggregations)

ï¿½ï¿½ Related Files:
   â€¢ 02_kafka_json_producer.py - Produce test messages
   â€¢ 03_windowed_aggregations.py - Time-based analytics
   â€¢ 04_stream_joins.py - Join multiple streams
   â€¢ ../streaming/03_kafka_streaming.py - Complete streaming guide

ğŸ“– Documentation:
   â€¢ Kafka: https://kafka.apache.org/documentation/
   â€¢ PySpark Kafka: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
    """)
    
    spark.stop()


if __name__ == "__main__":
    main()
