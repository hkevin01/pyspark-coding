"""
Hive Integration with PySpark Example

This module demonstrates how to use Apache Hive with PySpark, showcasing:
‚Ä¢ Hive's SQL-to-MapReduce/Tez translation
‚Ä¢ Schema-on-read flexibility
‚Ä¢ HBase integration
‚Ä¢ Batch processing patterns
‚Ä¢ Data warehousing workflows
‚Ä¢ Hive metastore integration

Hive makes big data analytics accessible to SQL users without requiring
extensive programming skills.

Author: PySpark Learning Series
Date: December 2024
"""

import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *


def create_hive_enabled_spark_session():
    """
    Create Spark session with Hive support enabled.

    CONNECTING PYSPARK TO HIVE - REQUIREMENTS:
    =========================================
    To connect PySpark to Hive, you need:

    1. HIVE INSTALLED AND CONFIGURED:
       ‚Ä¢ Hive binaries installed
       ‚Ä¢ hive-site.xml configuration file
       ‚Ä¢ Location: $HIVE_HOME/conf/hive-site.xml

       Key configurations in hive-site.xml:
       <configuration>
         <property>
           <name>hive.metastore.uris</name>
           <value>thrift://localhost:9083</value>
         </property>
         <property>
           <name>hive.metastore.warehouse.dir</name>
           <value>/user/hive/warehouse</value>
         </property>
       </configuration>

    2. RUNNING HIVE METASTORE:
       ‚Ä¢ Start metastore service:
         $ hive --service metastore

       ‚Ä¢ Metastore stores metadata (schemas, tables, partitions)
       ‚Ä¢ Default port: 9083
       ‚Ä¢ Database backend: Derby, MySQL, or PostgreSQL

    3. SPARK WITH HIVE SUPPORT:
       ‚Ä¢ Option A: Spark built with Hive support
         Check: spark-submit --version (should show "with Hive")

       ‚Ä¢ Option B: Add Hive jars to Spark classpath
         spark.jars = /path/to/hive-metastore.jar,
                      /path/to/hive-exec.jar,
                      /path/to/hive-common.jar

       ‚Ä¢ Copy hive-site.xml to $SPARK_HOME/conf/

    CONNECTION VERIFICATION:
    =======================
    # Check if Hive support is available
    spark.conf.get("spark.sql.catalogImplementation")
    # Returns: "hive" (if enabled) or "in-memory" (if not)

    TROUBLESHOOTING:
    ===============
    Error: "java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf"
    Solution: Add Hive jars to Spark classpath

    Error: "Could not connect to meta store"
    Solution: Start Hive metastore service

    Error: "Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
    Solution: Check hive-site.xml configuration

    WHY YOU MIGHT NOT ALWAYS ENABLE HIVE:
    =====================================
    1. EXTRA DEPENDENCIES:
       ‚Ä¢ Hive support requires Hive libraries and a metastore
       ‚Ä¢ If you don't need them, enabling Hive adds unnecessary overhead
       ‚Ä¢ Increases JAR file size and deployment complexity

    2. STARTUP COST:
       ‚Ä¢ SparkSession with Hive support takes longer to initialize
       ‚Ä¢ Must connect to Hive metastore on startup
       ‚Ä¢ Added latency for each Spark application start

    3. COMPLEXITY:
       ‚Ä¢ Hive introduces schema enforcement
       ‚Ä¢ Requires permissions and access control setup
       ‚Ä¢ Needs configuration files (hive-site.xml)
       ‚Ä¢ For quick, lightweight jobs, this can be overkill

    4. PORTABILITY:
       ‚Ä¢ Running Spark in environments without Hive installed causes errors
       ‚Ä¢ Local development environments may not have Hive
       ‚Ä¢ Lightweight clusters may not need full Hive infrastructure
       ‚Ä¢ Makes code less portable across different environments

    5. RESOURCE USAGE:
       ‚Ä¢ Hive metastore queries add latency
       ‚Ä¢ Extra network calls to metastore for metadata
       ‚Ä¢ If you only need temporary Spark tables, Hive is slower
       ‚Ä¢ Spark's in-memory catalog is faster for ephemeral data

    WHEN TO USE HIVE:
    ‚Ä¢ Need persistent tables across sessions
    ‚Ä¢ Sharing data with other Hive/SQL users
    ‚Ä¢ Enterprise data warehouse integration
    ‚Ä¢ Complex schema management requirements

    WHEN NOT TO USE HIVE:
    ‚Ä¢ One-off data processing jobs
    ‚Ä¢ Local development and testing
    ‚Ä¢ Temporary transformations
    ‚Ä¢ Simple ETL without persistent storage

    WHAT IS HIVE?
    =============
    Apache Hive is a data warehouse system built on top of Hadoop that provides:
    ‚Ä¢ SQL interface (HiveQL) for querying data
    ‚Ä¢ Translation of SQL ‚Üí MapReduce/Tez/Spark jobs
    ‚Ä¢ Schema-on-read (flexible data structures)
    ‚Ä¢ Metastore (centralized metadata catalog)
    ‚Ä¢ Integration with Hadoop ecosystem

    KEY CONCEPT: SQL-to-MapReduce Translation
    =========================================
    HiveQL Query:
    SELECT department, AVG(salary)
    FROM employees
    WHERE year = 2024
    GROUP BY department;

    Hive translates this to:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Step 1: MAP Phase                       ‚îÇ
    ‚îÇ ‚Ä¢ Read employees data from HDFS         ‚îÇ
    ‚îÇ ‚Ä¢ Filter: year = 2024                   ‚îÇ
    ‚îÇ ‚Ä¢ Emit: (department, salary)            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Step 2: SHUFFLE & SORT                  ‚îÇ
    ‚îÇ ‚Ä¢ Group by department                   ‚îÇ
    ‚îÇ ‚Ä¢ Engineering: [50K, 60K, 55K]         ‚îÇ
    ‚îÇ ‚Ä¢ Sales: [45K, 48K]                    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Step 3: REDUCE Phase                    ‚îÇ
    ‚îÇ ‚Ä¢ Calculate AVG per department          ‚îÇ
    ‚îÇ ‚Ä¢ Engineering: 55K                      ‚îÇ
    ‚îÇ ‚Ä¢ Sales: 46.5K                         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    NO PROGRAMMING REQUIRED - Just write SQL!

    Returns:
        SparkSession with Hive support
    """
    print("\n" + "=" * 70)
    print("CREATING HIVE-ENABLED SPARK SESSION")
    print("=" * 70)

    spark = (
        SparkSession.builder.appName("Hive Integration Example")
        .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse")
        .config("spark.sql.catalogImplementation", "hive")
        .enableHiveSupport()
        .getOrCreate()
    )

    print("\n‚úÖ Spark session created with Hive support")
    print(f"   Warehouse location: {spark.conf.get('spark.sql.warehouse.dir')}")
    print(f"   Catalog: {spark.conf.get('spark.sql.catalogImplementation')}")

    return spark


def example_1_basic_hive_table_operations(spark):
    """
    Demonstrate basic Hive table operations.

    HIVE TABLE TYPES:
    ================
    1. MANAGED TABLES (Internal):
       ‚Ä¢ Hive manages both data and metadata
       ‚Ä¢ DROP TABLE deletes data and metadata
       ‚Ä¢ Stored in Hive warehouse directory

    2. EXTERNAL TABLES:
       ‚Ä¢ Hive manages only metadata
       ‚Ä¢ DROP TABLE deletes only metadata (data preserved)
       ‚Ä¢ Data stored in user-specified location

    SCHEMA-ON-READ:
    ==============
    Traditional databases: Schema-on-write (enforce schema on insert)
    Hive: Schema-on-read (apply schema when querying)

    Benefit: Flexibility!
    ‚Ä¢ Store raw data in any format
    ‚Ä¢ Define schema later
    ‚Ä¢ Multiple schemas for same data

    Example:
    File: /data/logs.txt (CSV)
    timestamp,user,action,duration
    2024-01-01,Alice,login,10
    2024-01-01,Bob,purchase,30

    Schema 1 (Analysis):
    CREATE TABLE user_actions (ts STRING, user STRING, action STRING, duration INT);

    Schema 2 (Audit):
    CREATE TABLE audit_log (timestamp STRING, username STRING, event STRING);

    Same data, different schemas - applied at read time!
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 1: Basic Hive Table Operations")
    print("=" * 70)

    # Create database
    print("\nüìÅ Creating Hive database...")
    spark.sql("CREATE DATABASE IF NOT EXISTS company")
    spark.sql("USE company")
    print("   ‚úÖ Database 'company' created")

    # Show current database
    current_db = spark.sql("SELECT current_database()").collect()[0][0]
    print(f"   Current database: {current_db}")

    # Create sample employee data
    print("\nüìä Creating employee dataset...")
    employees_data = [
        (1, "Alice", "Engineering", 50000, 30, 2024),
        (2, "Bob", "Sales", 45000, 35, 2024),
        (3, "Charlie", "Engineering", 60000, 28, 2024),
        (4, "Diana", "Marketing", 48000, 32, 2024),
        (5, "Eve", "Engineering", 55000, 29, 2024),
        (6, "Frank", "Sales", 47000, 33, 2024),
        (7, "Grace", "Engineering", 62000, 27, 2024),
        (8, "Henry", "Marketing", 50000, 31, 2024),
    ]

    employees_df = spark.createDataFrame(
        employees_data, ["id", "name", "department", "salary", "age", "year"]
    )

    print(f"   Created {employees_df.count()} employee records")
    employees_df.show()

    # Create managed table
    print("\nüíæ Creating MANAGED table...")
    employees_df.write.mode("overwrite").saveAsTable("employees")
    print("   ‚úÖ Managed table 'employees' created")

    # Query table using HiveQL
    print("\nüîç Querying with HiveQL (SQL-like syntax)...")
    print("   Query: SELECT department, AVG(salary) GROUP BY department")

    result = spark.sql(
        """
        SELECT department, 
               ROUND(AVG(salary), 2) as avg_salary,
               COUNT(*) as employee_count
        FROM employees
        GROUP BY department
        ORDER BY avg_salary DESC
    """
    )
    result.show()

    # Show table metadata
    print("\nüìã Table metadata:")
    spark.sql("DESCRIBE FORMATTED employees").show(50, truncate=False)

    # Create partitioned table
    print("\nüìÇ Creating PARTITIONED table...")
    print("   Partitioning by 'department' for efficient queries")

    employees_df.write.mode("overwrite").partitionBy("department").saveAsTable(
        "employees_partitioned"
    )

    print("   ‚úÖ Partitioned table created")
    print("\n   Directory structure:")
    print("   /warehouse/company.db/employees_partitioned/")
    print("   ‚îú‚îÄ‚îÄ department=Engineering/")
    print("   ‚îÇ   ‚îî‚îÄ‚îÄ part-00000.snappy.parquet")
    print("   ‚îú‚îÄ‚îÄ department=Sales/")
    print("   ‚îÇ   ‚îî‚îÄ‚îÄ part-00000.snappy.parquet")
    print("   ‚îî‚îÄ‚îÄ department=Marketing/")
    print("       ‚îî‚îÄ‚îÄ part-00000.snappy.parquet")

    # Query partitioned table
    print("\nüöÄ Querying partitioned table (faster!)...")
    print("   Query: SELECT * WHERE department = 'Engineering'")
    print("   Benefit: Only reads Engineering partition!")

    eng_result = spark.sql(
        """
        SELECT * FROM employees_partitioned 
        WHERE department = 'Engineering'
    """
    )
    eng_result.show()

    # Show all databases
    print("\nüóÇÔ∏è  All databases:")
    spark.sql("SHOW DATABASES").show()

    # Show all tables
    print("\nüìã All tables in 'company' database:")
    spark.sql("SHOW TABLES IN company").show()


def example_2_schema_on_read_flexibility(spark):
    """
    Demonstrate Hive's schema-on-read flexibility.

    SCHEMA-ON-READ vs SCHEMA-ON-WRITE:
    ==================================

    SCHEMA-ON-WRITE (Traditional databases):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ INSERT INTO users VALUES (...)          ‚îÇ
    ‚îÇ          ‚Üì                              ‚îÇ
    ‚îÇ Validate schema (fail if mismatch)      ‚îÇ
    ‚îÇ          ‚Üì                              ‚îÇ
    ‚îÇ Write to disk                           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Problem: Must know schema upfront, hard to change

    SCHEMA-ON-READ (Hive):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Store raw data (any format)             ‚îÇ
    ‚îÇ          ‚Üì                              ‚îÇ
    ‚îÇ Define schema when reading              ‚îÇ
    ‚îÇ          ‚Üì                              ‚îÇ
    ‚îÇ Apply schema to data                    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Benefit: Flexibility! Change schema anytime.

    REAL-WORLD SCENARIO:
    ===================
    You have log files from 3 systems with different formats:

    System A logs: timestamp,user,action
    System B logs: time,username,event,status
    System C logs: date,user_id,activity,duration

    Solution: Store all as-is, define schemas when querying!
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 2: Schema-on-Read Flexibility")
    print("=" * 70)

    spark.sql("USE company")

    # Create external table from CSV
    print("\nüìÑ Creating EXTERNAL table from CSV...")

    # Write sample CSV data
    csv_data = [
        ("2024-01-01", "Alice", "login", 10),
        ("2024-01-01", "Bob", "purchase", 30),
        ("2024-01-02", "Alice", "logout", 5),
        ("2024-01-02", "Charlie", "browse", 120),
    ]

    logs_df = spark.createDataFrame(
        csv_data, ["timestamp", "user", "action", "duration"]
    )

    # Write as CSV
    csv_path = "/tmp/raw_logs"
    logs_df.write.mode("overwrite").option("header", "true").csv(csv_path)
    print(f"   ‚úÖ CSV data written to {csv_path}")

    # Create external table (schema defined, data stays in original location)
    print("\nüîó Creating external table pointing to CSV...")

    spark.sql(
        f"""
        CREATE EXTERNAL TABLE IF NOT EXISTS user_logs (
            timestamp STRING,
            user STRING,
            action STRING,
            duration INT
        )
        ROW FORMAT DELIMITED
        FIELDS TERMINATED BY ','
        STORED AS TEXTFILE
        LOCATION '{csv_path}'
        TBLPROPERTIES ('skip.header.line.count'='1')
    """
    )

    print("   ‚úÖ External table 'user_logs' created")
    print("   Note: Data stays at original location")

    # Query external table
    print("\nüîç Querying external table...")
    spark.sql("SELECT * FROM user_logs").show()

    # Now change schema interpretation (schema-on-read!)
    print("\nüîÑ Creating ALTERNATE schema for same data...")
    print("   Same data, different interpretation!")

    spark.sql(
        f"""
        CREATE EXTERNAL TABLE IF NOT EXISTS audit_trail (
            event_time STRING,
            username STRING,
            event_type STRING,
            event_duration_seconds INT
        )
        ROW FORMAT DELIMITED
        FIELDS TERMINATED BY ','
        STORED AS TEXTFILE
        LOCATION '{csv_path}'
        TBLPROPERTIES ('skip.header.line.count'='1')
    """
    )

    print("   ‚úÖ Alternate table 'audit_trail' created")
    print("   Note: Same data, different column names!")

    # Query alternate schema
    print("\nüìä Querying with alternate schema...")
    spark.sql("SELECT * FROM audit_trail").show()

    # Drop external table (data preserved)
    print("\nüóëÔ∏è  Dropping external table...")
    spark.sql("DROP TABLE IF EXISTS audit_trail")
    print("   ‚úÖ Table dropped, but CSV data still exists!")

    # Verify data still exists
    print("\n‚úÖ Verifying original CSV data still exists...")
    df_check = spark.read.option("header", "true").csv(csv_path)
    print(f"   Records in CSV: {df_check.count()}")

    print("\nüí° KEY INSIGHT:")
    print("   External tables: DROP TABLE only removes metadata")
    print("   Managed tables: DROP TABLE removes data AND metadata")


def example_3_hive_query_execution(spark):
    """
    Demonstrate how Hive translates SQL to execution plans.

    HIVE EXECUTION ENGINES:
    ======================
    1. MapReduce (Original):
       ‚Ä¢ Slower (writes intermediate data to disk)
       ‚Ä¢ Good for batch processing
       ‚Ä¢ Fault-tolerant

    2. Apache Tez (Faster):
       ‚Ä¢ DAG-based execution (Directed Acyclic Graph)
       ‚Ä¢ In-memory intermediate data
       ‚Ä¢ 5-10√ó faster than MapReduce

    3. Spark (Fastest):
       ‚Ä¢ In-memory computation
       ‚Ä¢ Optimized query planning
       ‚Ä¢ 10-100√ó faster than MapReduce

    QUERY EXECUTION FLOW:
    ====================
    User writes HiveQL:
    SELECT department, AVG(salary)
    FROM employees
    WHERE year = 2024
    GROUP BY department;

    Step 1: PARSE
    ‚îú‚îÄ Parse SQL into Abstract Syntax Tree (AST)
    ‚îî‚îÄ Validate syntax

    Step 2: ANALYZE
    ‚îú‚îÄ Check table exists in metastore
    ‚îú‚îÄ Check column names valid
    ‚îî‚îÄ Infer data types

    Step 3: OPTIMIZE
    ‚îú‚îÄ Predicate pushdown (filter early)
    ‚îú‚îÄ Column pruning (read only needed columns)
    ‚îî‚îÄ Join optimization

    Step 4: EXECUTE
    ‚îú‚îÄ Generate MapReduce/Tez/Spark job
    ‚îú‚îÄ Submit to YARN
    ‚îî‚îÄ Monitor execution

    Step 5: RETURN RESULTS
    ‚îî‚îÄ Collect results and display
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 3: Hive Query Execution")
    print("=" * 70)

    spark.sql("USE company")

    # Complex query demonstrating SQL-to-execution translation
    print("\nüîç Complex analytical query...")
    print("   Hive translates this SQL to optimized execution plan:")

    query = """
        SELECT 
            department,
            COUNT(*) as employee_count,
            ROUND(AVG(salary), 2) as avg_salary,
            MIN(salary) as min_salary,
            MAX(salary) as max_salary,
            ROUND(STDDEV(salary), 2) as salary_stddev
        FROM employees
        WHERE year = 2024 AND salary > 40000
        GROUP BY department
        HAVING employee_count > 1
        ORDER BY avg_salary DESC
    """

    print(f"\n   Query:\n{query}")

    # Show execution plan
    print("\nüìã EXECUTION PLAN:")
    print("   (What Hive does behind the scenes)")
    spark.sql(f"EXPLAIN EXTENDED {query}").show(truncate=False)

    # Execute query
    print("\n‚ö° Executing query...")
    start_time = time.time()
    result = spark.sql(query)
    result.show()
    execution_time = time.time() - start_time

    print(f"\n   ‚è±Ô∏è  Execution time: {execution_time:.3f} seconds")
    print("   Note: Hive handled all the complexity!")

    # Show query stages
    print("\nüéØ WHAT HIVE DID:")
    print(
        """
   1. FILTER: WHERE year = 2024 AND salary > 40000
      ‚îî‚îÄ Read only needed rows (predicate pushdown)
   
   2. PROJECT: SELECT department, salary
      ‚îî‚îÄ Read only needed columns (column pruning)
   
   3. AGGREGATE: GROUP BY department
      ‚îî‚îÄ Calculate COUNT, AVG, MIN, MAX, STDDEV
   
   4. FILTER: HAVING employee_count > 1
      ‚îî‚îÄ Filter aggregated results
   
   5. SORT: ORDER BY avg_salary DESC
      ‚îî‚îÄ Sort final results
   
   No MapReduce programming needed - just SQL!
    """
    )


def example_4_batch_processing_patterns(spark):
    """
    Demonstrate Hive batch processing and ETL patterns.

    BATCH PROCESSING:
    ================
    Hive is optimized for batch processing:
    ‚Ä¢ Process large volumes of data
    ‚Ä¢ Not real-time (latency: seconds to hours)
    ‚Ä¢ Cost-effective for big data

    COMMON BATCH PATTERNS:
    =====================
    1. Daily Aggregation:
       ‚Ä¢ Summarize yesterday's transactions
       ‚Ä¢ Generate daily reports

    2. ETL (Extract, Transform, Load):
       ‚Ä¢ Extract: Read from source systems
       ‚Ä¢ Transform: Clean, join, aggregate
       ‚Ä¢ Load: Write to target warehouse

    3. Data Summarization:
       ‚Ä¢ Hourly ‚Üí Daily ‚Üí Monthly rollups
       ‚Ä¢ Dimension tables for OLAP

    4. Historical Analysis:
       ‚Ä¢ Year-over-year comparisons
       ‚Ä¢ Trend analysis
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 4: Batch Processing Patterns")
    print("=" * 70)

    spark.sql("USE company")

    # Create fact table (large, transactional)
    print("\nüìä Creating fact table (sales transactions)...")

    sales_data = []
    for i in range(1, 101):
        sales_data.append(
            (
                i,
                f"2024-01-{(i % 30) + 1:02d}",
                f"Product_{i % 10}",
                i % 8 + 1,  # employee_id
                (i * 37) % 500 + 100,  # amount
                2024,
            )
        )

    sales_df = spark.createDataFrame(
        sales_data, ["sale_id", "sale_date", "product", "employee_id", "amount", "year"]
    )

    print(f"   Created {sales_df.count()} sales records")

    # Write partitioned by year and month
    sales_df_with_month = sales_df.withColumn(
        "month", substring(col("sale_date"), 6, 2)
    )

    sales_df_with_month.write.mode("overwrite").partitionBy(
        "year", "month"
    ).saveAsTable("sales_fact")

    print("   ‚úÖ Fact table 'sales_fact' created (partitioned by year, month)")

    # ETL Pattern 1: Daily aggregation
    print("\nüìà ETL Pattern 1: Daily Sales Aggregation")

    daily_agg = spark.sql(
        """
        SELECT 
            sale_date,
            COUNT(*) as transaction_count,
            ROUND(SUM(amount), 2) as total_sales,
            ROUND(AVG(amount), 2) as avg_sale
        FROM sales_fact
        WHERE year = 2024
        GROUP BY sale_date
        ORDER BY sale_date
    """
    )

    print("\n   Creating 'daily_sales_summary' table...")
    daily_agg.write.mode("overwrite").saveAsTable("daily_sales_summary")
    print("   ‚úÖ Daily summary created")
    daily_agg.show(10)

    # ETL Pattern 2: Join with dimension table
    print("\nüîó ETL Pattern 2: Join Fact with Dimension")

    enriched = spark.sql(
        """
        SELECT 
            s.sale_date,
            e.name as employee_name,
            e.department,
            s.product,
            s.amount
        FROM sales_fact s
        JOIN employees e ON s.employee_id = e.id
        WHERE s.year = 2024
        ORDER BY s.sale_date, s.amount DESC
    """
    )

    print("\n   Enriched sales data (with employee info):")
    enriched.show(10)

    # ETL Pattern 3: Department performance
    print("\nüèÜ ETL Pattern 3: Department Performance Summary")

    dept_performance = spark.sql(
        """
        SELECT 
            e.department,
            COUNT(DISTINCT s.employee_id) as num_sellers,
            COUNT(*) as total_sales,
            ROUND(SUM(s.amount), 2) as revenue,
            ROUND(AVG(s.amount), 2) as avg_sale_amount
        FROM sales_fact s
        JOIN employees e ON s.employee_id = e.id
        GROUP BY e.department
        ORDER BY revenue DESC
    """
    )

    print("\n   Department performance:")
    dept_performance.show()

    # Create summary table
    dept_performance.write.mode("overwrite").saveAsTable("department_performance")
    print("\n   ‚úÖ 'department_performance' table created")

    print("\nÔøΩÔøΩ BATCH PROCESSING BENEFITS:")
    print("   ‚úÖ Process large volumes efficiently")
    print("   ‚úÖ Cost-effective (batch vs real-time)")
    print("   ‚úÖ Easy to write (just SQL)")
    print("   ‚úÖ Automatic optimization by Hive")
    print("   ‚úÖ Fault-tolerant execution")


def example_5_hive_accessibility(spark):
    """
    Demonstrate how Hive makes big data accessible to SQL users.

    WHO USES HIVE?
    =============
    1. Data Analysts:
       ‚Ä¢ Know SQL, not Java/Scala/Python
       ‚Ä¢ Need to query big data
       ‚Ä¢ Use familiar SQL syntax

    2. Business Analysts:
       ‚Ä¢ Generate reports
       ‚Ä¢ Create dashboards
       ‚Ä¢ Ad-hoc analysis

    3. Data Scientists:
       ‚Ä¢ Exploratory data analysis
       ‚Ä¢ Feature engineering
       ‚Ä¢ Model training data preparation

    4. BI Tools:
       ‚Ä¢ Tableau, Power BI connect to Hive
       ‚Ä¢ Query big data like a database
       ‚Ä¢ Drag-and-drop interface

    NO PROGRAMMING SKILLS REQUIRED:
    ==============================
    Instead of writing:

    // Java MapReduce (100+ lines)
    public class SalesMapper extends Mapper<...> {
        public void map(...) { ... }
    }
    public class SalesReducer extends Reducer<...> {
        public void reduce(...) { ... }
    }
    // Configure job, handle I/O, error handling, etc.

    Write this in Hive:

    SELECT department, SUM(amount)
    FROM sales
    GROUP BY department;

    That's it! 3 lines vs 100+ lines of code.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 5: Hive Accessibility")
    print("=" * 70)

    spark.sql("USE company")

    print("\nüë• HIVE DEMOCRATIZES BIG DATA:")
    print("   Anyone who knows SQL can analyze big data!")

    # Typical analyst queries
    print("\nüìä Common Analyst Queries:")

    queries = [
        (
            "Top performing employees",
            """
            SELECT e.name, e.department, COUNT(*) as sales_count, 
                   SUM(s.amount) as total_revenue
            FROM sales_fact s
            JOIN employees e ON s.employee_id = e.id
            GROUP BY e.name, e.department
            ORDER BY total_revenue DESC
            LIMIT 5
        """,
        ),
        (
            "Monthly trends",
            """
            SELECT month, 
                   COUNT(*) as transaction_count,
                   ROUND(SUM(amount), 2) as revenue
            FROM sales_fact
            WHERE year = 2024
            GROUP BY month
            ORDER BY month
        """,
        ),
        (
            "Product performance",
            """
            SELECT product, 
                   COUNT(*) as units_sold,
                   ROUND(AVG(amount), 2) as avg_price
            FROM sales_fact
            GROUP BY product
            ORDER BY units_sold DESC
        """,
        ),
    ]

    for title, query in queries:
        print(f"\n   {title.upper()}:")
        print(f"   {query.strip()}")
        result = spark.sql(query)
        result.show(5)

    # Show metadata (helpful for analysts)
    print("\nüîç HELPFUL FOR ANALYSTS:")
    print("   View available tables:")
    spark.sql("SHOW TABLES").show()

    print("\n   View table structure:")
    spark.sql("DESCRIBE employees").show()

    print("\n   View table statistics:")
    spark.sql(
        """
        SELECT 
            'employees' as table_name,
            COUNT(*) as row_count
        FROM employees
    """
    ).show()

    print("\nüí° KEY BENEFITS FOR NON-PROGRAMMERS:")
    print(
        """
   1. FAMILIAR SYNTAX:
      ‚Ä¢ Standard SQL (similar to MySQL, PostgreSQL)
      ‚Ä¢ No need to learn MapReduce/Spark programming
   
   2. INTERACTIVE ANALYSIS:
      ‚Ä¢ Write query, get results
      ‚Ä¢ No compilation, no deployment
   
   3. TOOL INTEGRATION:
      ‚Ä¢ Excel, Tableau, Power BI can connect
      ‚Ä¢ Use drag-and-drop interfaces
   
   4. DOCUMENTATION:
      ‚Ä¢ DESCRIBE tables
      ‚Ä¢ SHOW tables
      ‚Ä¢ View partitions, statistics
   
   5. SCALABILITY:
      ‚Ä¢ Same SQL works on 1 GB or 1 PB
      ‚Ä¢ Hive handles distribution automatically
    """
    )


def main():
    """
    Run all Hive integration examples.
    """
    print("\n" + "=" * 70)
    print(" HIVE INTEGRATION WITH PYSPARK - COMPLETE GUIDE ")
    print("=" * 70)

    print(
        """
Apache Hive makes big data analytics accessible by providing:

1. SQL Interface:       Write HiveQL (SQL-like) instead of MapReduce code
2. Schema-on-Read:      Flexible data structures, define schema at query time
3. Query Translation:   Automatically converts SQL to MapReduce/Tez/Spark jobs
4. Batch Processing:    Optimized for large-scale data processing
5. Accessibility:       No programming skills required - just SQL

This module demonstrates all key Hive concepts with working examples.
    """
    )

    try:
        # Create Hive-enabled Spark session
        spark = create_hive_enabled_spark_session()

        # Run all examples
        example_1_basic_hive_table_operations(spark)
        example_2_schema_on_read_flexibility(spark)
        example_3_hive_query_execution(spark)
        example_4_batch_processing_patterns(spark)
        example_5_hive_accessibility(spark)

        print("\n" + "=" * 70)
        print("‚úÖ ALL HIVE EXAMPLES COMPLETED SUCCESSFULLY")
        print("=" * 70)

        print("\nüìö KEY TAKEAWAYS:")
        print(
            """
   1. HIVE TRANSLATES SQL TO JOBS:
      ‚Ä¢ Write HiveQL (SQL-like queries)
      ‚Ä¢ Hive converts to MapReduce/Tez/Spark
      ‚Ä¢ No MapReduce programming needed
   
   2. SCHEMA-ON-READ FLEXIBILITY:
      ‚Ä¢ Store data in any format
      ‚Ä¢ Define schema when querying
      ‚Ä¢ Change schema without reprocessing
   
   3. BATCH PROCESSING:
      ‚Ä¢ Optimized for large-scale analytics
      ‚Ä¢ Cost-effective for big data
      ‚Ä¢ ETL and data warehouse patterns
   
   4. ACCESSIBILITY:
      ‚Ä¢ SQL users can analyze big data
      ‚Ä¢ Integrates with BI tools
      ‚Ä¢ No programming skills required
   
   5. HADOOP ECOSYSTEM:
      ‚Ä¢ Hive Metastore: Centralized metadata
      ‚Ä¢ HiveQL: SQL dialect
      ‚Ä¢ HDFS: Distributed storage
      ‚Ä¢ YARN: Resource management
        """
        )

        # Clean up
        print("\nüßπ Cleaning up demo databases...")
        spark.sql("DROP DATABASE IF EXISTS company CASCADE")
        print("   ‚úÖ Cleanup complete")

        spark.stop()

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
