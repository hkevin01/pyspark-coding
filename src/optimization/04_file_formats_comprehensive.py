"""
Comprehensive File Format Examples
===================================

WHAT THIS MODULE COVERS:
1. Delta Lake (ACID transactions, time travel)
2. ORC (Optimized Row Columnar for Hive)
3. Avro (Row-based with schema evolution)
4. JSON (Semi-structured data)
5. HDF5 (Scientific multi-dimensional arrays)

QUICK COMPARISON:
----------------
| Format      | Structure  | ACID | Time Travel | Streaming | Best Use Case           |
|-------------|------------|------|-------------|-----------|-------------------------|
| Delta Lake  | Columnar   | âœ…   | âœ…          | âœ…        | Data lake with ACID     |
| ORC         | Columnar   | âœ…*  | âŒ          | âŒ        | Hive warehouse          |
| Avro        | Row-based  | âŒ   | âŒ          | âœ…        | Kafka, schema evolution |
| JSON        | Nested     | âŒ   | âŒ          | âœ…        | APIs, semi-structured   |
| HDF5        | Array      | âŒ   | âŒ          | âŒ        | Scientific computing    |

* ORC ACID only in Hive context

FILE FORMAT SELECTION FLOWCHART:
--------------------------------
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Choose File Format                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Need ACID transactions or updates?      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    YES â”‚        â”‚ NO
                        â”‚        â”‚
                        â–¼        â–¼
                  Delta Lake   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚ Working with Hive ecosystem? â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     YES â”‚    â”‚ NO
                                         â”‚    â”‚
                                         â–¼    â–¼
                                       ORC  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Schema changes frequently?â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  YES â”‚    â”‚ NO
                                                      â”‚    â”‚
                                                      â–¼    â–¼
                                                   Avro  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                         â”‚ Nested/hierarchical?â”‚
                                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               YES â”‚  â”‚ NO
                                                                   â”‚  â”‚
                                                                   â–¼  â–¼
                                                                JSON Parquet
                                                                
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Scientific arrays/matrices?â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         YES â”‚
                                                             â–¼
                                                           HDF5

Author: PySpark Training
Date: 2024-12
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, current_timestamp, expr, struct, array,
    from_json, to_json, explode, rand, when
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, TimestampType, ArrayType
)
import time
import os


def create_spark_session():
    """
    Create Spark session with Delta Lake support.
    """
    return SparkSession.builder \
        .appName("FileFormats_Comprehensive") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()


def example_1_delta_lake_acid_transactions():
    """
    Demonstrate Delta Lake ACID transactions.
    
    WHAT ARE ACID TRANSACTIONS?
    ==========================
    ACID = Atomicity, Consistency, Isolation, Durability
    
    Database guarantees for reliable data operations:
    
    1. ATOMICITY (All or Nothing):
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Transaction: Transfer $100 from Account A â†’ B   â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚ Step 1: Subtract $100 from Account A            â”‚
       â”‚ Step 2: Add $100 to Account B                   â”‚
       â”‚                                                  â”‚
       â”‚ âœ… BOTH steps succeed â†’ Transaction commits     â”‚
       â”‚ âŒ ANY step fails â†’ ENTIRE transaction rolls backâ”‚
       â”‚    (No partial updates!)                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       
       WITHOUT ACID (Regular Parquet):
       â€¢ Write 1000 files
       â€¢ Crash after 500 files written
       â€¢ Result: Partial data, corrupt state
       
       WITH ACID (Delta Lake):
       â€¢ Write 1000 files + transaction log
       â€¢ Crash after 500 files written
       â€¢ Result: Transaction not committed, old version still valid
    
    2. CONSISTENCY (Rules Always Enforced):
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Rule: Age must be > 0                            â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚ Try to insert: {name: "Alice", age: -5}          â”‚
       â”‚ Result: âŒ REJECTED (violates constraint)        â”‚
       â”‚                                                  â”‚
       â”‚ Try to insert: {name: "Bob", age: 25}            â”‚
       â”‚ Result: âœ… ACCEPTED (satisfies constraint)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       
       WITHOUT ACID:
       â€¢ Schema enforcement weak or none
       â€¢ Data types can be wrong
       â€¢ Constraints not enforced
       
       WITH ACID:
       â€¢ Schema enforced on every write
       â€¢ Type checking automatic
       â€¢ Constraints validated
    
    3. ISOLATION (Concurrent Operations Don't Interfere):
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Timeline:                                        â”‚
       â”‚ T1: User A reads balance = $1000                 â”‚
       â”‚ T2: User B reads balance = $1000                 â”‚
       â”‚ T3: User A withdraws $100 â†’ writes $900          â”‚
       â”‚ T4: User B withdraws $200 â†’ writes $800          â”‚
       â”‚                                                  â”‚
       â”‚ WITHOUT ISOLATION:                               â”‚
       â”‚   Final balance = $800 (lost User A's update!)   â”‚
       â”‚                                                  â”‚
       â”‚ WITH ISOLATION:                                  â”‚
       â”‚   User B's write detects conflict, retries       â”‚
       â”‚   Final balance = $700 (both updates applied)    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       
       Delta Lake uses Optimistic Concurrency Control:
       â€¢ Each transaction reads current version
       â€¢ Before commit, checks if base version changed
       â€¢ If changed â†’ conflict, transaction retries
       â€¢ If same â†’ commit succeeds
    
    4. DURABILITY (Committed Data Never Lost):
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Transaction commits at 10:00 AM                  â”‚
       â”‚ Server crashes at 10:01 AM                       â”‚
       â”‚ Server restarts at 10:05 AM                      â”‚
       â”‚                                                  â”‚
       â”‚ Result: Transaction data still there! âœ…         â”‚
       â”‚ (Written to persistent storage before commit ack)â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       
       Delta Lake guarantees:
       â€¢ Transaction log written to storage (S3, HDFS)
       â€¢ Data files written before log commit
       â€¢ Log commit is atomic (single file rename)
       â€¢ Crash recovery: Read log, apply all committed transactions
    
    WHY ACID MATTERS FOR DATA LAKES:
    --------------------------------
    Traditional Data Lake (Parquet files):
    âŒ No atomicity: Partial writes leave corrupt data
    âŒ No consistency: Schema can drift
    âŒ No isolation: Concurrent writes = data loss
    âŒ No durability: No transaction log = no recovery
    
    Delta Lake:
    âœ… Atomicity: All-or-nothing writes
    âœ… Consistency: Schema enforced
    âœ… Isolation: Concurrent reads/writes safe
    âœ… Durability: Transaction log = complete audit trail
    
    DELTA LAKE FUNDAMENTALS:
    -----------------------
    Delta Lake = Parquet files + Transaction Log
    
    STRUCTURE:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Delta Lake Table Directory                          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                     â”‚
    â”‚ _delta_log/                                         â”‚
    â”‚ â”œâ”€â”€ 00000000000000000000.json  (Transaction 0)     â”‚
    â”‚ â”œâ”€â”€ 00000000000000000001.json  (Transaction 1)     â”‚
    â”‚ â”œâ”€â”€ 00000000000000000002.json  (Transaction 2)     â”‚
    â”‚ â””â”€â”€ 00000000000000000010.checkpoint.parquet         â”‚
    â”‚                                                     â”‚
    â”‚ part-00000-*.parquet  (Data file 1)                 â”‚
    â”‚ part-00001-*.parquet  (Data file 2)                 â”‚
    â”‚ part-00002-*.parquet  (Data file 3)                 â”‚
    â”‚ ...                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Transaction Log Entry Example:
    {
      "add": {
        "path": "part-00001.parquet",
        "size": 1024,
        "modificationTime": 1234567890,
        "dataChange": true,
        "stats": "{\"numRecords\":100,\"minValues\":{\"id\":1},...}"
      }
    }
    
    How Delta Lake Provides ACID:
    1. Atomicity: Log commit = atomic file operation
    2. Consistency: Schema tracked in log, enforced on write
    3. Isolation: Version numbers + optimistic concurrency
    4. Durability: Log persisted before acknowledging write
    
    File Structure:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ /path/to/delta_table/                          â”‚
    â”‚ â”œâ”€â”€ _delta_log/                                â”‚
    â”‚ â”‚   â”œâ”€â”€ 00000000000000000000.json  (V0)       â”‚
    â”‚ â”‚   â”œâ”€â”€ 00000000000000000001.json  (V1)       â”‚
    â”‚ â”‚   â”œâ”€â”€ 00000000000000000002.json  (V2)       â”‚
    â”‚ â”‚   â””â”€â”€ 00000000000000000010.checkpoint.parquetâ”‚
    â”‚ â”œâ”€â”€ part-00000-xxx.snappy.parquet              â”‚
    â”‚ â”œâ”€â”€ part-00001-xxx.snappy.parquet              â”‚
    â”‚ â””â”€â”€ part-00002-xxx.snappy.parquet              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Transaction Log tracks:
    â€¢ Add/remove files
    â€¢ Schema changes
    â€¢ Metadata updates
    â€¢ Commit information
    
    ACID PROPERTIES:
    ---------------
    â€¢ Atomicity: All-or-nothing writes
    â€¢ Consistency: Schema enforcement
    â€¢ Isolation: Serializable isolation
    â€¢ Durability: Write-ahead log
    """
    print("\n" + "="*70)
    print("EXAMPLE 1: Delta Lake ACID Transactions")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create initial dataset
    print("\nğŸ“Š Creating initial Delta table...")
    df_initial = spark.createDataFrame([
        (1, "Alice", 30, 50000.0, "Engineering"),
        (2, "Bob", 35, 60000.0, "Sales"),
        (3, "Charlie", 28, 55000.0, "Engineering"),
        (4, "Diana", 32, 58000.0, "Marketing")
    ], ["id", "name", "age", "salary", "department"])
    
    delta_path = "/tmp/delta_table"
    
    # Write as Delta table (Version 0)
    print(f"\nğŸ’¾ Writing initial version to {delta_path}")
    df_initial.write.format("delta").mode("overwrite").save(delta_path)
    
    print("\nğŸ“– Reading Delta table:")
    df_read = spark.read.format("delta").load(delta_path)
    df_read.show()
    
    # UPDATE operation (Version 1)
    print("\nğŸ”„ UPDATE: Giving Engineering 10% raise")
    print("   SQL: UPDATE table SET salary = salary * 1.1 WHERE department = 'Engineering'")
    
    from delta.tables import DeltaTable
    delta_table = DeltaTable.forPath(spark, delta_path)
    
    delta_table.update(
        condition=col("department") == "Engineering",
        set={"salary": col("salary") * 1.1}
    )
    
    print("\n   After UPDATE:")
    spark.read.format("delta").load(delta_path).show()
    
    # DELETE operation (Version 2)
    print("\nğŸ—‘ï¸  DELETE: Removing employees under 30")
    print("   SQL: DELETE FROM table WHERE age < 30")
    
    delta_table.delete(condition=col("age") < 30)
    
    print("\n   After DELETE:")
    spark.read.format("delta").load(delta_path).show()
    
    # MERGE (UPSERT) operation (Version 3)
    print("\nğŸ”€ MERGE: Upserting new employees")
    df_new = spark.createDataFrame([
        (2, "Bob", 36, 65000.0, "Sales"),  # Update existing
        (5, "Eve", 29, 52000.0, "Engineering")  # Insert new
    ], ["id", "name", "age", "salary", "department"])
    
    print("\n   New data:")
    df_new.show()
    
    delta_table.alias("target").merge(
        df_new.alias("source"),
        "target.id = source.id"
    ).whenMatchedUpdate(set={
        "name": col("source.name"),
        "age": col("source.age"),
        "salary": col("source.salary"),
        "department": col("source.department")
    }).whenNotMatchedInsert(values={
        "id": col("source.id"),
        "name": col("source.name"),
        "age": col("source.age"),
        "salary": col("source.salary"),
        "department": col("source.department")
    }).execute()
    
    print("\n   After MERGE:")
    spark.read.format("delta").load(delta_path).show()
    
    # Show version history
    print("\nğŸ“œ VERSION HISTORY:")
    history = delta_table.history()
    history.select("version", "timestamp", "operation", "operationMetrics").show(truncate=False)
    
    # Time travel - read old version
    print("\nâ° TIME TRAVEL: Reading version 0 (original data)")
    df_v0 = spark.read.format("delta").option("versionAsOf", 0).load(delta_path)
    df_v0.show()
    
    print("\n" + "="*70)
    print("DELTA LAKE vs PARQUET: WHAT'S THE REAL DIFFERENCE?")
    print("="*70)
    
    print("\nğŸ”´ OPERATIONS PARQUET **CANNOT** DO:")
    print("   " + "-"*60)
    print("\n   1. UPDATE (Modify existing rows):")
    print("      âŒ Parquet: Must rewrite ENTIRE file")
    print("      âœ… Delta Lake: Atomic UPDATE operation")
    print("\n      Example: Change 1 row in 1 million rows")
    print("      Parquet:    Rewrite all 1M rows (slow, expensive)")
    print("      Delta Lake: Track change in log (instant)")
    
    print("\n   2. DELETE (Remove rows):")
    print("      âŒ Parquet: Must rewrite file without deleted rows")
    print("      âœ… Delta Lake: Atomic DELETE operation")
    print("\n      Example: Delete inactive users")
    print("      Parquet:    Read all â†’ filter â†’ rewrite (hours)")
    print("      Delta Lake: DELETE WHERE active = false (seconds)")
    
    print("\n   3. MERGE/UPSERT (Insert or Update):")
    print("      âŒ Parquet: Complex manual logic required")
    print("         1. Read existing data")
    print("         2. Join with new data")
    print("         3. Deduplicate")
    print("         4. Overwrite (not atomic!)")
    print("      âœ… Delta Lake: Single MERGE command (atomic)")
    
    print("\n   4. TIME TRAVEL (Query old versions):")
    print("      âŒ Parquet: Keep copies manually (expensive)")
    print("         /data/2024-01-01/")
    print("         /data/2024-01-02/  â† Duplicate storage!")
    print("         /data/2024-01-03/")
    print("      âœ… Delta Lake: Automatic versioning (storage-efficient)")
    print("         Only stores changes, not full copies")
    
    print("\n   5. CONCURRENT WRITES (Multiple writers):")
    print("      âŒ Parquet: Race conditions!")
    print("         Writer 1: Overwrites file at 10:00 AM")
    print("         Writer 2: Overwrites file at 10:01 AM")
    print("         Result: Writer 1's data LOST!")
    print("      âœ… Delta Lake: Optimistic concurrency control")
    print("         Detects conflicts, retries automatically")
    
    print("\n   6. SCHEMA ENFORCEMENT:")
    print("      âŒ Parquet: Write any schema (causes errors later)")
    print("      âœ… Delta Lake: Rejects incompatible writes")
    
    print("\n   7. AUDIT TRAIL:")
    print("      âŒ Parquet: No history (who changed what?)")
    print("      âœ… Delta Lake: Full transaction log")
    
    # Demonstrate the practical difference with code
    print("\n" + "="*70)
    print("CONCRETE CODE COMPARISON")
    print("="*70)
    
    print("\nğŸ“ SCENARIO: Update employee salary")
    print("\n   WITH PARQUET (Manual, Complex, NOT ATOMIC):")
    print("   " + "-"*60)
    print("""
   # Step 1: Read all data
   df = spark.read.parquet("/tmp/parquet_table")
   
   # Step 2: Update in memory
   df_updated = df.withColumn(
       "salary",
       when(col("id") == 2, col("salary") + 5000)
       .otherwise(col("salary"))
   )
   
   # Step 3: Overwrite (NOT ATOMIC!)
   df_updated.write.mode("overwrite").parquet("/tmp/parquet_table")
   
   # Problems:
   # âŒ If crash during write â†’ corrupt data
   # âŒ Concurrent writers â†’ data loss
   # âŒ Must rewrite ALL data (slow for large tables)
   # âŒ No rollback if error
   # âŒ No audit trail
    """)
    
    print("\n   WITH DELTA LAKE (Simple, Fast, ATOMIC):")
    print("   " + "-"*60)
    print("""
   from delta.tables import DeltaTable
   
   # Single atomic operation
   delta_table = DeltaTable.forPath(spark, "/tmp/delta_table")
   delta_table.update(
       condition = "id = 2",
       set = {"salary": col("salary") + 5000}
   )
   
   # Benefits:
   # âœ… Atomic (all-or-nothing)
   # âœ… Concurrent safe
   # âœ… Only updates changed rows (fast)
   # âœ… Auto-rollback on error
   # âœ… Full audit trail
    """)
    
    print("\n" + "="*70)
    print("STORAGE DIFFERENCE")
    print("="*70)
    
    print("\n   PARQUET: Just data files")
    print("   " + "-"*40)
    print("""
   /tmp/parquet_table/
   â”œâ”€â”€ part-00000.snappy.parquet  (200 MB)
   â”œâ”€â”€ part-00001.snappy.parquet  (200 MB)
   â””â”€â”€ part-00002.snappy.parquet  (200 MB)
   
   Total: 600 MB
    """)
    
    print("\n   DELTA LAKE: Data files + Transaction log")
    print("   " + "-"*40)
    print("""
   /tmp/delta_table/
   â”œâ”€â”€ _delta_log/
   â”‚   â”œâ”€â”€ 00000000000000000000.json  (Transaction 0: Initial write)
   â”‚   â”œâ”€â”€ 00000000000000000001.json  (Transaction 1: UPDATE)
   â”‚   â”œâ”€â”€ 00000000000000000002.json  (Transaction 2: DELETE)
   â”‚   â””â”€â”€ 00000000000000000003.json  (Transaction 3: MERGE)
   â”œâ”€â”€ part-00000.snappy.parquet  (200 MB)
   â”œâ”€â”€ part-00001.snappy.parquet  (200 MB)
   â””â”€â”€ part-00002.snappy.parquet  (200 MB)
   
   Total: 600 MB (data) + 4 KB (transaction log)
   
   Transaction log overhead: ~0.001% â† Nearly zero!
    """)
    
    print("\n" + "="*70)
    print("WHEN TO USE EACH FORMAT")
    print("="*70)
    
    print("\n   USE PARQUET WHEN:")
    print("   " + "-"*40)
    print("   âœ… Write-once, read-many (immutable data)")
    print("   âœ… Single writer only")
    print("   âœ… No need for updates/deletes")
    print("   âœ… Maximum portability (works everywhere)")
    print("   âœ… Simpler stack (no Delta Lake dependency)")
    print("\n   Example: Historical logs, archives, ML training data")
    
    print("\n   USE DELTA LAKE WHEN:")
    print("   " + "-"*40)
    print("   âœ… Need UPDATE/DELETE operations")
    print("   âœ… Multiple concurrent writers")
    print("   âœ… Need ACID guarantees")
    print("   âœ… Need time travel (query old versions)")
    print("   âœ… Need audit trail (compliance)")
    print("   âœ… CDC (Change Data Capture) pipelines")
    print("   âœ… Real-time analytics with updates")
    print("\n   Example: Customer databases, inventory, financial transactions")
    
    print("\nğŸ’¡ KEY INSIGHT:")
    print("   " + "="*60)
    print("   Delta Lake IS Parquet + Transaction Log")
    print("   " + "="*60)
    print("   â€¢ Same columnar storage (Parquet)")
    print("   â€¢ Same compression (Snappy/ZSTD)")
    print("   â€¢ Same performance for reads")
    print("   â€¢ PLUS: ACID transactions")
    print("   â€¢ PLUS: Time travel")
    print("   â€¢ PLUS: Schema evolution")
    print("   â€¢ Cost: ~4 KB transaction log (negligible!)")


def example_2_orc_hive_ecosystem():
    """
    Demonstrate ORC format optimized for Hive.
    
    WHAT IS THE HIVE ECOSYSTEM?
    ===========================
    Hive is a data warehouse system built on top of Hadoop that provides:
    â€¢ SQL interface to query data stored in HDFS
    â€¢ Metastore (centralized schema registry)
    â€¢ Query execution engine
    â€¢ Integration with Hadoop ecosystem (YARN, HDFS, HBase)
    
    HIVE ECOSYSTEM COMPONENTS:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    Hive Architecture                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                         â”‚
    â”‚  1. Hive Metastore (Schema & Metadata)                  â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚     â”‚ Table: sales                           â”‚         â”‚
    â”‚     â”‚ Location: hdfs://warehouse/sales/      â”‚         â”‚
    â”‚     â”‚ Format: ORC                            â”‚         â”‚
    â”‚     â”‚ Partitions: year, month                â”‚         â”‚
    â”‚     â”‚ Columns: id, product, amount, date     â”‚         â”‚
    â”‚     â”‚ Statistics: row count, file sizes      â”‚         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                                                         â”‚
    â”‚  2. Query Interface (HiveQL - SQL-like)                 â”‚
    â”‚     SELECT product, SUM(amount)                         â”‚
    â”‚     FROM sales                                          â”‚
    â”‚     WHERE year = 2024                                   â”‚
    â”‚     GROUP BY product;                                   â”‚
    â”‚                                                         â”‚
    â”‚  3. Execution Engine                                    â”‚
    â”‚     â€¢ Translates SQL â†’ MapReduce/Tez/Spark jobs        â”‚
    â”‚     â€¢ Optimizes queries                                 â”‚
    â”‚     â€¢ Manages resources via YARN                        â”‚
    â”‚                                                         â”‚
    â”‚  4. Storage Layer (HDFS)                                â”‚
    â”‚     /warehouse/sales/year=2024/month=01/file1.orc       â”‚
    â”‚     /warehouse/sales/year=2024/month=02/file2.orc       â”‚
    â”‚                                                         â”‚
    â”‚  5. Integration Points                                  â”‚
    â”‚     â€¢ HBase: NoSQL database                             â”‚
    â”‚     â€¢ Kafka: Streaming ingestion                        â”‚
    â”‚     â€¢ Sqoop: RDBMS import/export                        â”‚
    â”‚     â€¢ Spark: Fast query engine                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    WHY ORC FOR HIVE?
    ----------------
    ORC was designed specifically for Hive:
    
    1. Hive Metastore Integration:
       â€¢ ORC stores statistics in file footer
       â€¢ Hive reads stats without scanning data
       â€¢ Faster query planning
    
    2. ACID Support in Hive:
       â€¢ ORC files support INSERT, UPDATE, DELETE
       â€¢ Row-level modifications tracked
       â€¢ Hive manages transaction log
    
    3. Predicate Pushdown:
       â€¢ ORC: Min/max in footer + bloom filters
       â€¢ Hive optimizer uses stats to skip files
       â€¢ Faster than Parquet for some queries
    
    4. Compression:
       â€¢ ORC: Slightly better compression than Parquet
       â€¢ Uses ZLIB by default (Parquet uses Snappy)
       â€¢ 5-10% smaller files
    
    5. Hive-Specific Optimizations:
       â€¢ Vectorized query execution
       â€¢ Dictionary encoding optimized for Hive
       â€¢ Direct integration with Hive operators
    
    HIVE ECOSYSTEM USE CASES:
    ------------------------
    âœ… Enterprise data warehouse on Hadoop
    âœ… Batch processing of large datasets
    âœ… Integration with existing Hadoop infrastructure
    âœ… Need ACID transactions (INSERT/UPDATE/DELETE)
    âœ… Centralized schema management (Metastore)
    âœ… SQL interface for analysts (HiveQL)
    
    âŒ Real-time/streaming (use Kafka + Spark)
    âŒ Cloud-native (use Parquet + Delta Lake)
    âŒ Machine learning (use Parquet for portability)
    
    ORC (Optimized Row Columnar) Format:
    -----------------------------------
    Similar to Parquet but optimized for Hive ecosystem.
    
    Structure:
    
    spark = create_spark_session()
    
    # Create dataset
    print("\nğŸ“Š Creating sample dataset...")
    df = spark.range(1, 100001) \
        .withColumn("name", expr("concat('User_', id)")) \
        .withColumn("age", (rand() * 50 + 20).cast("int")) \
        .withColumn("salary", (rand() * 100000 + 30000).cast("double")) \
        .withColumn("active", when(rand() > 0.3, True).otherwise(False))
    
    print(f"   Rows: {df.count():,}")
    df.show(5)
    
    # Write as ORC
    print("\nğŸ’¾ Writing as ORC...")
    orc_path = "/tmp/data.orc"
    start_time = time.time()
    df.write.mode("overwrite").orc(orc_path)
    orc_write_time = time.time() - start_time
    print(f"   Write time: {orc_write_time:.2f} seconds")
    
    # Compare with Parquet
    print("\nğŸ’¾ Writing as Parquet (for comparison)...")
    parquet_path = "/tmp/data_compare.parquet"
    start_time = time.time()
    df.write.mode("overwrite").parquet(parquet_path)
    parquet_write_time = time.time() - start_time
    print(f"   Write time: {parquet_write_time:.2f} seconds")
    
    # Compare file sizes
    print("\nğŸ“ FILE SIZE COMPARISON:")
    import subprocess
    orc_size = subprocess.check_output(f"du -sh {orc_path}", shell=True).decode().split()[0]
    parquet_size = subprocess.check_output(f"du -sh {parquet_path}", shell=True).decode().split()[0]
    print(f"   ORC:     {orc_size}")
    print(f"   Parquet: {parquet_size}")
    print("   â†’ ORC typically 10-15% smaller due to better compression")
    
    # Read performance
    print("\nâ±ï¸  READ PERFORMANCE:")
    
    # ORC read
    start_time = time.time()
    df_orc = spark.read.orc(orc_path)
    orc_count = df_orc.filter(col("age") > 40).count()
    orc_read_time = time.time() - start_time
    print(f"   ORC read + filter:     {orc_read_time:.2f} seconds ({orc_count:,} rows)")
    
    # Parquet read
    start_time = time.time()
    df_parquet = spark.read.parquet(parquet_path)
    parquet_count = df_parquet.filter(col("age") > 40).count()
    parquet_read_time = time.time() - start_time
    print(f"   Parquet read + filter: {parquet_read_time:.2f} seconds ({parquet_count:,} rows)")
    
    # ORC compression codecs
    print("\nğŸ—œï¸  ORC COMPRESSION CODECS:")
    codecs = ["NONE", "ZLIB", "SNAPPY", "LZ4", "ZSTD"]
    
    for codec in codecs:
        path = f"/tmp/orc_{codec.lower()}"
        df.limit(10000).write.mode("overwrite") \
            .option("compression", codec) \
            .orc(path)
        size = subprocess.check_output(f"du -sh {path}", shell=True).decode().split()[0]
        print(f"   {codec:8s}: {size}")
    
    print("\nğŸ’¡ WHEN TO USE ORC:")
    print("   âœ… Working with Hive ecosystem")
    print("   âœ… Need slightly better compression than Parquet")
    print("   âœ… ACID transactions in Hive")
    print("   âœ… Hive metastore integration")
    print("   âŒ Use Parquet for non-Hive systems (more universal)")


def example_3_avro_schema_evolution():
    """
    Demonstrate Avro format with schema evolution.
    
    WHY ARE SCHEMA CHANGES FREQUENT?
    ================================
    In modern software development, schemas change constantly:
    
    1. API EVOLUTION:
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ API Version 1 (January 2024)                â”‚
       â”‚ {                                           â”‚
       â”‚   "user_id": 123,                           â”‚
       â”‚   "name": "Alice",                          â”‚
       â”‚   "email": "alice@example.com"              â”‚
       â”‚ }                                           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ API Version 2 (March 2024)                  â”‚
       â”‚ {                                           â”‚
       â”‚   "user_id": 123,                           â”‚
       â”‚   "name": "Alice",                          â”‚
       â”‚   "email": "alice@example.com",             â”‚
       â”‚   "phone": "+1-555-0123",      â† NEW FIELD  â”‚
       â”‚   "country": "USA"              â† NEW FIELD  â”‚
       â”‚ }                                           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    2. BUSINESS REQUIREMENTS:
       â€¢ "We need to track customer age for marketing campaigns"
         â†’ Add 'age' column to customer table
       
       â€¢ "GDPR compliance: delete user addresses"
         â†’ Remove 'address' fields from schema
       
       â€¢ "Rename 'ssn' to 'national_id' for international customers"
         â†’ Field rename with alias
    
    3. AGILE DEVELOPMENT:
       Week 1: Launch MVP with basic fields
       Week 2: Add analytics fields (click_count, session_duration)
       Week 3: Add A/B testing fields (experiment_id, variant)
       Week 4: Add recommendation fields (recommended_items)
       
       Result: Schema changes EVERY WEEK
    
    4. DATA INTEGRATION:
       â€¢ Merge data from acquisition: New company has extra fields
       â€¢ Add new data source: Different schema structure
       â€¢ External API changes: Must adapt to their schema updates
    
    5. MACHINE LEARNING EVOLUTION:
       â€¢ Model V1: Uses 10 features
       â€¢ Model V2: Adds 5 new features (requires new columns)
       â€¢ Model V3: Removes 3 irrelevant features
    
    THE SCHEMA EVOLUTION PROBLEM:
    ============================
    WITHOUT schema evolution support:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Problem: Schema changed, old data incompatible â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Old data (Jan-Feb):                            â”‚
    â”‚   Parquet files with schema V1                 â”‚
    â”‚   âŒ Can't read with V2 schema                 â”‚
    â”‚                                                â”‚
    â”‚ New data (Mar):                                â”‚
    â”‚   Parquet files with schema V2                 â”‚
    â”‚   âŒ Can't read with V1 schema                 â”‚
    â”‚                                                â”‚
    â”‚ Solution: REPROCESS ALL HISTORICAL DATA        â”‚
    â”‚ Cost: $50,000+ in compute time                 â”‚
    â”‚ Time: 3 days to reprocess 100 TB               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    WITH schema evolution (Avro):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Solution: Read all data together!              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Old data (Jan-Feb):                            â”‚
    â”‚   Avro files with schema V1                    â”‚
    â”‚   âœ… Read with V2 schema (missing fields = null)â”‚
    â”‚                                                â”‚
    â”‚ New data (Mar):                                â”‚
    â”‚   Avro files with schema V2                    â”‚
    â”‚   âœ… Read with V1 schema (extra fields ignored) â”‚
    â”‚                                                â”‚
    â”‚ Solution: NO REPROCESSING NEEDED               â”‚
    â”‚ Cost: $0                                       â”‚
    â”‚ Time: Instant                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    AVRO SCHEMA EVOLUTION RULES:
    ---------------------------
    âœ… BACKWARD COMPATIBLE (New code reads old data):
       â€¢ Add optional field with default value
       â€¢ Delete field
    
    âœ… FORWARD COMPATIBLE (Old code reads new data):
       â€¢ Add field (old code ignores it)
       â€¢ Delete optional field
    
    âœ… FULL COMPATIBLE (Both directions):
       â€¢ Add optional field with default
       â€¢ Delete optional field
    
    âŒ BREAKING CHANGES (Not compatible):
       â€¢ Change field type (int â†’ string)
       â€¢ Rename without alias
       â€¢ Remove required field
       â€¢ Add required field without default
    
    AVRO FUNDAMENTALS:
    -----------------
    Row-based binary format with embedded schema.
    
    Avro File Structure:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Header                                 â”‚
    â”‚ â”œâ”€ Magic bytes (Obj\x01)             â”‚
    â”‚ â”œâ”€ Schema (JSON)                      â”‚
    â”‚ â””â”€ Codec (compression)                â”‚
    â”‚                                        â”‚
    â”‚ Data Block 1                           â”‚
    â”‚ â”œâ”€ Row 1 (binary)                     â”‚
    â”‚ â”œâ”€ Row 2 (binary)                     â”‚
    â”‚ â””â”€ Sync marker                        â”‚
    â”‚                                        â”‚
    â”‚ Data Block 2                           â”‚
    â”‚ ...                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Use Cases:
    â€¢ Kafka message serialization
    â€¢ Streaming data with schema changes
    â€¢ RPC systems (Apache protocols)
    â€¢ APIs with frequent updates
    """
    print("\n" + "="*70)
    print("EXAMPLE 3: Avro Schema Evolution")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create initial dataset (Version 1 schema)
    print("\nğŸ“Š Creating dataset with V1 schema...")
    df_v1 = spark.createDataFrame([
        (1, "Alice", 30),
        (2, "Bob", 35),
        (3, "Charlie", 28)
    ], ["id", "name", "age"])
    
    print("\n   V1 Schema:")
    df_v1.printSchema()
    df_v1.show()
    
    # Write as Avro
    avro_path_v1 = "/tmp/avro_v1"
    print(f"\nğŸ’¾ Writing V1 as Avro to {avro_path_v1}")
    df_v1.write.format("avro").mode("overwrite").save(avro_path_v1)
    
    # Evolve schema - add new field (Version 2)
    print("\nğŸ”„ Evolving schema - adding 'salary' field")
    df_v2 = spark.createDataFrame([
        (4, "Diana", 32, 58000.0),
        (5, "Eve", 29, 52000.0)
    ], ["id", "name", "age", "salary"])
    
    print("\n   V2 Schema (with salary):")
    df_v2.printSchema()
    df_v2.show()
    
    avro_path_v2 = "/tmp/avro_v2"
    print(f"\nğŸ’¾ Writing V2 as Avro to {avro_path_v2}")
    df_v2.write.format("avro").mode("overwrite").save(avro_path_v2)
    
    # Read both versions together (schema evolution)
    print("\nğŸ“– Reading both V1 and V2 together (schema evolution)...")
    df_combined = spark.read.format("avro").load(avro_path_v1, avro_path_v2)
    
    print("\n   Combined data (V1 has null salary):")
    df_combined.orderBy("id").show()
    
    # Demonstrate Avro for streaming (Kafka use case simulation)
    print("\nğŸ“¡ AVRO FOR STREAMING (Kafka simulation):")
    print("""
   Avro is popular for Kafka because:
   
   1. Compact binary format (smaller than JSON)
   2. Schema registry integration (centralized schema management)
   3. Schema evolution (backward/forward compatible)
   4. Fast serialization/deserialization
   
   Workflow:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Producer â”‚â”€â”€â”€â”€â–¶â”‚ Kafka + Schema â”‚â”€â”€â”€â”€â–¶â”‚ Consumer â”‚
   â”‚          â”‚     â”‚    Registry    â”‚     â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                    â”‚
        â–¼                   â–¼                    â–¼
   Register schema    Store schema V1       Read with
   V1 â†’ get ID       Store schema V2       compatible schema
                                           (V1 or V2)
    """)
    
    # Compare Avro vs Parquet file sizes
    print("\nğŸ“ AVRO vs PARQUET SIZE:")
    df_test = spark.range(1, 50001) \
        .withColumn("value", rand() * 1000)
    
    avro_test_path = "/tmp/avro_test"
    parquet_test_path = "/tmp/parquet_test"
    
    df_test.write.format("avro").mode("overwrite").save(avro_test_path)
    df_test.write.mode("overwrite").parquet(parquet_test_path)
    
    import subprocess
    avro_size = subprocess.check_output(f"du -sh {avro_test_path}", shell=True).decode().split()[0]
    parquet_size = subprocess.check_output(f"du -sh {parquet_test_path}", shell=True).decode().split()[0]
    
    print(f"   Avro:    {avro_size} (row-based, good compression)")
    print(f"   Parquet: {parquet_size} (columnar, excellent compression)")
    
    print("\nğŸ’¡ WHEN TO USE AVRO:")
    print("   âœ… Kafka streaming (industry standard)")
    print("   âœ… Schema evolution frequently")
    print("   âœ… RPC/messaging systems")
    print("   âœ… Row-based access patterns")
    print("   âŒ Use Parquet for analytics (better compression)")


def example_4_json_nested_hierarchical():
    """
    Demonstrate JSON format for nested and hierarchical data structures.
    
    WHAT ARE NESTED AND HIERARCHICAL DATA STRUCTURES?
    ================================================
    
    1. NESTED DATA (Objects within objects):
    ----------------------------------------
    {
      "user": {                    â† Nested object (1 level)
        "id": 123,
        "name": "Alice",
        "address": {               â† Nested object (2 levels deep)
          "street": "123 Main",
          "city": "NYC",
          "coordinates": {         â† Nested object (3 levels deep)
            "lat": 40.7128,
            "lon": -74.0060
          }
        }
      }
    }
    
    Access: user.address.city = "NYC"
            user.address.coordinates.lat = 40.7128
    
    2. HIERARCHICAL DATA (Tree structures):
    ---------------------------------------
    {
      "organization": {
        "name": "TechCorp",
        "departments": [          â† Array of departments
          {
            "name": "Engineering",
            "teams": [            â† Array of teams
              {
                "name": "Backend",
                "employees": [    â† Array of employees
                  {"id": 1, "name": "Alice"},
                  {"id": 2, "name": "Bob"}
                ]
              },
              {
                "name": "Frontend",
                "employees": [
                  {"id": 3, "name": "Charlie"}
                ]
              }
            ]
          },
          {
            "name": "Sales",
            "teams": [
              {
                "name": "Enterprise",
                "employees": [
                  {"id": 4, "name": "Diana"}
                ]
              }
            ]
          }
        ]
      }
    }
    
    Hierarchy (tree):
    Organization
    â”œâ”€â”€ Engineering
    â”‚   â”œâ”€â”€ Backend
    â”‚   â”‚   â”œâ”€â”€ Alice
    â”‚   â”‚   â””â”€â”€ Bob
    â”‚   â””â”€â”€ Frontend
    â”‚       â””â”€â”€ Charlie
    â””â”€â”€ Sales
        â””â”€â”€ Enterprise
            â””â”€â”€ Diana
    
    3. REAL-WORLD EXAMPLES:
    ----------------------
    
    API RESPONSE (Nested):
    {
      "status": "success",
      "data": {
        "user": {...},
        "settings": {...},
        "metadata": {...}
      }
    }
    
    E-COMMERCE ORDER (Hierarchical):
    {
      "order_id": "ORD-12345",
      "customer": {
        "id": 789,
        "name": "Alice",
        "shipping_address": {...}
      },
      "items": [                  â† Array of items
        {
          "product_id": "P-001",
          "name": "Laptop",
          "price": 1200,
          "options": [            â† Nested array
            {"name": "RAM", "value": "16GB"},
            {"name": "Storage", "value": "512GB SSD"}
          ]
        },
        {
          "product_id": "P-002",
          "name": "Mouse",
          "price": 25
        }
      ],
      "payment": {
        "method": "credit_card",
        "card": {
          "last4": "1234",
          "brand": "Visa"
        }
      }
    }
    
    APPLICATION LOGS (Semi-structured):
    {
      "timestamp": "2024-01-15T10:30:00Z",
      "level": "ERROR",
      "message": "Database connection failed",
      "context": {
        "service": "api-server",
        "host": "prod-01",
        "error": {
          "type": "ConnectionTimeout",
          "details": {
            "timeout_ms": 5000,
            "retry_count": 3
          }
        }
      }
    }
    
    WHY JSON FOR NESTED/HIERARCHICAL DATA?
    -------------------------------------
    âœ… Natural representation of nested structures
    âœ… Flexible schema (add/remove fields easily)
    âœ… Human-readable (debugging, logging)
    âœ… Universal format (every language supports it)
    âœ… API standard (REST APIs use JSON)
    
    âŒ Large file sizes (text format, no compression)
    âŒ Slow parsing (text â†’ binary conversion)
    âŒ No schema enforcement (errors at read time)
    âŒ Not optimized for analytics
    
    JSON CHARACTERISTICS:
    --------------------
    â€¢ Human-readable text format
    â€¢ Flexible nested structures
    â€¢ Schema-on-read (inferred)
    â€¢ Large file sizes (no compression by default)
    
    JSON vs Parquet:
    â€¢ JSON: Flexible schema, nested objects, human-readable
    â€¢ Parquet: Strict schema, flat/nested, binary, compressed
    
    JSON Use Cases:
    â€¢ API responses
    â€¢ Log files (application logs)
    â€¢ Configuration files
    â€¢ Semi-structured data with varying schemas
    """
    print("\n" + "="*70)
    print("EXAMPLE 4: JSON Semi-Structured Data")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create nested JSON structure
    print("\nğŸ“Š Creating nested JSON data...")
    df_json = spark.createDataFrame([
        (1, "Alice", {"street": "123 Main St", "city": "NYC", "zip": "10001"},
         [{"skill": "Python", "level": 9}, {"skill": "Spark", "level": 8}]),
        (2, "Bob", {"street": "456 Oak Ave", "city": "SF", "zip": "94102"},
         [{"skill": "Java", "level": 7}, {"skill": "Scala", "level": 8}]),
        (3, "Charlie", {"street": "789 Elm Rd", "city": "LA", "zip": "90001"},
         [{"skill": "SQL", "level": 9}])
    ], ["id", "name", "address", "skills"])
    
    print("\n   Schema:")
    df_json.printSchema()
    df_json.show(truncate=False)
    
    # Write as JSON
    json_path = "/tmp/data.json"
    print(f"\nğŸ’¾ Writing as JSON to {json_path}")
    df_json.write.mode("overwrite").json(json_path)
    
    # Show raw JSON file
    print("\nğŸ“„ Raw JSON content (first 3 records):")
    import subprocess
    result = subprocess.check_output(f"head -n 3 {json_path}/part-*.json", shell=True).decode()
    print(result)
    
    # Read JSON back
    print("\nğŸ“– Reading JSON back:")
    df_read = spark.read.json(json_path)
    df_read.show(truncate=False)
    
    # Query nested fields
    print("\nğŸ” Querying nested fields:")
    print("   Query: Select name, city, and Python skill level")
    
    df_query = df_read.select(
        col("name"),
        col("address.city").alias("city"),
        expr("filter(skills, x -> x.skill = 'Python')[0].level").alias("python_level")
    )
    df_query.show()
    
    # Explode array
    print("\nğŸ“Š Exploding skills array (flatten):")
    df_exploded = df_read.select(
        col("name"),
        explode(col("skills")).alias("skill_info")
    ).select(
        col("name"),
        col("skill_info.skill").alias("skill"),
        col("skill_info.level").alias("level")
    )
    df_exploded.show()
    
    # Compare JSON vs Parquet
    print("\nğŸ“ JSON vs PARQUET SIZE:")
    parquet_path = "/tmp/data_json_compare.parquet"
    df_json.write.mode("overwrite").parquet(parquet_path)
    
    import subprocess
    json_size = subprocess.check_output(f"du -sh {json_path}", shell=True).decode().split()[0]
    parquet_size = subprocess.check_output(f"du -sh {parquet_path}", shell=True).decode().split()[0]
    
    print(f"   JSON:    {json_size} (text, uncompressed)")
    print(f"   Parquet: {parquet_size} (binary, compressed)")
    print("   â†’ Parquet typically 5-10Ã— smaller")
    
    # JSON Lines format (newline-delimited JSON)
    print("\nğŸ“ JSON LINES FORMAT:")
    print("""
   Standard JSON: Entire file is one JSON object/array
   JSON Lines:    One JSON object per line (better for streaming)
   
   Example:
   {"id": 1, "name": "Alice"}
   {"id": 2, "name": "Bob"}
   {"id": 3, "name": "Charlie"}
   
   Spark uses JSON Lines by default (one record per line).
    """)
    
    print("\nğŸ’¡ WHEN TO USE JSON:")
    print("   âœ… API integration (REST APIs)")
    print("   âœ… Application logs (structured logging)")
    print("   âœ… Configuration files")
    print("   âœ… Semi-structured data with varying schemas")
    print("   âœ… Human readability important")
    print("   âŒ Use Parquet for production data lakes (much smaller)")


def example_5_hdf5_scientific_arrays_matrices():
    """
    Demonstrate HDF5 for scientific multi-dimensional arrays and matrices.
    
    WHAT ARE SCIENTIFIC ARRAYS AND MATRICES?
    ========================================
    
    1. ARRAYS (1D, 2D, 3D, ..., N-dimensional):
    ------------------------------------------
    
    1D Array (Vector):
    temperature = [72.5, 73.1, 71.8, 74.2, 73.9]
    Length: 5
    Access: temperature[2] = 71.8
    
    2D Array (Matrix):
    image = [
      [255, 128,  64],    â† Row 0
      [192, 224, 160],    â† Row 1
      [ 32,  96, 128]     â† Row 2
    ]
    Shape: (3 rows, 3 columns) = 3Ã—3 matrix
    Access: image[1, 2] = 160 (row 1, column 2)
    
    3D Array (Spatial + Time):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ temperature[latitude, longitude, time] â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Dimensions:                            â”‚
    â”‚ â€¢ Latitude:  1000 points (90Â°N to 90Â°S)â”‚
    â”‚ â€¢ Longitude: 2000 points (180Â°W to 180Â°E)â”‚
    â”‚ â€¢ Time:      365 days                  â”‚
    â”‚                                        â”‚
    â”‚ Shape: (1000, 2000, 365)               â”‚
    â”‚ Total elements: 730,000,000            â”‚
    â”‚                                        â”‚
    â”‚ Access examples:                       â”‚
    â”‚ â€¢ temp[500, 1000, 180]                 â”‚
    â”‚   â†’ Temperature at specific point on day 180â”‚
    â”‚                                        â”‚
    â”‚ â€¢ temp[500, 1000, :]                   â”‚
    â”‚   â†’ Temperature time series at one locationâ”‚
    â”‚                                        â”‚
    â”‚ â€¢ temp[:, :, 180]                      â”‚
    â”‚   â†’ Entire spatial grid for day 180   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Visualization:
    
         Time (365 days)
          â†—
         /
        /__________ Longitude (2000 points)
        |
        |
        â†“
     Latitude
    (1000 points)
    
    Each "cell" contains a temperature value.
    
    4D Array (Spatial + Time + Variable):
    climate_data[lat, lon, time, variable]
    â€¢ Latitude: 1000
    â€¢ Longitude: 2000
    â€¢ Time: 365 days
    â€¢ Variable: 4 (temperature, humidity, pressure, wind_speed)
    Shape: (1000, 2000, 365, 4)
    
    2. REAL-WORLD SCIENTIFIC DATA EXAMPLES:
    ---------------------------------------
    
    MEDICAL IMAGING (3D MRI scan):
    mri_scan[x, y, z]
    â€¢ x: 256 slices (left-right)
    â€¢ y: 256 slices (front-back)
    â€¢ z: 128 slices (top-bottom)
    Shape: (256, 256, 128)
    Size: 8.4 million voxels (3D pixels)
    
    ASTRONOMY (Telescope images):
    sky_image[x, y, wavelength, time]
    â€¢ x, y: 4096Ã—4096 pixels (spatial)
    â€¢ wavelength: 100 channels (different light wavelengths)
    â€¢ time: 1000 exposures
    Shape: (4096, 4096, 100, 1000)
    Size: 1.67 trillion values!
    
    PARTICLE PHYSICS (Detector readings):
    detector_data[event, particle, measurement]
    â€¢ event: 1,000,000 collisions
    â€¢ particle: 500 particles per event
    â€¢ measurement: 20 properties (position, momentum, energy, etc.)
    Shape: (1000000, 500, 20)
    
    CLIMATE MODELING:
    ocean_temperature[depth, lat, lon, time]
    â€¢ depth: 100 levels (surface to 5000m deep)
    â€¢ lat: 180 points
    â€¢ lon: 360 points
    â€¢ time: 365 days Ã— 10 years = 3650
    Shape: (100, 180, 360, 3650)
    
    GENOMICS (Gene expression):
    expression_data[gene, cell, condition]
    â€¢ gene: 20,000 genes
    â€¢ cell: 10,000 cells
    â€¢ condition: 50 experimental conditions
    Shape: (20000, 10000, 50)
    
    3. MATRIX OPERATIONS:
    --------------------
    
    Matrix Multiplication (Linear Algebra):
    A = [[1, 2],      B = [[5, 6],
         [3, 4]]           [7, 8]]
    
    C = A Ã— B = [[19, 22],
                 [43, 50]]
    
    Used in: Machine learning, simulations, physics
    
    Element-wise Operations:
    temperature_celsius = [20, 25, 30]
    temperature_fahrenheit = temperature_celsius * 1.8 + 32
    Result: [68, 77, 86]
    
    Slicing (Access sub-arrays):
    data[0:10, 0:10, :]    â† First 10Ã—10 spatial points, all times
    data[:, :, 0]          â† Entire spatial grid at time 0
    data[50, 100, :]       â† Time series at one location
    
    4. WHY HDF5 FOR SCIENTIFIC DATA?
    --------------------------------
    âœ… Efficient storage of N-dimensional arrays
    âœ… Chunking: Fast access to array slices
    âœ… Compression: GZIP, LZF (smaller files)
    âœ… Partial reads: Read subset without loading all data
    âœ… Metadata: Store units, calibration, etc.
    âœ… Industry standard: NASA, CERN, NIH use HDF5
    
    âŒ Single-machine focused (not distributed)
    âŒ Limited Spark support (need h5spark library)
    âŒ Complex format (steep learning curve)
    
    5. HDF5 vs PARQUET FOR SCIENTIFIC DATA:
    --------------------------------------
    
    HDF5:
    â€¢ Optimized for: Array slicing (get slice of 3D array)
    â€¢ Storage: N-dimensional arrays natively
    â€¢ Access: Fast random access to array slices
    â€¢ Use case: Single-machine scientific computing
    
    Parquet:
    â€¢ Optimized for: Column scanning (get all values of one column)
    â€¢ Storage: Tabular data (rows Ã— columns)
    â€¢ Access: Fast column reads, slow row access
    â€¢ Use case: Distributed analytics on tabular data
    
    Example:
    Get all temperatures for one location over time:
    â€¢ HDF5:    temp[500, 1000, :] â†’ Fast (single slice)
    â€¢ Parquet: SELECT temp WHERE lat=500 AND lon=1000 â†’ Slow (scan all)
    
    Get average temperature across all locations:
    â€¢ HDF5:    np.mean(temp) â†’ Slow (not optimized for aggregation)
    â€¢ Parquet: SELECT AVG(temp) â†’ Fast (columnar aggregation)
    
    HDF5 (Hierarchical Data Format 5):
    ----------------------------------
    Designed for scientific computing and large numerical arrays.
    
    HDF5 Structure:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HDF5 File                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ /                     (root group)     â”‚
    â”‚ â”œâ”€â”€ /temperature     (3D dataset)     â”‚
    â”‚ â”‚   â”œâ”€â”€ Datatype: float64            â”‚
    â”‚ â”‚   â”œâ”€â”€ Shape: (1000, 1000, 365)     â”‚
    â”‚ â”‚   â”œâ”€â”€ Chunks: (100, 100, 1)        â”‚
    â”‚ â”‚   â””â”€â”€ Compression: gzip            â”‚
    â”‚ â”œâ”€â”€ /pressure        (3D dataset)     â”‚
    â”‚ â””â”€â”€ /metadata        (attributes)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    HDF5 Features:
    â€¢ Hierarchical structure (like filesystem)
    â€¢ Multi-dimensional arrays
    â€¢ Chunked storage (efficient slicing)
    â€¢ Fast random access
    â€¢ Metadata attributes
    
    HDF5 vs Parquet:
    â€¢ HDF5: Arrays, single-machine, NumPy-centric
    â€¢ Parquet: Tables, distributed, Spark-centric
    
    NOTE: HDF5 is NOT natively supported by Spark.
    This example shows conceptual comparison.
    """
    print("\n" + "="*70)
    print("EXAMPLE 5: HDF5 Scientific Arrays (Conceptual)")
    print("="*70)
    
    spark = create_spark_session()
    
    print("\nâš ï¸  NOTE: HDF5 is NOT natively supported by PySpark.")
    print("   This example demonstrates the concept and shows conversion.")
    
    # Create sample data (simulating sensor readings)
    print("\nğŸ“Š Creating sample 3D data (sensors Ã— time Ã— measurements)...")
    print("   Simulating: 100 sensors Ã— 365 days Ã— 24 hours")
    
    # In HDF5, this would be a 3D array
    # In Spark, we represent it as a table
    
    df_sensors = spark.range(0, 100).alias("sensor_id") \
        .crossJoin(spark.range(0, 365).alias("day")) \
        .crossJoin(spark.range(0, 24).alias("hour")) \
        .withColumn("temperature", rand() * 30 + 10) \
        .withColumn("humidity", rand() * 100) \
        .select("sensor_id", "day", "hour", "temperature", "humidity")
    
    print(f"\n   Total records: {df_sensors.count():,}")
    print("   (100 sensors Ã— 365 days Ã— 24 hours = 876,000 records)")
    df_sensors.show(5)
    
    # Conceptual HDF5 structure
    print("\nğŸ“¦ HDF5 CONCEPTUAL STRUCTURE:")
    print("""
   In HDF5, this would be stored as:
   
   /sensors.h5
   â”œâ”€â”€ /temperature [100, 365, 24]  (3D array)
   â”‚   â””â”€â”€ chunks: (10, 1, 24)  [10 sensors, 1 day, all hours]
   â”œâ”€â”€ /humidity    [100, 365, 24]  (3D array)
   â””â”€â”€ /metadata
       â”œâ”€â”€ units: "celsius"
       â””â”€â”€ source: "weather_stations"
   
   Access patterns:
   â€¢ temps[50, :, :]      â†’ All data for sensor 50
   â€¢ temps[:, 180, 12]    â†’ All sensors at day 180, hour 12
   â€¢ temps[0:10, 0:7, :]  â†’ First 10 sensors, first week
    """)
    
    # PySpark equivalent using Parquet
    print("\nğŸ’¾ PYSPARK/PARQUET EQUIVALENT:")
    parquet_path = "/tmp/sensors.parquet"
    
    print(f"   Writing as partitioned Parquet to {parquet_path}")
    df_sensors.write.mode("overwrite") \
        .partitionBy("day") \
        .parquet(parquet_path)
    
    print("\n   Querying specific sensor (equivalent to HDF5 slice):")
    df_sensor_50 = spark.read.parquet(parquet_path) \
        .filter(col("sensor_id") == 50)
    print(f"   Sensor 50 data: {df_sensor_50.count()} records")
    df_sensor_50.orderBy("day", "hour").show(5)
    
    print("\n   Querying specific time (day 180, hour 12):")
    df_snapshot = spark.read.parquet(parquet_path) \
        .filter((col("day") == 180) & (col("hour") == 12))
    print(f"   Snapshot: {df_snapshot.count()} sensors")
    df_snapshot.show(5)
    
    # Performance comparison
    print("\nâš¡ PERFORMANCE COMPARISON:")
    print("""
   HDF5 (single machine):
   â€¢ Random access: Very fast (memory-mapped I/O)
   â€¢ Array slicing: Optimized for NumPy operations
   â€¢ Parallel: Limited (multi-threading, not distributed)
   â€¢ Scale: Limited by single machine memory
   
   Parquet + Spark (distributed):
   â€¢ Random access: Slower (file I/O, not memory-mapped)
   â€¢ Query: Optimized for SQL-like operations
   â€¢ Parallel: Excellent (distributed across cluster)
   â€¢ Scale: Unlimited (petabytes)
   
   Rule of thumb:
   â€¢ HDF5: Single machine, < 1 TB, array operations, NumPy
   â€¢ Parquet + Spark: Distributed, > 1 TB, SQL queries, analytics
    """)
    
    # Conversion workflow
    print("\nğŸ”„ HDF5 â†” PARQUET CONVERSION WORKFLOW:")
    print("""
   Option 1: Convert HDF5 to Parquet (for Spark processing)
   
   import h5py
   import pandas as pd
   
   # Read HDF5
   with h5py.File('data.h5', 'r') as f:
       temps = f['temperature'][:]  # Load entire array
   
   # Convert to DataFrame
   df_pd = pd.DataFrame({
       'sensor_id': ...,
       'day': ...,
       'temperature': temps.flatten()
   })
   
   # Write to Parquet
   df_spark = spark.createDataFrame(df_pd)
   df_spark.write.parquet('output.parquet')
   
   
   Option 2: Process HDF5 directly (for small data)
   
   from pyspark.sql.types import Row
   
   # Read HDF5 and yield rows
   def read_hdf5(path):
       with h5py.File(path, 'r') as f:
           data = f['dataset'][:]
           for i, row in enumerate(data):
               yield Row(id=i, value=float(row))
   
   # Create Spark DataFrame
   rdd = spark.sparkContext.parallelize(read_hdf5('data.h5'))
   df = spark.createDataFrame(rdd)
    """)
    
    print("\nğŸ’¡ WHEN TO USE HDF5:")
    print("   âœ… Scientific computing (physics, astronomy, biology)")
    print("   âœ… Multi-dimensional numerical arrays")
    print("   âœ… Single-machine workflows with NumPy")
    print("   âœ… Fast random access to array slices")
    print("   âœ… Hierarchical data organization")
    print("   âŒ Use Parquet for distributed analytics with Spark")


def example_6_format_comparison_benchmark():
    """
    Comprehensive benchmark comparing all formats.
    """
    print("\n" + "="*70)
    print("EXAMPLE 6: Format Comparison Benchmark")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create test dataset
    print("\nğŸ“Š Creating benchmark dataset (50,000 rows)...")
    df = spark.range(1, 50001) \
        .withColumn("name", expr("concat('User_', id)")) \
        .withColumn("age", (rand() * 50 + 20).cast("int")) \
        .withColumn("salary", (rand() * 100000 + 30000).cast("double")) \
        .withColumn("department", expr("concat('Dept_', cast(id % 10 as string))"))
    
    print(f"   Rows: {df.count():,}")
    print(f"   Columns: {len(df.columns)}")
    
    # Test formats
    formats = {
        "parquet": {"write": lambda: df.write.mode("overwrite").parquet("/tmp/bench_parquet")},
        "orc": {"write": lambda: df.write.mode("overwrite").orc("/tmp/bench_orc")},
        "avro": {"write": lambda: df.write.format("avro").mode("overwrite").save("/tmp/bench_avro")},
        "json": {"write": lambda: df.write.mode("overwrite").json("/tmp/bench_json")},
    }
    
    print("\nâ±ï¸  BENCHMARK RESULTS:")
    print("   " + "â”€" * 70)
    
    results = []
    for fmt, ops in formats.items():
        path = f"/tmp/bench_{fmt}"
        
        # Write
        start = time.time()
        ops["write"]()
        write_time = time.time() - start
        
        # Get size
        import subprocess
        size_output = subprocess.check_output(f"du -sh {path}", shell=True).decode()
        size = size_output.split()[0]
        
        # Read
        start = time.time()
        if fmt == "parquet":
            df_read = spark.read.parquet(path)
        elif fmt == "orc":
            df_read = spark.read.orc(path)
        elif fmt == "avro":
            df_read = spark.read.format("avro").load(path)
        elif fmt == "json":
            df_read = spark.read.json(path)
        
        count = df_read.count()
        read_time = time.time() - start
        
        results.append({
            "format": fmt.upper(),
            "size": size,
            "write_time": write_time,
            "read_time": read_time
        })
        
        print(f"\n   {fmt.upper()}:")
        print(f"      Size:  {size:>8s}")
        print(f"      Write: {write_time:>6.2f}s")
        print(f"      Read:  {read_time:>6.2f}s")
    
    print("\n   " + "â”€" * 70)
    
    print("\nğŸ“Š SUMMARY TABLE:")
    print("""
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Format    â”‚ Size     â”‚ Write Speed â”‚ Read Speed â”‚ Best For            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Parquet   â”‚ Smallest â”‚ Medium      â”‚ Fast       â”‚ Analytics (best!)   â”‚
   â”‚ ORC       â”‚ Smaller  â”‚ Medium      â”‚ Fast       â”‚ Hive ecosystem      â”‚
   â”‚ Avro      â”‚ Medium   â”‚ Fast        â”‚ Medium     â”‚ Streaming, schema   â”‚
   â”‚ JSON      â”‚ Largest  â”‚ Fast        â”‚ Slow       â”‚ APIs, human-read    â”‚
   â”‚ Delta     â”‚ Smallest*â”‚ Medium      â”‚ Fast       â”‚ ACID + time travel  â”‚
   â”‚ HDF5      â”‚ Medium   â”‚ Very Fast** â”‚ Very Fast**â”‚ Scientific arrays   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   * Delta = Parquet + transaction log
   ** HDF5 on single machine (not distributed)
    """)
    
    print("\nğŸ¯ FORMAT SELECTION GUIDE:")
    print("""
   Production Data Lake: 
   â†’ Delta Lake (ACID) or Parquet (simple)
   
   Hive Warehouse:
   â†’ ORC
   
   Kafka Streaming:
   â†’ Avro
   
   API Integration:
   â†’ JSON
   
   Scientific Computing:
   â†’ HDF5 (single machine) or Parquet (distributed)
   
   ACID Transactions:
   â†’ Delta Lake
   
   Maximum Compression:
   â†’ ORC or Parquet with ZSTD
    """)


def main():
    """
    Run all file format examples.
    """
    print("\n" + "="*70)
    print(" COMPREHENSIVE FILE FORMAT EXAMPLES ")
    print("="*70)
    
    print("""
This module demonstrates 5 different file formats:

1. Delta Lake  â†’ ACID transactions, time travel, updates/deletes
2. ORC         â†’ Hive integration, better compression
3. Avro        â†’ Schema evolution, Kafka streaming
4. JSON        â†’ Semi-structured, API integration
5. HDF5        â†’ Scientific arrays, NumPy integration

Each format has specific use cases where it excels.
    """)
    
    try:
        example_1_delta_lake_acid_transactions()
        example_2_orc_hive_ecosystem()
        example_3_avro_schema_evolution()
        example_4_json_nested_hierarchical()
        example_5_hdf5_scientific_arrays_matrices()
        example_6_format_comparison_benchmark()
        
        print("\n" + "="*70)
        print("âœ… ALL EXAMPLES COMPLETED SUCCESSFULLY")
        print("="*70)
        
        print("\nğŸ“š KEY TAKEAWAYS:")
        print("   1. Delta Lake: Best for data lakes with ACID requirements")
        print("   2. ORC: Best for Hive ecosystem")
        print("   3. Avro: Best for Kafka and schema evolution")
        print("   4. JSON: Best for APIs and semi-structured data")
        print("   5. HDF5: Best for scientific arrays (use Parquet for Spark)")
        print("   6. Parquet: Best default choice for Spark analytics")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
