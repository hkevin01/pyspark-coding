"""
Broadcast Joins and Parquet File Format
========================================

WHAT THIS MODULE COVERS:
1. Broadcast Joins (Small Table Optimization)
2. Parquet File Format (Columnar Storage)
3. When to Use Parquet vs Alternatives
4. Performance Comparisons

BROADCAST JOIN FUNDAMENTALS:
---------------------------
A broadcast join is an optimization where a small table is replicated (broadcast) 
to all executor nodes, eliminating the need to shuffle the large table.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGULAR JOIN (SHUFFLE)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Large Table (1 TB)          Small Table (10 MB)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ Part 1   â”‚                â”‚ Part 1   â”‚                      â”‚
â”‚  â”‚ Part 2   â”‚   SHUFFLE â†’    â”‚ Part 2   â”‚   â† SHUFFLE         â”‚
â”‚  â”‚ Part 3   â”‚                â”‚ Part 3   â”‚                      â”‚
â”‚  â”‚ Part ... â”‚                â”‚ Part ... â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚       â†“                           â†“                            â”‚
â”‚  Data moves across               Data moves across             â”‚
â”‚  network (1 TB!)                 network (10 MB)               â”‚
â”‚                                                                 â”‚
â”‚  Total Network Transfer: 1.01 TB                               â”‚
â”‚  Time: ~30 minutes                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BROADCAST JOIN (OPTIMIZED)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Large Table (1 TB)          Small Table (10 MB)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ Part 1   â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  COPIED  â”‚ (Broadcast)         â”‚
â”‚  â”‚ Part 2   â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  TO ALL  â”‚                     â”‚
â”‚  â”‚ Part 3   â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  NODES   â”‚                     â”‚
â”‚  â”‚ Part ... â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚          â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚       â†“                                                         â”‚
â”‚  Stays in place!             Each executor gets                 â”‚
â”‚  (No shuffle)                copy (10 MB Ã— 10 nodes = 100 MB)  â”‚
â”‚                                                                 â”‚
â”‚  Total Network Transfer: 100 MB (10Ã— fewer!)                   â”‚
â”‚  Time: ~3 minutes (10Ã— faster!)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY INSIGHT:
-----------
Broadcasting makes sense when:
â€¢ Small table size Ã— number of executors < Large table shuffle cost
â€¢ Example: 10 MB Ã— 100 executors = 1 GB << 1 TB shuffle

WHEN TO USE BROADCAST JOINS:
---------------------------
âœ… Small table < 10 MB (default broadcast threshold)
âœ… One table much smaller than other (1:100+ ratio)
âœ… Repeated joins with same small table
âœ… Star schema (fact table with dimension tables)
âœ… Lookup tables, reference data, configuration tables

âŒ Both tables large (> 100 MB each)
âŒ Small table changes frequently (broadcast overhead)
âŒ Limited driver/executor memory
âŒ Network bandwidth constrained

PARQUET FILE FORMAT:
-------------------
Parquet is a columnar storage format optimized for analytics.

ROW-BASED FORMAT (CSV, JSON):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Row 1: id=1, name="Alice", age=25       â”‚
â”‚ Row 2: id=2, name="Bob", age=30         â”‚
â”‚ Row 3: id=3, name="Charlie", age=35     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â€¢ Stores data row-by-row
â€¢ Good for: Reading entire rows, transactional systems
â€¢ Bad for: Analytical queries on specific columns

COLUMN-BASED FORMAT (Parquet):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column "id":    [1, 2, 3]               â”‚
â”‚ Column "name":  ["Alice", "Bob", "Charlie"] â”‚
â”‚ Column "age":   [25, 30, 35]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â€¢ Stores data column-by-column
â€¢ Good for: Analytical queries, aggregations, column pruning
â€¢ Bad for: Random row access

PARQUET ADVANTAGES:
------------------
1. **Compression**: Similar values compress better (5-10Ã— smaller)
2. **Column Pruning**: Read only needed columns (not entire row)
3. **Predicate Pushdown**: Skip row groups based on metadata
4. **Schema Evolution**: Add/remove columns without rewriting data
5. **Type Safety**: Stores typed data (int, string, timestamp)
6. **Compatibility**: Works across Spark, Hive, Impala, Presto

EXAMPLE COMPRESSION:
-------------------
CSV File (1 GB):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1,Alice,25,F,2020-01-01,100.50                       â”‚
â”‚ 2,Bob,30,M,2020-01-02,200.75                         â”‚
â”‚ 3,Charlie,35,M,2020-01-03,150.25                     â”‚
â”‚ ... (1 million rows)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Size: 1 GB

Parquet File (Same Data):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column "age": [25, 30, 35, 25, 30, ...] â†’ Compressedâ”‚
â”‚   (Only ~100 unique values, high compression!)       â”‚
â”‚ Column "gender": [F, M, M, F, M, ...] â†’ Compressed  â”‚
â”‚   (Only 2 unique values, extreme compression!)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Size: 100-200 MB (5-10Ã— smaller!)

WHEN TO USE PARQUET:
-------------------
âœ… Analytical workloads (aggregations, filtering)
âœ… Data warehouse / data lake storage
âœ… Read-heavy workloads with column-specific queries
âœ… Long-term storage (compressed, efficient)
âœ… Large datasets (TB+)
âœ… Schema evolution needed

PARQUET ALTERNATIVES:
--------------------
1. CSV
   â€¢ Use case: Simple interchange, human-readable
   â€¢ Pros: Universal support, text-based
   â€¢ Cons: No compression, no types, slow to parse
   â€¢ When: Small files (< 100 MB), one-time imports

2. JSON
   â€¢ Use case: Nested/hierarchical data, APIs
   â€¢ Pros: Flexible schema, nested structures
   â€¢ Cons: Large file size, slow parsing
   â€¢ When: Semi-structured data, nested objects

3. Avro
   â€¢ Use case: Row-based streaming, schema evolution
   â€¢ Pros: Compact binary, schema evolution, fast writes
   â€¢ Cons: Row-based (slower analytics), less compression
   â€¢ When: Kafka streams, frequent writes, schema changes

4. ORC (Optimized Row Columnar)
   â€¢ Use case: Hive integration, similar to Parquet
   â€¢ Pros: Better compression, ACID support, predicate pushdown
   â€¢ Cons: Hive-specific, less universal than Parquet
   â€¢ When: Hive ecosystem, need ACID transactions

5. Delta Lake
   â€¢ Use case: ACID transactions, time travel, updates/deletes
   â€¢ Pros: Built on Parquet + transaction log, ACID, versioning
   â€¢ Cons: Requires Delta Lake library, slightly more overhead
   â€¢ When: Need updates/deletes, ACID guarantees, audit trail

6. HDF5 (Hierarchical Data Format 5)
   â€¢ Use case: Scientific computing, numerical arrays, multi-dimensional data
   â€¢ Pros: Fast random access, chunked storage, hierarchical structure
   â€¢ Cons: Not distributed-friendly, complex API, poor compression vs Parquet
   â€¢ When: Scientific data (numpy arrays), single-machine workflows

COMPARISON TABLE:
----------------
| Format      | Type      | Compression | Speed (Read) | Speed (Write) | Best For              |
|-------------|-----------|-------------|--------------|---------------|-----------------------|
| CSV         | Row       | None        | Slow         | Fast          | Simple interchange    |
| JSON        | Row       | None        | Slow         | Medium        | Nested data           |
| Avro        | Row       | Good        | Medium       | Fast          | Streaming, Kafka      |
| Parquet     | Column    | Excellent   | Fast         | Medium        | Analytics, data lake  |
| ORC         | Column    | Excellent   | Fast         | Medium        | Hive, ACID            |
| Delta Lake  | Column    | Excellent   | Fast         | Medium        | ACID, time travel     |
| HDF5        | Array     | Good        | Very Fast*   | Very Fast*    | Scientific arrays     |

* Fast for random access on single machine, slow for distributed processing

PARQUET vs HDF5 DEEP DIVE:
==========================

Parquet Row Groups vs HDF5 Chunks
----------------------------------

PARQUET ROW GROUPS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parquet File Structure                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  File Metadata (schema, row group metadata)                â”‚
â”‚  â”œâ”€ Total rows: 1,000,000                                  â”‚
â”‚  â”œâ”€ Row groups: 10                                         â”‚
â”‚  â””â”€ Columns: id, name, age, salary                         â”‚
â”‚                                                             â”‚
â”‚  Row Group 1 (100,000 rows)                                â”‚
â”‚  â”œâ”€ Column Chunk: id [1, 2, ..., 100000]                   â”‚
â”‚  â”‚  â””â”€ Pages: [Page1: 0-10K] [Page2: 10K-20K] ...         â”‚
â”‚  â”œâ”€ Column Chunk: name ["Alice", "Bob", ...]               â”‚
â”‚  â”œâ”€ Column Chunk: age [25, 30, 35, ...]                    â”‚
â”‚  â””â”€ Column Chunk: salary [50000, 60000, ...]               â”‚
â”‚                                                             â”‚
â”‚  Row Group 2 (100,000 rows)                                â”‚
â”‚  â”œâ”€ Column Chunk: id [100001, 100002, ...]                 â”‚
â”‚  â””â”€ ...                                                     â”‚
â”‚                                                             â”‚
â”‚  ... (Row Groups 3-10)                                     â”‚
â”‚                                                             â”‚
â”‚  Footer (metadata, column statistics, offsets)             â”‚
â”‚  â”œâ”€ Row Group 1: min/max values per column                 â”‚
â”‚  â”œâ”€ Row Group 2: min/max values per column                 â”‚
â”‚  â””â”€ ...                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Row Group Size (default: 128 MB)
â€¢ Balances: Memory usage vs parallelism
â€¢ Too small: Overhead from metadata, poor compression
â€¢ Too large: Memory pressure, coarse parallelism
â€¢ Optimal: 128-512 MB per row group

HDF5 CHUNKS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HDF5 File Structure                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  File Header (format signature, super block)               â”‚
â”‚                                                             â”‚
â”‚  Dataset: /data/temperatures (1000 Ã— 1000 Ã— 365)           â”‚
â”‚  â”œâ”€ Datatype: float64                                       â”‚
â”‚  â”œâ”€ Dimensions: [latitude, longitude, day_of_year]         â”‚
â”‚  â”œâ”€ Chunk size: (100, 100, 1) [10,000 floats = 80 KB]      â”‚
â”‚  â””â”€ Compression: gzip level 4                               â”‚
â”‚                                                             â”‚
â”‚  Physical Layout:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚ Day 1          â”‚
â”‚  â”‚ (0,0,0) â”‚ (0,1,0) â”‚ (0,2,0) â”‚ ...     â”‚                â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚  â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚                â”‚
â”‚  â”‚ (1,0,0) â”‚ (1,1,0) â”‚ (1,2,0) â”‚ ...     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚ Chunk   â”‚ Day 2          â”‚
â”‚  â”‚ (0,0,1) â”‚ (0,1,1) â”‚ (0,2,1) â”‚ ...     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  ... (365 days)                                             â”‚
â”‚                                                             â”‚
â”‚  B-tree Index (chunk locations on disk)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Chunk Size (user-defined, critical!)
â€¢ Access pattern dependent: Match expected query shape
â€¢ Too small: Too many seeks, index overhead
â€¢ Too large: Read unnecessary data
â€¢ Optimal: Depends on access pattern (1-10 MB typical)

KEY DIFFERENCES:
---------------

1. DATA MODEL:
   Parquet: Tabular data (rows Ã— columns)
   HDF5:    Multi-dimensional arrays (N-dimensional tensors)

2. STORAGE LAYOUT:
   Parquet: Columnar (all values of one column together)
   HDF5:    Chunked arrays (N-dimensional blocks)

3. ROW GROUP SIZE:
   Parquet: Fixed size (128 MB default, automatic)
   HDF5:    User-defined chunks (must match access pattern)

4. COMPRESSION:
   Parquet: Per column chunk (optimal for each type)
   HDF5:    Per chunk (uniform across dataset)

5. METADATA:
   Parquet: Column statistics (min/max per row group) â†’ predicate pushdown
   HDF5:    Dimension info, chunk locations â†’ no query optimization

6. DISTRIBUTED PROCESSING:
   Parquet: Native support (each row group = 1 partition)
   HDF5:    Poor support (designed for single machine, shared filesystem)

7. RANDOM ACCESS:
   Parquet: Column-level (read specific columns from any row group)
   HDF5:    Chunk-level (read specific N-dimensional regions)

8. USE CASE ALIGNMENT:
   Parquet: Business analytics, ETL, data warehousing
   HDF5:    Scientific computing, simulations, image stacks

EXAMPLE COMPARISON:
------------------

Scenario: Store 1 billion temperature readings
â€¢ Dimensions: 1000 locations Ã— 1000 sensors Ã— 365 days Ã— 3 years
â€¢ Data size: ~3 TB

PARQUET APPROACH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Table: temperature_readings                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ location_id | sensor_id | day | temp | ...    â”‚
â”‚ 1           | 1         | 1   | 25.3 |        â”‚
â”‚ 1           | 1         | 2   | 26.1 |        â”‚
â”‚ ...         | ...       | ... | ...  |        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Storage: Columnar by column
â€¢ Row groups: ~25,000 (3 TB / 128 MB)
â€¢ Parallelism: 25,000 partitions (excellent)
â€¢ Query: "Average temp by location" â†’ read only location_id, temp columns
â€¢ Compression: Exceptional (temp values similar, dict encoding)

HDF5 APPROACH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset: /temperatures [1000, 1000, 365, 3]   â”‚
â”‚ 4D array indexed by [location, sensor, day, year]
â”‚ Chunks: (10, 10, 1, 1) [100 readings = 800 bytes]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Storage: Chunked multi-dimensional array
â€¢ Chunks: 109,500,000 (1000/10 Ã— 1000/10 Ã— 365 Ã— 3)
â€¢ Access: temps[500, :, 180, 2] (all sensors at location 500, day 180, year 2)
â€¢ Fast random slicing, but poor for distributed processing
â€¢ Compression: Good, but not as good as Parquet (less repetition per chunk)

WHEN TO CHOOSE PARQUET:
âœ… Tabular data (rows and columns)
âœ… Distributed processing (Spark, Dask, etc.)
âœ… Analytical queries (aggregations, filtering)
âœ… Data warehouse / data lake
âœ… Heterogeneous data types per column
âœ… Cloud storage (S3, GCS, Azure Blob)
âœ… Schema evolution needed

WHEN TO CHOOSE HDF5:
âœ… Multi-dimensional numerical arrays
âœ… Single-machine workflows
âœ… Scientific computing (NumPy-centric)
âœ… Random access to array slices critical
âœ… Hierarchical data organization needed
âœ… Append-heavy workloads
âœ… Shared filesystem (not cloud-native)

PARQUET ROW GROUP SIZING:
-------------------------
Row group size affects:
â€¢ Memory usage (must fit in executor memory)
â€¢ Parallelism (1 row group = 1 task)
â€¢ Compression ratio (larger = better compression)
â€¢ Predicate pushdown (coarser granularity for large row groups)

Default: 128 MB (good balance)
Configuration: spark.sql.parquet.block.size

Small row groups (32 MB):
+ More parallelism (4Ã— more tasks)
+ Lower memory per task
- More metadata overhead
- Slightly worse compression
When: Large cluster, limited memory per executor

Large row groups (512 MB):
+ Better compression (more context)
+ Less metadata overhead
- Less parallelism (fewer tasks)
- Higher memory per task
When: Smaller cluster, high-memory executors

HDF5 CHUNK SIZING:
-----------------
Chunk size must match access pattern!

Example: Image time series (1000 Ã— 1000 Ã— 10,000 frames)

Access Pattern 1: Process entire frames
Chunks: (1000, 1000, 1) [1 frame = 1 chunk]
âœ… Perfect: Read whole frame in 1 I/O
âŒ Bad for: Time-series at single pixel

Access Pattern 2: Time-series at pixels
Chunks: (1, 1, 10000) [all time for 1 pixel]
âœ… Perfect: Read pixel time-series in 1 I/O
âŒ Bad for: Viewing single frames

Access Pattern 3: Mixed workload
Chunks: (100, 100, 100) [balanced]
âš ï¸  Compromise: Multiple I/O for both patterns

Key insight: HDF5 requires knowing access pattern upfront!
Parquet: Columnar layout works well for most analytical queries.

PARQUET + SPARK INTEGRATION:
----------------------------
â€¢ 1 row group = 1 Spark partition (automatic parallelism)
â€¢ Column pruning (read only needed columns)
â€¢ Predicate pushdown (skip row groups using min/max stats)
â€¢ Vectorized reading (columnar â†’ Arrow â†’ fast)
â€¢ Cloud-native (works well with S3, GCS, Azure Blob)

HDF5 + SPARK LIMITATIONS:
------------------------
â€¢ No native Spark support (must use custom readers)
â€¢ Each worker must read entire HDF5 file (no parallel reads)
â€¢ No predicate pushdown
â€¢ Designed for shared filesystem (NFS), not cloud storage
â€¢ Workaround: Convert HDF5 â†’ Parquet for Spark processing

Author: PySpark Training
Date: 2024-12
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    broadcast, col, sum as _sum, avg, count, when, 
    current_timestamp, date_format, rand
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
import time


def create_spark_session():
    """
    Create Spark session with broadcast join configuration.
    """
    return SparkSession.builder \
        .appName("BroadcastJoins_and_Parquet") \
        .config("spark.sql.autoBroadcastJoinThreshold", "10485760")  # 10 MB default \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()


def example_1_broadcast_join_basics():
    """
    Demonstrate basic broadcast join vs regular join.
    
    SCENARIO:
    ---------
    Large sales table (1M rows) join with small products table (100 rows).
    Without broadcast: Both tables shuffle (expensive).
    With broadcast: Products table copied to all executors (cheap).
    """
    print("\n" + "="*70)
    print("EXAMPLE 1: Broadcast Join Basics")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create large sales table (simulating 1M rows)
    print("\nğŸ“Š Creating large sales table (1M rows)...")
    sales_data = [
        (i, f"P{i % 100}", i * 10.5, f"2024-{(i % 12) + 1:02d}-01")
        for i in range(1, 100001)  # 100K for demo (imagine 1M)
    ]
    
    df_sales = spark.createDataFrame(
        sales_data,
        ["sale_id", "product_id", "amount", "sale_date"]
    )
    
    print(f"   Sales table: {df_sales.count():,} rows")
    df_sales.show(5, truncate=False)
    
    # Create small products table (100 rows)
    print("\nğŸ“¦ Creating small products table (100 rows)...")
    products_data = [
        (f"P{i}", f"Product_{i}", f"Category_{i % 10}", 50.0 + i)
        for i in range(100)
    ]
    
    df_products = spark.createDataFrame(
        products_data,
        ["product_id", "product_name", "category", "unit_price"]
    )
    
    print(f"   Products table: {df_products.count()} rows")
    df_products.show(5, truncate=False)
    
    # âŒ REGULAR JOIN (shuffle both sides)
    print("\nâŒ REGULAR JOIN (No Broadcast):")
    print("   â€¢ Both tables will be shuffled")
    print("   â€¢ Network transfer: Large table + small table")
    
    start_time = time.time()
    df_regular_join = df_sales.join(df_products, "product_id")
    result_count = df_regular_join.count()
    regular_time = time.time() - start_time
    
    print(f"   â€¢ Result: {result_count:,} rows")
    print(f"   â€¢ Time: {regular_time:.2f} seconds")
    df_regular_join.show(5, truncate=False)
    
    # âœ… BROADCAST JOIN (broadcast small table)
    print("\nâœ… BROADCAST JOIN (Explicit):")
    print("   â€¢ Products table broadcast to all executors")
    print("   â€¢ Sales table stays in place (no shuffle)")
    print("   â€¢ Network transfer: Only small table Ã— number of executors")
    
    start_time = time.time()
    df_broadcast_join = df_sales.join(broadcast(df_products), "product_id")
    result_count = df_broadcast_join.count()
    broadcast_time = time.time() - start_time
    
    print(f"   â€¢ Result: {result_count:,} rows")
    print(f"   â€¢ Time: {broadcast_time:.2f} seconds")
    df_broadcast_join.show(5, truncate=False)
    
    # Performance comparison
    print("\nğŸ“ˆ PERFORMANCE COMPARISON:")
    print(f"   Regular Join:   {regular_time:.2f} seconds")
    print(f"   Broadcast Join: {broadcast_time:.2f} seconds")
    if broadcast_time > 0:
        speedup = regular_time / broadcast_time
        print(f"   Speedup:        {speedup:.2f}Ã— faster")
    
    print("\nğŸ’¡ KEY INSIGHT:")
    print("   Broadcast join eliminates shuffle on large table.")
    print("   Cost: 100 rows Ã— 10 executors = 1,000 rows transferred")
    print("   Savings: 100,000 rows NOT shuffled!")


def example_2_automatic_broadcast():
    """
    Demonstrate automatic broadcast join detection.
    
    Spark automatically broadcasts tables < 10 MB (default threshold).
    """
    print("\n" + "="*70)
    print("EXAMPLE 2: Automatic Broadcast Detection")
    print("="*70)
    
    spark = create_spark_session()
    
    # Small table (will auto-broadcast)
    print("\nğŸ“¦ Creating dimension tables...")
    df_customers = spark.createDataFrame(
        [(i, f"Customer_{i}", f"Tier_{i % 3}") for i in range(1, 1001)],
        ["customer_id", "customer_name", "tier"]
    )
    
    df_regions = spark.createDataFrame(
        [(i, f"Region_{i}") for i in range(1, 11)],
        ["region_id", "region_name"]
    )
    
    # Large fact table
    print("ğŸ“Š Creating fact table...")
    df_orders = spark.createDataFrame(
        [
            (i, i % 1000 + 1, i % 10 + 1, i * 25.5)
            for i in range(1, 50001)
        ],
        ["order_id", "customer_id", "region_id", "amount"]
    )
    
    print(f"\n   Orders: {df_orders.count():,} rows")
    print(f"   Customers: {df_customers.count()} rows (< 10 MB â†’ auto-broadcast)")
    print(f"   Regions: {df_regions.count()} rows (< 10 MB â†’ auto-broadcast)")
    
    # Join without explicit broadcast() - Spark auto-detects
    print("\nğŸ” Joining without explicit broadcast() call...")
    print("   Spark will automatically broadcast small tables!")
    
    df_enriched = df_orders \
        .join(df_customers, "customer_id") \
        .join(df_regions, "region_id")
    
    print("\n   Result schema:")
    df_enriched.printSchema()
    
    df_enriched.show(5, truncate=False)
    
    print("\nğŸ’¡ CHECK SPARK UI:")
    print("   â€¢ Go to Spark UI â†’ SQL tab")
    print("   â€¢ Look for 'BroadcastHashJoin' in query plan")
    print("   â€¢ Confirms automatic broadcast optimization")
    
    # Check query plan
    print("\nğŸ“‹ QUERY PLAN (showing broadcast):")
    df_enriched.explain()
    
    print("\nâš™ï¸  BROADCAST THRESHOLD CONFIGURATION:")
    threshold = spark.conf.get("spark.sql.autoBroadcastJoinThreshold")
    print(f"   Current: {int(threshold):,} bytes ({int(threshold) / 1024 / 1024:.1f} MB)")
    print("   Adjust with: spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '20971520')")


def example_3_parquet_vs_csv():
    """
    Demonstrate Parquet file format advantages over CSV.
    
    COMPARISON:
    ----------
    â€¢ File size (compression)
    â€¢ Read speed (columnar access)
    â€¢ Column pruning (selective reads)
    â€¢ Predicate pushdown (filter early)
    """
    print("\n" + "="*70)
    print("EXAMPLE 3: Parquet vs CSV Comparison")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create sample dataset
    print("\nğŸ“Š Creating sample dataset (10,000 rows)...")
    df_data = spark.range(1, 10001) \
        .withColumn("name", col("id").cast("string")) \
        .withColumn("age", (rand() * 50 + 20).cast("int")) \
        .withColumn("salary", (rand() * 100000 + 30000).cast("double")) \
        .withColumn("department", (rand() * 10).cast("int").cast("string")) \
        .withColumn("hire_date", date_format(current_timestamp(), "yyyy-MM-dd"))
    
    df_data.show(5, truncate=False)
    
    # Write as CSV
    print("\nğŸ’¾ Writing as CSV...")
    csv_path = "/tmp/pyspark_data.csv"
    start_time = time.time()
    df_data.coalesce(1).write.mode("overwrite").option("header", "true").csv(csv_path)
    csv_write_time = time.time() - start_time
    print(f"   Write time: {csv_write_time:.2f} seconds")
    
    # Write as Parquet
    print("\nğŸ’¾ Writing as Parquet...")
    parquet_path = "/tmp/pyspark_data.parquet"
    start_time = time.time()
    df_data.coalesce(1).write.mode("overwrite").parquet(parquet_path)
    parquet_write_time = time.time() - start_time
    print(f"   Write time: {parquet_write_time:.2f} seconds")
    
    # Compare file sizes
    print("\nğŸ“ FILE SIZE COMPARISON:")
    import subprocess
    csv_size = subprocess.check_output(f"du -sh {csv_path}", shell=True).decode().split()[0]
    parquet_size = subprocess.check_output(f"du -sh {parquet_path}", shell=True).decode().split()[0]
    print(f"   CSV:     {csv_size}")
    print(f"   Parquet: {parquet_size}")
    print("   â†’ Parquet typically 5-10Ã— smaller due to compression")
    
    # Read speed comparison - FULL SCAN
    print("\nâ±ï¸  READ SPEED - Full Scan:")
    
    # CSV read
    start_time = time.time()
    df_csv = spark.read.option("header", "true").csv(csv_path)
    csv_count = df_csv.count()
    csv_read_time = time.time() - start_time
    print(f"   CSV:     {csv_read_time:.2f} seconds ({csv_count:,} rows)")
    
    # Parquet read
    start_time = time.time()
    df_parquet = spark.read.parquet(parquet_path)
    parquet_count = df_parquet.count()
    parquet_read_time = time.time() - start_time
    print(f"   Parquet: {parquet_read_time:.2f} seconds ({parquet_count:,} rows)")
    print(f"   â†’ Parquet {csv_read_time/parquet_read_time:.2f}Ã— faster")
    
    # Column pruning test
    print("\nğŸ“Š COLUMN PRUNING TEST (select 2 of 6 columns):")
    print("   CSV must read all columns, then discard unneeded ones.")
    print("   Parquet reads only requested columns from storage.")
    
    # CSV - column select
    start_time = time.time()
    df_csv_select = spark.read.option("header", "true").csv(csv_path).select("id", "salary")
    csv_select_count = df_csv_select.count()
    csv_select_time = time.time() - start_time
    print(f"\n   CSV (select 2 cols):     {csv_select_time:.2f} seconds")
    
    # Parquet - column select
    start_time = time.time()
    df_parquet_select = spark.read.parquet(parquet_path).select("id", "salary")
    parquet_select_count = df_parquet_select.count()
    parquet_select_time = time.time() - start_time
    print(f"   Parquet (select 2 cols): {parquet_select_time:.2f} seconds")
    print(f"   â†’ Parquet {csv_select_time/parquet_select_time:.2f}Ã— faster (column pruning!)")
    
    # Predicate pushdown test
    print("\nğŸ” PREDICATE PUSHDOWN TEST (filter on age > 40):")
    print("   Parquet can skip entire row groups based on metadata.")
    print("   CSV must scan all rows.")
    
    # CSV - filter
    start_time = time.time()
    df_csv_filter = spark.read.option("header", "true").csv(csv_path).filter(col("age") > 40)
    csv_filter_count = df_csv_filter.count()
    csv_filter_time = time.time() - start_time
    print(f"\n   CSV (filter):     {csv_filter_time:.2f} seconds ({csv_filter_count:,} rows)")
    
    # Parquet - filter
    start_time = time.time()
    df_parquet_filter = spark.read.parquet(parquet_path).filter(col("age") > 40)
    parquet_filter_count = df_parquet_filter.count()
    parquet_filter_time = time.time() - start_time
    print(f"   Parquet (filter): {parquet_filter_time:.2f} seconds ({parquet_filter_count:,} rows)")
    print(f"   â†’ Parquet {csv_filter_time/parquet_filter_time:.2f}Ã— faster (predicate pushdown!)")
    
    print("\nğŸ“Š SUMMARY:")
    print("   âœ… Parquet is smaller (compression)")
    print("   âœ… Parquet reads faster (columnar layout)")
    print("   âœ… Parquet supports column pruning (read only needed columns)")
    print("   âœ… Parquet supports predicate pushdown (skip data early)")


def example_4_parquet_compression_codecs():
    """
    Demonstrate different Parquet compression codecs.
    
    CODECS:
    ------
    â€¢ SNAPPY: Fast compression/decompression (default)
    â€¢ GZIP: Better compression, slower
    â€¢ LZ4: Fastest, less compression
    â€¢ ZSTD: Best balance (modern codec)
    â€¢ UNCOMPRESSED: No compression
    """
    print("\n" + "="*70)
    print("EXAMPLE 4: Parquet Compression Codecs")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create dataset
    print("\nğŸ“Š Creating sample dataset...")
    df = spark.range(1, 100001) \
        .withColumn("text", col("id").cast("string")) \
        .withColumn("value", (rand() * 1000).cast("double"))
    
    codecs = ["none", "snappy", "gzip", "lz4", "zstd"]
    
    print("\nğŸ’¾ Testing compression codecs...\n")
    
    results = []
    for codec in codecs:
        path = f"/tmp/parquet_{codec}"
        
        # Write
        start_time = time.time()
        df.write.mode("overwrite") \
            .option("compression", codec) \
            .parquet(path)
        write_time = time.time() - start_time
        
        # Get size
        import subprocess
        size_output = subprocess.check_output(f"du -sh {path}", shell=True).decode()
        size = size_output.split()[0]
        
        # Read
        start_time = time.time()
        df_read = spark.read.parquet(path)
        count = df_read.count()
        read_time = time.time() - start_time
        
        results.append({
            "codec": codec.upper(),
            "size": size,
            "write_time": write_time,
            "read_time": read_time
        })
        
        print(f"   {codec.upper():12s} | Size: {size:>8s} | Write: {write_time:>5.2f}s | Read: {read_time:>5.2f}s")
    
    print("\nğŸ“Š CODEC COMPARISON:")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Codec       â”‚ Size       â”‚ Write Speed  â”‚ Read Speed  â”‚")
    print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    for r in results:
        print(f"   â”‚ {r['codec']:11s} â”‚ {r['size']:>10s} â”‚ {r['write_time']:>10.2f}s â”‚ {r['read_time']:>9.2f}s â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\nğŸ’¡ CODEC SELECTION GUIDE:")
    print("   â€¢ SNAPPY (default):  Good balance, fast decompression")
    print("   â€¢ GZIP:              Best compression, slower (cold storage)")
    print("   â€¢ LZ4:               Fastest, use for hot data")
    print("   â€¢ ZSTD:              Modern, best compression + speed")
    print("   â€¢ UNCOMPRESSED:      Only for benchmarking or pre-compressed data")


def example_5_format_comparison():
    """
    Compare different file formats: CSV, JSON, Avro, Parquet, ORC.
    """
    print("\n" + "="*70)
    print("EXAMPLE 5: File Format Comparison")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create dataset with nested structure
    print("\nğŸ“Š Creating sample dataset (nested structure)...")
    df = spark.createDataFrame([
        (1, "Alice", 25, {"street": "123 Main", "city": "NYC"}, ["Python", "Spark"]),
        (2, "Bob", 30, {"street": "456 Oak", "city": "SF"}, ["Java", "Scala"]),
        (3, "Charlie", 35, {"street": "789 Elm", "city": "LA"}, ["Go", "Rust"])
    ] * 1000, ["id", "name", "age", "address", "skills"])
    
    print(f"   Rows: {df.count():,}")
    df.show(3, truncate=False)
    
    formats = {
        "csv": {"path": "/tmp/data.csv", "write_options": {"header": "true"}},
        "json": {"path": "/tmp/data.json", "write_options": {}},
        "parquet": {"path": "/tmp/data.parquet", "write_options": {}},
        # Note: Avro requires spark-avro package
        # "avro": {"path": "/tmp/data.avro", "write_options": {}},
    }
    
    print("\nğŸ’¾ Writing in different formats...")
    print("   " + "â”€" * 60)
    
    results = {}
    for fmt, config in formats.items():
        print(f"\n   {fmt.upper()}:")
        
        # Write
        start_time = time.time()
        writer = df.coalesce(1).write.mode("overwrite")
        for key, value in config["write_options"].items():
            writer = writer.option(key, value)
        
        if fmt == "csv":
            writer.csv(config["path"])
        elif fmt == "json":
            writer.json(config["path"])
        elif fmt == "parquet":
            writer.parquet(config["path"])
        
        write_time = time.time() - start_time
        
        # Get size
        import subprocess
        size_output = subprocess.check_output(f"du -sh {config['path']}", shell=True).decode()
        size = size_output.split()[0]
        
        # Read
        start_time = time.time()
        if fmt == "csv":
            df_read = spark.read.option("header", "true").csv(config["path"])
        elif fmt == "json":
            df_read = spark.read.json(config["path"])
        elif fmt == "parquet":
            df_read = spark.read.parquet(config["path"])
        
        count = df_read.count()
        read_time = time.time() - start_time
        
        results[fmt] = {
            "size": size,
            "write_time": write_time,
            "read_time": read_time
        }
        
        print(f"      Size: {size:>8s} | Write: {write_time:.2f}s | Read: {read_time:.2f}s")
    
    print("\n" + "   " + "â”€" * 60)
    print("\nğŸ“Š FORMAT COMPARISON TABLE:")
    print("""
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Format   â”‚ Structure   â”‚ Compression â”‚ Best For     â”‚ Use Case            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ CSV      â”‚ Row-based   â”‚ Poor        â”‚ Interchange  â”‚ Simple data export  â”‚
   â”‚ JSON     â”‚ Nested      â”‚ Poor        â”‚ APIs         â”‚ Semi-structured     â”‚
   â”‚ Parquet  â”‚ Columnar    â”‚ Excellent   â”‚ Analytics    â”‚ Data lake (best!)   â”‚
   â”‚ Avro     â”‚ Row-based   â”‚ Good        â”‚ Streaming    â”‚ Kafka, schema evo   â”‚
   â”‚ ORC      â”‚ Columnar    â”‚ Excellent   â”‚ Hive         â”‚ Hive warehouse      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def example_6_parquet_row_groups_vs_hdf5_chunks():
    """
    Demonstrate Parquet row group sizing and compare with HDF5 chunking.
    
    KEY CONCEPTS:
    ------------
    Parquet Row Groups:
    â€¢ Fixed size (128 MB default)
    â€¢ Automatic parallelism (1 row group = 1 Spark partition)
    â€¢ Columnar within each row group
    
    HDF5 Chunks:
    â€¢ User-defined size (access pattern dependent)
    â€¢ Multi-dimensional blocks
    â€¢ Must read entire chunk even for partial access
    """
    print("\n" + "="*70)
    print("EXAMPLE 6: Parquet Row Groups vs HDF5 Chunks")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create dataset with different row group sizes
    print("\nğŸ“Š Creating sample dataset (100,000 rows)...")
    df = spark.range(1, 100001) \
        .withColumn("value1", (rand() * 1000).cast("double")) \
        .withColumn("value2", (rand() * 1000).cast("double")) \
        .withColumn("value3", (rand() * 1000).cast("double"))
    
    print(f"   Rows: {df.count():,}")
    print(f"   Columns: {len(df.columns)}")
    
    # Test different row group sizes
    row_group_sizes = [
        ("32MB", 32 * 1024 * 1024),
        ("128MB", 128 * 1024 * 1024),  # Default
        ("512MB", 512 * 1024 * 1024)
    ]
    
    print("\nğŸ’¾ Writing Parquet with different row group sizes...\n")
    
    results = []
    for label, size in row_group_sizes:
        path = f"/tmp/parquet_rowgroup_{label}"
        
        # Write with specific row group size
        start_time = time.time()
        df.write.mode("overwrite") \
            .option("parquet.block.size", str(size)) \
            .parquet(path)
        write_time = time.time() - start_time
        
        # Get file info
        import subprocess
        file_size = subprocess.check_output(f"du -sh {path}", shell=True).decode().split()[0]
        
        # Count actual row groups (read Parquet metadata)
        df_read = spark.read.parquet(path)
        
        # Read with column pruning
        start_time = time.time()
        count = df_read.select("id", "value1").filter(col("value1") > 500).count()
        read_time = time.time() - start_time
        
        results.append({
            "label": label,
            "size": file_size,
            "write_time": write_time,
            "read_time": read_time,
            "filtered_rows": count
        })
        
        print(f"   {label:8s} | File: {file_size:>8s} | Write: {write_time:.2f}s | Read: {read_time:.2f}s")
    
    print("\nğŸ“Š ROW GROUP SIZE IMPACT:")
    print("""
   Small Row Groups (32 MB):
   â€¢ More parallelism (more partitions)
   â€¢ Lower memory per task
   â€¢ Slightly more metadata overhead
   â€¢ Better for: Large clusters, memory-constrained executors
   
   Default Row Groups (128 MB):
   â€¢ Balanced parallelism and compression
   â€¢ Standard for most workloads
   â€¢ Good predicate pushdown granularity
   â€¢ Better for: General purpose analytics
   
   Large Row Groups (512 MB):
   â€¢ Best compression (more context)
   â€¢ Less parallelism (fewer partitions)
   â€¢ Higher memory requirement
   â€¢ Better for: Small clusters, high-memory executors
    """)
    
    print("\nğŸ’¡ HDF5 COMPARISON:")
    print("""
   HDF5 Chunks are fundamentally different:
   
   Parquet Row Groups:
   â€¢ Organized by ROWS (horizontal slicing)
   â€¢ All columns for rows 1-100K in row group 1
   â€¢ Column pruning within row group (read only needed columns)
   â€¢ Automatic parallelism in Spark
   
   HDF5 Chunks:
   â€¢ Organized by N-DIMENSIONAL BLOCKS
   â€¢ For 3D array [1000, 1000, 365], chunk might be [100, 100, 1]
   â€¢ Must read entire chunk (all dimensions)
   â€¢ No automatic parallelism
   
   Example: Temperature data [lat, lon, day]
   
   Parquet (tabular):
   Row Group 1: rows 1-100K (all columns)
   Row Group 2: rows 100K-200K (all columns)
   Query "avg temp by lat": Read only 'lat' and 'temp' columns
   
   HDF5 (array):
   Chunk (0,0,0): temps[0:100, 0:100, 0]
   Chunk (0,0,1): temps[0:100, 0:100, 1]
   Query temps[:, 500, :]: Must read many chunks to get 1 longitude slice
    """)
    
    print("\nğŸ” KEY INSIGHT:")
    print("   Parquet row groups optimize for ANALYTICAL queries (columns, filtering)")
    print("   HDF5 chunks optimize for ARRAY SLICING (spatial/temporal regions)")
    print("   Parquet wins for distributed analytics, HDF5 wins for array computation")


def example_7_broadcast_join_with_parquet():
    """
    Complete example: Broadcast join with Parquet files.
    
    REAL-WORLD SCENARIO:
    -------------------
    â€¢ Large sales data stored as Parquet (optimized for analytics)
    â€¢ Small dimension tables (products, customers) as Parquet
    â€¢ Use broadcast joins for efficient star schema queries
    """
    print("\n" + "="*70)
    print("EXAMPLE 7: Broadcast Join + Parquet (Production Pattern)")
    print("="*70)
    
    spark = create_spark_session()
    
    # Create and save dimension tables as Parquet
    print("\nğŸ“¦ Creating dimension tables (Parquet)...")
    
    # Products dimension
    df_products = spark.createDataFrame([
        (f"P{i:04d}", f"Product_{i}", f"Category_{i % 5}", 50.0 + i)
        for i in range(1, 101)
    ], ["product_id", "product_name", "category", "unit_price"])
    
    products_path = "/tmp/warehouse/dim_products.parquet"
    df_products.write.mode("overwrite").parquet(products_path)
    print(f"   âœ… Products: {df_products.count()} rows â†’ {products_path}")
    
    # Customers dimension
    df_customers = spark.createDataFrame([
        (f"C{i:05d}", f"Customer_{i}", f"Region_{i % 10}")
        for i in range(1, 1001)
    ], ["customer_id", "customer_name", "region"])
    
    customers_path = "/tmp/warehouse/dim_customers.parquet"
    df_customers.write.mode("overwrite").parquet(customers_path)
    print(f"   âœ… Customers: {df_customers.count()} rows â†’ {customers_path}")
    
    # Create large fact table as Parquet
    print("\nğŸ“Š Creating fact table (Parquet)...")
    df_sales = spark.createDataFrame([
        (
            i,
            f"P{(i % 100) + 1:04d}",
            f"C{(i % 1000) + 1:05d}",
            i * 10.5,
            f"2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}"
        )
        for i in range(1, 100001)
    ], ["sale_id", "product_id", "customer_id", "amount", "sale_date"])
    
    sales_path = "/tmp/warehouse/fact_sales.parquet"
    df_sales.write.mode("overwrite") \
        .partitionBy("sale_date") \
        .option("compression", "snappy") \
        .parquet(sales_path)
    
    print(f"   âœ… Sales: {df_sales.count():,} rows â†’ {sales_path}")
    print("   ğŸ“ Partitioned by sale_date for efficient filtering")
    
    # Read back from Parquet
    print("\nğŸ“– Reading from Parquet warehouse...")
    df_products_read = spark.read.parquet(products_path)
    df_customers_read = spark.read.parquet(customers_path)
    df_sales_read = spark.read.parquet(sales_path)
    
    # Analytical query with broadcast joins
    print("\nğŸ” ANALYTICAL QUERY: Sales by category and region")
    print("   Using broadcast joins for dimension tables...")
    
    start_time = time.time()
    
    df_analysis = df_sales_read \
        .join(broadcast(df_products_read), "product_id") \
        .join(broadcast(df_customers_read), "customer_id") \
        .groupBy("category", "region") \
        .agg(
            _sum("amount").alias("total_sales"),
            avg("amount").alias("avg_sale"),
            count("*").alias("num_sales")
        ) \
        .orderBy(col("total_sales").desc())
    
    query_time = time.time() - start_time
    
    print(f"\n   Query completed in {query_time:.2f} seconds")
    print("\n   Top 10 Results:")
    df_analysis.show(10, truncate=False)
    
    # Show query plan
    print("\nğŸ“‹ QUERY PLAN (verify broadcast):")
    df_analysis.explain()
    
    print("\nğŸ’¡ PRODUCTION BENEFITS:")
    print("   âœ… Parquet: Compressed storage (5-10Ã— smaller)")
    print("   âœ… Parquet: Column pruning (read only needed columns)")
    print("   âœ… Parquet: Partitioned by date (skip irrelevant data)")
    print("   âœ… Broadcast: No shuffle on large fact table")
    print("   âœ… Result: Fast analytical queries on large datasets")


def main():
    """
    Run all broadcast join and Parquet examples.
    """
    print("\n" + "="*70)
    print(" BROADCAST JOINS AND PARQUET FILE FORMAT ")
    print("="*70)
    
    print("""
This module demonstrates:
1. Broadcast join optimization (eliminate shuffles)
2. Parquet columnar storage format
3. File format comparison (CSV, JSON, Parquet, etc.)
4. Production patterns (broadcast + Parquet)

KEY CONCEPTS:
------------
â€¢ Broadcast Join: Replicate small table to all executors
â€¢ Parquet: Columnar format optimized for analytics
â€¢ Column Pruning: Read only needed columns
â€¢ Predicate Pushdown: Filter data early using metadata
â€¢ Compression: 5-10Ã— smaller files with Parquet

WHEN TO USE:
-----------
Broadcast Join:
  âœ… Small table (< 10 MB)
  âœ… One-to-many relationships
  âœ… Star schema (dimension tables)
  
Parquet:
  âœ… Analytical workloads
  âœ… Data warehouse/lake
  âœ… Large datasets (TB+)
  âœ… Long-term storage
    """)
    
    try:
        example_1_broadcast_join_basics()
        example_2_automatic_broadcast()
        example_3_parquet_vs_csv()
        example_4_parquet_compression_codecs()
        example_5_format_comparison()
        example_6_parquet_row_groups_vs_hdf5_chunks()
        example_7_broadcast_join_with_parquet()
        
        print("\n" + "="*70)
        print("âœ… ALL EXAMPLES COMPLETED SUCCESSFULLY")
        print("="*70)
        
        print("\nğŸ“š NEXT STEPS:")
        print("   1. Check Spark UI for BroadcastHashJoin in query plans")
        print("   2. Compare file sizes: ls -lh /tmp/*.{csv,parquet}")
        print("   3. Experiment with broadcast threshold settings")
        print("   4. Try different compression codecs for your data")
        print("   5. Use Parquet for all production data lake storage")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
