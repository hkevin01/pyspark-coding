"""
02_performance_tuning.py
=========================

Spark Performance Tuning and Configuration

Demonstrates:
- Memory configuration
- Parallelism tuning
- Shuffle optimization
- Caching strategies
- Adaptive Query Execution (AQE)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, avg, count
import time


def create_optimized_spark():
    """
    Create Spark session with optimized configuration.
    """
    return SparkSession.builder \
        .appName("PerformanceTuning") \
        .master("local[*]") \
        .config("spark.executor.memory", "2g") \
        .config("spark.driver.memory", "1g") \
        .config("spark.memory.fraction", "0.6") \
        .config("spark.memory.storageFraction", "0.5") \
        .config("spark.sql.shuffle.partitions", "8") \
        .config("spark.default.parallelism", "8") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.autoBroadcastJoinThreshold", 10485760) \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()


def demonstrate_memory_tuning():
    """
    Memory configuration and tuning.
    """
    print("\n" + "=" * 80)
    print("MEMORY CONFIGURATION AND TUNING")
    print("=" * 80)
    
    print("\nğŸ“Š SPARK MEMORY MODEL:")
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              EXECUTOR MEMORY (e.g., 4GB)               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚      RESERVED MEMORY (300MB fixed)             â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚      USABLE MEMORY (3.7GB)                      â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚                                                 â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
    â”‚  â”‚  â”‚  SPARK MEMORY (60% = 2.22GB)            â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  (spark.memory.fraction = 0.6)          â”‚  â”‚  â”‚
    â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  â”‚
    â”‚  â”‚  â”‚                                          â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ STORAGE MEMORY (50% = 1.11GB)     â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ (spark.memory.storageFraction)    â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Cache                           â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Broadcast variables             â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚                                          â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ EXECUTION MEMORY (50% = 1.11GB)   â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Shuffles                        â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Joins                           â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Sorts                           â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚ â€¢ Aggregations                    â”‚ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚                                          â”‚  â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
    â”‚  â”‚                                                 â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
    â”‚  â”‚  â”‚  USER MEMORY (40% = 1.48GB)             â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â€¢ User data structures                 â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â€¢ UDFs                                  â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  â€¢ Python objects                        â”‚  â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
    â”‚  â”‚                                                 â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    print("\nâš™ï¸  KEY CONFIGURATION PARAMETERS:")
    print("""
    # Executor Memory
    --executor-memory 4g
    
    # Spark Memory Fraction (default: 0.6)
    --conf spark.memory.fraction=0.6
    
    # Storage vs Execution split (default: 0.5)
    --conf spark.memory.storageFraction=0.5
    
    # Driver Memory
    --driver-memory 2g
    """)
    
    print("\nğŸ¯ TUNING GUIDELINES:")
    print("""
    1. Executor Memory:
       â€¢ Too small â†’ frequent spills to disk
       â€¢ Too large â†’ GC overhead
       â€¢ Recommended: 8-32GB per executor
    
    2. Storage Fraction:
       â€¢ High (0.7) â†’ more caching, less execution memory
       â€¢ Low (0.3) â†’ less caching, more shuffle memory
       â€¢ Default (0.5) â†’ balanced
    
    3. Memory Fraction:
       â€¢ Default (0.6) works for most cases
       â€¢ Increase if lots of UDFs or Python objects
    """)
    
    print("\nğŸ’¡ COMMON MEMORY ISSUES:")
    print("""
    âŒ OutOfMemoryError:
       â†’ Increase executor memory
       â†’ Reduce cache usage
       â†’ Increase shuffle partitions
    
    âŒ GC Overhead:
       â†’ Reduce executor memory size
       â†’ Increase number of executors
       â†’ Use off-heap memory
    
    âŒ Shuffle Spill:
       â†’ Increase execution memory
       â†’ Increase shuffle partitions
       â†’ Enable compression
    """)


def demonstrate_parallelism_tuning(spark):
    """
    Parallelism and partition tuning.
    """
    print("\n" + "=" * 80)
    print("PARALLELISM AND PARTITION TUNING")
    print("=" * 80)
    
    # Create sample data
    df = spark.range(0, 1000000)
    
    print("\nğŸ“Š PARTITION METRICS:")
    print(f"   Default parallelism: {spark.sparkContext.defaultParallelism}")
    print(f"   Initial partitions: {df.rdd.getNumPartitions()}")
    
    print("\nâš™ï¸  KEY PARAMETERS:")
    print("""
    1. spark.default.parallelism
       â€¢ For RDD operations
       â€¢ Default: number of cores in cluster
       â€¢ Set to: 2-3x available cores
    
    2. spark.sql.shuffle.partitions
       â€¢ For DataFrame shuffle operations
       â€¢ Default: 200 (often too high!)
       â€¢ Set based on data size:
         - Small data (<1GB): 8-16
         - Medium data (1-10GB): 50-100
         - Large data (>10GB): 200-500
    """)
    
    # Demonstrate partition tuning
    print("\nğŸ§ª PARTITION TUNING EXAMPLE:")
    
    # Too many partitions (overhead)
    spark.conf.set("spark.sql.shuffle.partitions", "200")
    start = time.time()
    result1 = df.groupBy((col("id") % 10).alias("key")).count()
    result1.show()
    time_200 = time.time() - start
    print(f"   With 200 partitions: {time_200:.3f}s")
    print(f"   Partitions after shuffle: {result1.rdd.getNumPartitions()}")
    
    # Optimized partitions
    spark.conf.set("spark.sql.shuffle.partitions", "8")
    start = time.time()
    result2 = df.groupBy((col("id") % 10).alias("key")).count()
    result2.show()
    time_8 = time.time() - start
    print(f"   With 8 partitions: {time_8:.3f}s")
    print(f"   Partitions after shuffle: {result2.rdd.getNumPartitions()}")
    print(f"   Speedup: {time_200/time_8:.2f}x")
    
    print("\nğŸ¯ PARTITION SIZE GUIDELINES:")
    print("""
    Ideal partition size: 100-200MB
    
    Too many partitions:
    âŒ Small tasks â†’ scheduling overhead
    âŒ Too many small files on write
    
    Too few partitions:
    âŒ Large tasks â†’ memory pressure
    âŒ Poor parallelism
    âŒ Stragglers
    
    Calculate optimal partitions:
    partitions = data_size_MB / target_partition_size_MB
    partitions = 10GB / 128MB = ~80 partitions
    """)


def demonstrate_shuffle_optimization(spark):
    """
    Shuffle operation optimization.
    """
    print("\n" + "=" * 80)
    print("SHUFFLE OPTIMIZATION")
    print("=" * 80)
    
    df = spark.range(0, 100000)
    
    print("\nâš™ï¸  SHUFFLE CONFIGURATION:")
    print("""
    # Shuffle partitions (most important!)
    --conf spark.sql.shuffle.partitions=100
    
    # Shuffle compression (default: true)
    --conf spark.sql.shuffle.compress=true
    
    # Shuffle compression codec (default: lz4)
    --conf spark.io.compression.codec=lz4
    # Options: lz4 (fast), snappy (balanced), gzip (high compression)
    
    # Shuffle file buffer (default: 32k)
    --conf spark.shuffle.file.buffer=32k
    
    # Shuffle spill compression (default: true)
    --conf spark.shuffle.spill.compress=true
    
    # Reducer memory fraction for sorting (default: 0.2)
    --conf spark.shuffle.memoryFraction=0.2
    """)
    
    print("\nğŸ¯ SHUFFLE OPTIMIZATION TECHNIQUES:")
    print("""
    1. Filter Before Shuffle:
       âŒ df.groupBy("key").count().filter(col("count") > 10)
       âœ… df.filter(col("value") > 100).groupBy("key").count()
    
    2. Repartition by Join Key:
       df1.repartition("key").join(df2.repartition("key"), "key")
    
    3. Coalesce After Filter:
       df.filter(col("value") > 1000).coalesce(8)
    
    4. Use Broadcast for Small Tables:
       large_df.join(broadcast(small_df), "key")
    
    5. Enable Sort-Based Shuffle:
       --conf spark.shuffle.sort.bypassMergeThreshold=200
    """)
    
    # Demonstrate filter before shuffle
    print("\nğŸ§ª EXAMPLE: Filter Before Shuffle")
    
    # Bad: filter after shuffle
    print("   âŒ Filter AFTER shuffle:")
    start = time.time()
    bad = df.groupBy((col("id") % 100).alias("key")).count() \
        .filter(col("count") > 900)
    bad.show(5)
    time_bad = time.time() - start
    print(f"      Time: {time_bad:.3f}s")
    
    # Good: filter before shuffle
    print("   âœ… Filter BEFORE shuffle:")
    start = time.time()
    good = df.filter(col("id") > 10000) \
        .groupBy((col("id") % 100).alias("key")).count()
    good.show(5)
    time_good = time.time() - start
    print(f"      Time: {time_good:.3f}s")


def demonstrate_caching_strategies(spark):
    """
    Effective caching strategies.
    """
    print("\n" + "=" * 80)
    print("CACHING STRATEGIES")
    print("=" * 80)
    
    df = spark.range(0, 1000000) \
        .withColumn("value", col("id") * 2)
    
    print("\nğŸ“Š STORAGE LEVELS:")
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Storage Level            â”‚ Memory   â”‚ Disk     â”‚ Serialized  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ MEMORY_ONLY (default)    â”‚ Yes      â”‚ No       â”‚ No          â”‚
    â”‚ MEMORY_AND_DISK          â”‚ Yes      â”‚ Yes      â”‚ No          â”‚
    â”‚ MEMORY_ONLY_SER          â”‚ Yes      â”‚ No       â”‚ Yes         â”‚
    â”‚ MEMORY_AND_DISK_SER      â”‚ Yes      â”‚ Yes      â”‚ Yes         â”‚
    â”‚ DISK_ONLY                â”‚ No       â”‚ Yes      â”‚ Yes         â”‚
    â”‚ MEMORY_ONLY_2            â”‚ Yes (2x) â”‚ No       â”‚ No          â”‚
    â”‚ MEMORY_AND_DISK_2        â”‚ Yes (2x) â”‚ Yes      â”‚ No          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    2x = Replicated (fault tolerance)
    SER = Serialized (less memory, more CPU)
    """)
    
    print("\nğŸ¯ WHEN TO CACHE:")
    print("""
    âœ… DataFrame used multiple times
    âœ… Expensive transformations (joins, aggregations)
    âœ… Iterative algorithms (ML training)
    âœ… Interactive queries
    
    âŒ Used only once
    âŒ Before filtering (cache after filter!)
    âŒ Very large datasets (OOM risk)
    âŒ Frequent updates
    """)
    
    print("\nğŸ§ª CACHING DEMO:")
    
    # Without cache
    print("   Without cache (multiple actions):")
    start = time.time()
    df.count()
    df.filter(col("value") > 1000).count()
    df.groupBy((col("id") % 10).alias("key")).count().show(5, False)
    time_no_cache = time.time() - start
    print(f"      Total time: {time_no_cache:.3f}s")
    
    # With cache
    print("   With cache:")
    cached_df = df.cache()
    start = time.time()
    cached_df.count()  # Materializes cache
    cached_df.filter(col("value") > 1000).count()
    cached_df.groupBy((col("id") % 10).alias("key")).count().show(5, False)
    time_cache = time.time() - start
    print(f"      Total time: {time_cache:.3f}s")
    print(f"      Speedup: {time_no_cache/time_cache:.2f}x")
    
    # Cleanup
    cached_df.unpersist()
    
    print("\nğŸ’¡ CACHE MANAGEMENT:")
    print("""
    # Cache DataFrame
    df.cache()  # or df.persist()
    
    # Materialize cache
    df.count()  # or any action
    
    # Check cache status
    # Spark UI â†’ Storage tab
    
    # Remove from cache
    df.unpersist()
    
    # Clear all cache
    spark.catalog.clearCache()
    """)


def demonstrate_aqe(spark):
    """
    Adaptive Query Execution (AQE) features.
    """
    print("\n" + "=" * 80)
    print("ADAPTIVE QUERY EXECUTION (AQE)")
    print("=" * 80)
    
    print("\nâš™ï¸  AQE CONFIGURATION:")
    print("""
    # Enable AQE (Spark 3.0+)
    --conf spark.sql.adaptive.enabled=true
    
    # Coalesce partitions (combine small partitions)
    --conf spark.sql.adaptive.coalescePartitions.enabled=true
    --conf spark.sql.adaptive.coalescePartitions.minPartitionNum=1
    --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=64MB
    
    # Dynamic join strategy
    --conf spark.sql.adaptive.localShuffleReader.enabled=true
    
    # Skew join optimization
    --conf spark.sql.adaptive.skewJoin.enabled=true
    --conf spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
    --conf spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=256MB
    """)
    
    print("\nğŸ¯ AQE FEATURES:")
    print("""
    1. Dynamic Partition Coalescing:
       â€¢ Reduces shuffle partitions at runtime
       â€¢ Combines small partitions after shuffle
       â€¢ Saves overhead from empty/small tasks
    
    2. Dynamic Join Strategy Switching:
       â€¢ Switches to broadcast join if table is small
       â€¢ Decision made at runtime (not planning time)
       â€¢ Based on actual data size, not estimates
    
    3. Dynamic Skew Join Optimization:
       â€¢ Detects skewed partitions
       â€¢ Splits large partitions into smaller chunks
       â€¢ Improves parallelism for skewed data
    """)
    
    df = spark.range(0, 10000)
    
    # Demonstrate AQE partition coalescing
    print("\nğŸ§ª AQE DEMO: Partition Coalescing")
    spark.conf.set("spark.sql.shuffle.partitions", "200")
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    
    result = df.groupBy((col("id") % 10).alias("key")).count()
    result.explain()
    result.show()
    
    print("   With AQE enabled:")
    print("   â€¢ Started with 200 shuffle partitions")
    print("   â€¢ AQE coalesced to ~10 partitions (based on data size)")
    print("   â€¢ Check explain() for 'AQE' annotations")


def demonstrate_configuration_checklist():
    """
    Complete configuration checklist.
    """
    print("\n" + "=" * 80)
    print("PERFORMANCE TUNING CHECKLIST")
    print("=" * 80)
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                 SPARK PERFORMANCE TUNING CHECKLIST                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ“‹ MEMORY CONFIGURATION
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Set appropriate executor memory (8-32GB)
    â–¡ Configure driver memory based on collect size
    â–¡ Adjust memory.fraction if using many UDFs (0.5-0.7)
    â–¡ Tune storageFraction based on cache vs shuffle (0.3-0.7)
    
    ğŸ“‹ PARALLELISM
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Set spark.default.parallelism = 2-3x cores
    â–¡ Tune spark.sql.shuffle.partitions based on data size:
      â€¢ Small (<1GB): 8-16
      â€¢ Medium (1-10GB): 50-100
      â€¢ Large (>10GB): 200-500
    â–¡ Target partition size: 100-200MB
    
    ğŸ“‹ SHUFFLE OPTIMIZATION
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Enable shuffle compression (default: true)
    â–¡ Choose codec: lz4 (fast), snappy (balanced)
    â–¡ Filter before shuffle operations
    â–¡ Use broadcast for small tables (<10MB)
    â–¡ Repartition by join key for large-large joins
    
    ğŸ“‹ CACHING
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Cache frequently accessed DataFrames
    â–¡ Use appropriate storage level (MEMORY_AND_DISK safe default)
    â–¡ Cache AFTER filtering, not before
    â–¡ Unpersist when done
    â–¡ Monitor cache usage in Spark UI
    
    ğŸ“‹ ADAPTIVE QUERY EXECUTION (AQE)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Enable AQE (Spark 3.0+)
    â–¡ Enable partition coalescing
    â–¡ Enable skew join optimization
    â–¡ Let AQE switch join strategies dynamically
    
    ğŸ“‹ SERIALIZATION
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Use KryoSerializer (faster than Java)
    â–¡ Register classes with Kryo for better performance
    
    ğŸ“‹ DATA FORMAT
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Use Parquet (columnar, compressed, predicate pushdown)
    â–¡ Partition data by frequently filtered columns
    â–¡ Use appropriate compression (snappy default)
    
    ğŸ“‹ CODE OPTIMIZATION
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Filter early (predicate pushdown)
    â–¡ Select only needed columns (projection pruning)
    â–¡ Avoid UDFs (use built-in functions)
    â–¡ Use DataFrame API (not RDD) for Catalyst optimization
    â–¡ Check explain() plans
    
    ğŸ“‹ MONITORING
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â–¡ Monitor Spark UI (http://localhost:4040)
    â–¡ Check for data skew (task duration variance)
    â–¡ Monitor shuffle read/write sizes
    â–¡ Watch for spill to disk
    â–¡ Track GC time (should be <10% of task time)
    """)


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ¯" * 40)
    print("SPARK PERFORMANCE TUNING")
    print("ğŸ¯" * 40)
    
    spark = create_optimized_spark()
    
    demonstrate_memory_tuning()
    demonstrate_parallelism_tuning(spark)
    demonstrate_shuffle_optimization(spark)
    demonstrate_caching_strategies(spark)
    demonstrate_aqe(spark)
    demonstrate_configuration_checklist()
    
    print("\n" + "=" * 80)
    print("âœ… PERFORMANCE TUNING COMPLETE")
    print("=" * 80)
    
    print("\nğŸ“š Key Takeaways:")
    print("   1. Tune shuffle partitions based on data size")
    print("   2. Filter before shuffle operations")
    print("   3. Cache strategically (after filters)")
    print("   4. Enable AQE for automatic optimization")
    print("   5. Use Parquet with partitioning")
    print("   6. Monitor Spark UI continuously")
    print("   7. Target 100-200MB partition size")
    
    spark.stop()


if __name__ == "__main__":
    main()
