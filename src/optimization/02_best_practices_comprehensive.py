#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
PYSPARK OPTIMIZATION BEST PRACTICES - COMPREHENSIVE GUIDE
================================================================================

MODULE OVERVIEW:
----------------
This module demonstrates the CORRECT approaches to common PySpark optimization
pitfalls, explaining:
â€¢ WHAT the correct approach is
â€¢ WHY it's better than the anti-pattern
â€¢ WHEN to apply it (timing matters!)
â€¢ HOW to implement it properly

ANTI-PATTERNS vs BEST PRACTICES:
---------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ ANTI-PATTERN                    âœ… BEST PRACTICE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ collect() on large data            show(n) or write to storage             â”‚
â”‚ â†’ Driver OOM crash                 â†’ Controlled memory usage               â”‚
â”‚                                                                             â”‚
â”‚ Loading model per row              Broadcast model once                    â”‚
â”‚ â†’ 10M rows = 10M model loads       â†’ Load once, share across executors    â”‚
â”‚                                                                             â”‚
â”‚ Ignoring data skew                 Salt keys + repartition                 â”‚
â”‚ â†’ 1 partition takes 99% time       â†’ Balanced work distribution           â”‚
â”‚                                                                             â”‚
â”‚ Schema inference                   Explicit schema definition              â”‚
â”‚ â†’ Slow, type errors                â†’ Fast, type-safe                       â”‚
â”‚                                                                             â”‚
â”‚ No caching                         Cache frequently-used DFs               â”‚
â”‚ â†’ Recompute from source            â†’ Compute once, reuse many times       â”‚
â”‚                                                                             â”‚
â”‚ UDFs for everything                Spark SQL functions first               â”‚
â”‚ â†’ Python serialization overhead    â†’ Native Spark execution (10-100Ã— faster)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIMING IS CRITICAL:
-------------------

The ORDER and TIMING of optimizations matter! Apply them at the right stage:

1. DEFINE SCHEMA          â†’ At data load (before any processing)
2. BROADCAST MODELS       â†’ Before map/filter operations that use them
3. CACHE STRATEGICALLY    â†’ After expensive transformations, before reuse
4. USE SPARK SQL FIRST    â†’ During transformation logic
5. HANDLE SKEW            â†’ When joins/aggregations are slow
6. AVOID COLLECT()        â†’ At output stage (use show/write instead)

Think of it like cooking:
â€¢ Add salt at the right time (not all at the end!)
â€¢ Prep ingredients before cooking (schema definition)
â€¢ Use proper tools (Spark SQL not UDFs)
â€¢ Don't bring entire ocean to your kitchen (no collect())

PERFORMANCE IMPACT:
-------------------

Real-world speedups from these practices:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Optimization                 â”‚ Typical Speedupâ”‚ Memory Savings â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Explicit schema vs inference â”‚ 2-5Ã—           â”‚ 20-30%         â”‚
â”‚ Broadcast vs repeated load   â”‚ 10-100Ã—        â”‚ 90%+           â”‚
â”‚ Caching frequently-used DFs  â”‚ 3-10Ã—          â”‚ Varies         â”‚
â”‚ Spark SQL vs UDFs            â”‚ 10-100Ã—        â”‚ 50%+           â”‚
â”‚ Salting skewed keys          â”‚ 5-50Ã—          â”‚ Better balance â”‚
â”‚ show() vs collect()          â”‚ Infinite       â”‚ Prevents OOM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 1.0.0 - Optimization Best Practices
UPDATED: December 2024
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, lit, rand, concat, expr, broadcast,
    count, sum as spark_sum, avg, max as spark_max,
    explode, array, struct, to_json, from_json,
    udf, pandas_udf, PandasUDFType
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, TimestampType, ArrayType, MapType, LongType
)
from pyspark.broadcast import Broadcast
import time
import numpy as np
import pandas as pd
from typing import Iterator


def create_spark_session():
    """
    Create SparkSession with optimized configuration.
    
    WHY these configurations:
    â€¢ adaptive.enabled: Let Spark optimize at runtime
    â€¢ shuffle.partitions: Default 200 too high for small data, too low for large
    â€¢ memory settings: Prevent OOM, enable caching
    """
    return SparkSession.builder \
        .appName("OptimizationBestPractices") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.shuffle.partitions", "auto") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.memory.fraction", "0.8") \
        .config("spark.memory.storageFraction", "0.3") \
        .getOrCreate()


def best_practice_1_explicit_schema(spark: SparkSession):
    """
    âœ… BEST PRACTICE #1: Define Schema Explicitly
    
    WHEN: At data load time, BEFORE any processing
    WHY: Fast loading, type safety, memory efficiency
    HOW: Define StructType with exact column types
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df = spark.read.csv("data.csv", header=True, inferSchema=True)
    
    Problems:
    â€¢ Reads data TWICE (once for inference, once for loading)
    â€¢ May infer wrong types (e.g., "123" could be String or Int)
    â€¢ Slower startup (scans all data to infer)
    â€¢ Memory waste (wrong types use more memory)
    
    âœ… Best Practice:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    schema = StructType([...])
    df = spark.read.csv("data.csv", header=True, schema=schema)
    
    Benefits:
    â€¢ Reads data ONCE (50% faster)
    â€¢ Guaranteed type safety (no surprises)
    â€¢ Better memory usage (correct types)
    â€¢ Catches errors early (invalid data fails fast)
    """
    print("=" * 80)
    print("BEST PRACTICE #1: EXPLICIT SCHEMA DEFINITION")
    print("=" * 80)
    
    # âŒ WRONG: Schema inference (slow, unreliable)
    print("\nâŒ Anti-Pattern: Schema Inference")
    print("   df = spark.read.csv(..., inferSchema=True)")
    print("   â€¢ Reads data twice")
    print("   â€¢ May infer wrong types")
    print("   â€¢ Slower startup\n")
    
    # âœ… CORRECT: Explicit schema
    print("âœ… Best Practice: Explicit Schema Definition")
    print("   WHEN: At data load, BEFORE processing")
    print("   WHY: Fast, type-safe, memory-efficient\n")
    
    # Define schema explicitly
    sales_schema = StructType([
        StructField("transaction_id", StringType(), False),      # NOT NULL
        StructField("customer_id", StringType(), False),
        StructField("product_id", StringType(), False),
        StructField("category", StringType(), True),              # NULLABLE
        StructField("amount", DoubleType(), False),
        StructField("quantity", IntegerType(), False),
        StructField("timestamp", TimestampType(), False),
        StructField("region", StringType(), True),
        StructField("payment_method", StringType(), True)
    ])
    
    print("Schema defined:")
    print(sales_schema.simpleString())
    
    # Create sample data
    sample_data = [
        ("TXN001", "C001", "P001", "Electronics", 1200.50, 1, "2024-01-15 10:30:00", "US-WEST", "Credit"),
        ("TXN002", "C002", "P002", "Electronics", 899.99, 2, "2024-01-15 11:45:00", "US-EAST", "Debit"),
        ("TXN003", "C001", "P003", "Clothing", 49.99, 3, "2024-01-15 12:00:00", "US-WEST", "Credit"),
        ("TXN004", "C003", "P001", "Electronics", 1200.50, 1, "2024-01-15 13:15:00", "EU", "PayPal"),
        ("TXN005", "C002", "P004", "Books", 29.99, 5, "2024-01-15 14:30:00", "US-EAST", "Credit")
    ] * 200  # Simulate 1000 rows
    
    # Apply schema at load time (WHEN: Now, at the start!)
    df_sales = spark.createDataFrame(sample_data, schema=sales_schema)
    
    print(f"\nâœ… DataFrame created with explicit schema")
    print(f"   Rows: {df_sales.count():,}")
    print(f"   Columns: {len(df_sales.columns)}")
    print(f"\n   Benefits:")
    print(f"   â€¢ Type safety guaranteed")
    print(f"   â€¢ Fast load (single pass)")
    print(f"   â€¢ Memory optimized")
    
    df_sales.printSchema()
    df_sales.show(3, truncate=False)
    
    return df_sales


def best_practice_2_broadcast_models(spark: SparkSession, df: "DataFrame"):
    """
    âœ… BEST PRACTICE #2: Broadcast Models and Lookup Tables
    
    WHEN: BEFORE map/filter/UDF operations that use the model/lookup
    WHY: Load once, share across all executors (not once per row!)
    HOW: Use spark.sparkContext.broadcast() or broadcast() hint
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def predict_udf(features):
        model = load_model()  # âŒ Loads model FOR EACH ROW!
        return model.predict(features)
    
    If you have 10M rows â†’ loads model 10M times!
    â€¢ Extremely slow (I/O bottleneck)
    â€¢ Wastes memory (multiple copies)
    â€¢ Network overhead (repeated loads)
    
    âœ… Best Practice:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    model = load_model()
    broadcast_model = spark.sparkContext.broadcast(model)
    
    def predict_udf(features):
        return broadcast_model.value.predict(features)  # âœ… Reuses model!
    
    Loads model ONCE per executor (not per row):
    â€¢ 10M rows â†’ loads model ~10 times (one per executor)
    â€¢ 1,000,000Ã— fewer loads!
    â€¢ Dramatic speedup (10-100Ã—)
    
    TIMING: Broadcast BEFORE the operation that uses it!
    """
    print("\n" + "=" * 80)
    print("BEST PRACTICE #2: BROADCAST MODELS AND LOOKUP TABLES")
    print("=" * 80)
    
    # âŒ WRONG: Load model/lookup in UDF (per row)
    print("\nâŒ Anti-Pattern: Load Model Per Row")
    print("   def predict_udf(x):")
    print("       model = load_model()  # âŒ Loads for EACH row!")
    print("       return model.predict(x)")
    print("\n   Impact: 10M rows = 10M model loads = DISASTER\n")
    
    # âœ… CORRECT: Broadcast model once
    print("âœ… Best Practice: Broadcast Model Once")
    print("   WHEN: BEFORE map/filter/UDF operations")
    print("   WHY: Load once per executor, not per row\n")
    
    # Simulate a "model" (pricing rules lookup table)
    # In production, this could be a ML model, config dict, etc.
    pricing_rules = {
        "Electronics": {"discount": 0.10, "tax": 0.08, "shipping": 15.99},
        "Clothing": {"discount": 0.15, "tax": 0.06, "shipping": 5.99},
        "Books": {"discount": 0.05, "tax": 0.00, "shipping": 3.99},
        "default": {"discount": 0.00, "tax": 0.07, "shipping": 9.99}
    }
    
    print("Pricing rules (simulated model):")
    for category, rules in list(pricing_rules.items())[:3]:
        print(f"  {category}: {rules}")
    
    # TIMING: Broadcast NOW, before we use it!
    # This is THE RIGHT TIME - after defining, before using
    broadcast_rules = spark.sparkContext.broadcast(pricing_rules)
    
    print(f"\nâœ… Model broadcast to all executors")
    print(f"   Size: {len(pricing_rules)} categories")
    print(f"   Cost: Load once per executor (not per row!)")
    
    # Define UDF that uses broadcast variable
    # Note: It accesses broadcast_rules.value (not loading fresh each time)
    def calculate_final_price_udf(category: str, amount: float) -> float:
        """
        Calculate final price with discount, tax, and shipping.
        
        WHY this works well:
        â€¢ broadcast_rules.value gives us the model (already loaded!)
        â€¢ No I/O per row
        â€¢ No repeated deserialization
        â€¢ Shared across all tasks on this executor
        """
        rules = broadcast_rules.value.get(category, broadcast_rules.value["default"])
        discounted = amount * (1 - rules["discount"])
        with_tax = discounted * (1 + rules["tax"])
        final = with_tax + rules["shipping"]
        return round(final, 2)
    
    # Register UDF
    calculate_price_spark_udf = udf(calculate_final_price_udf, DoubleType())
    
    # Apply UDF (using broadcast model)
    df_with_prices = df.withColumn(
        "final_price",
        calculate_price_spark_udf(col("category"), col("amount"))
    )
    
    print("\nâœ… Applied UDF using broadcast model")
    print("   Each executor loads model ONCE")
    print("   All rows on that executor reuse the same model")
    
    df_with_prices.select(
        "transaction_id", "category", "amount", "final_price"
    ).show(5, truncate=False)
    
    print("\nğŸ’¡ Performance Impact:")
    print("   âŒ Without broadcast: 1000 rows = 1000 model loads")
    print("   âœ… With broadcast: 1000 rows = ~10 model loads (one per executor)")
    print("   ğŸš€ Speedup: ~100Ã— faster!")
    
    return df_with_prices, broadcast_rules


def best_practice_3_avoid_collect(spark: SparkSession, df: "DataFrame"):
    """
    âœ… BEST PRACTICE #3: Never collect() Large Data to Driver
    
    WHEN: At output/inspection stage
    WHY: Driver has limited memory (will crash with OOM)
    HOW: Use show(n), write(), or aggregations instead
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results = df.collect()  # âŒ Brings ALL data to driver!
    for row in results:
        print(row)
    
    Problems:
    â€¢ Driver OOM crash (driver has ~4-16GB, data has 500GB)
    â€¢ Defeats purpose of distributed computing
    â€¢ Single point of failure
    â€¢ Network bottleneck (all executors â†’ one driver)
    
    âœ… Best Practice:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # For inspection:
    df.show(20)              # Show sample (doesn't load all data)
    
    # For analysis:
    df.groupBy("category").count().show()  # Aggregate first!
    
    # For output:
    df.write.parquet("output/")  # Distributed write (each executor writes)
    
    # For small results:
    summary = df.groupBy("region").count().collect()  # OK (few rows)
    
    TIMING: At the END, when outputting results
    """
    print("\n" + "=" * 80)
    print("BEST PRACTICE #3: AVOID collect() ON LARGE DATA")
    print("=" * 80)
    
    # âŒ WRONG: collect() all data to driver
    print("\nâŒ Anti-Pattern: collect() Large Data")
    print("   results = df.collect()  # âŒ Brings ALL data to driver!")
    print("   for row in results:")
    print("       process(row)")
    print("\n   Impact:")
    print("   â€¢ Driver OOM crash (500GB data â†’ 4GB driver)")
    print("   â€¢ Defeats distributed computing")
    print("   â€¢ Single point of failure\n")
    
    # âœ… CORRECT: Use appropriate alternatives
    print("âœ… Best Practice: Use Alternatives to collect()")
    print("   WHEN: At output stage, for inspection or writing")
    print("   WHY: Driver has limited memory\n")
    
    print("ğŸ“Š OPTION 1: show() for inspection")
    print("   df.show(20)  # Only brings 20 rows to driver")
    df.show(5, truncate=False)
    
    print("\nğŸ“Š OPTION 2: Aggregate first, then collect()")
    print("   summary = df.groupBy('category').count().collect()")
    print("   âœ… OK because result is small (few categories)\n")
    
    summary = df.groupBy("category").agg(
        count("*").alias("count"),
        spark_sum("amount").alias("total_revenue"),
        avg("amount").alias("avg_amount")
    )
    summary.show(truncate=False)
    
    # This collect() is OK because summary is small (few rows)
    summary_rows = summary.collect()
    print(f"   âœ… Collected {len(summary_rows)} rows (SMALL result set)")
    
    print("\nğŸ“Š OPTION 3: Write to storage (distributed)")
    print("   df.write.parquet('output/')  # Each executor writes its partitions")
    print("   âœ… No driver bottleneck, truly distributed")
    
    # Simulate writing (would write to disk in production)
    print("\n   Partitions before write:")
    print(f"   â€¢ Current partitions: {df.rdd.getNumPartitions()}")
    print(f"   â€¢ Each executor writes its own partitions")
    print(f"   â€¢ Driver only coordinates, doesn't handle data")
    
    print("\nğŸ’¡ Memory Comparison:")
    print("   Scenario: 100GB data, 4GB driver memory")
    print("   âŒ collect(): Tries to load 100GB â†’ OOM crash")
    print("   âœ… show(20): Loads ~1KB â†’ works perfectly")
    print("   âœ… write(): Each executor writes its chunk â†’ works perfectly")
    
    print("\nğŸ¯ RULE OF THUMB:")
    print("   â€¢ Inspecting: show(n) or take(n)")
    print("   â€¢ Analyzing: Aggregate first, then collect()")
    print("   â€¢ Outputting: write() to storage")
    print("   â€¢ Never: collect() entire large DataFrame")
    
    return df


def best_practice_4_handle_data_skew(spark: SparkSession, df: "DataFrame"):
    """
    âœ… BEST PRACTICE #4: Handle Data Skew
    
    WHEN: When joins/aggregations are slow with uneven partition sizes
    WHY: One giant partition blocks entire job (Spark is only as fast as slowest task)
    HOW: Salt keys, repartition, increase parallelism
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df.groupBy("customer_id").count()  # âŒ If one customer has 90% of data!
    
    Problems:
    â€¢ One partition has 90% of data (whale customer)
    â€¢ Other partitions finish in 1 second
    â€¢ Giant partition takes 100 seconds
    â€¢ Total time: 100 seconds (limited by slowest!)
    â€¢ 99% of cluster sits idle waiting
    
    âœ… Best Practice: SALTING
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Add random salt to split hot key
    df_salted = df.withColumn("salt", (rand() * 10).cast("int"))
    df_salted = df_salted.withColumn(
        "customer_id_salted",
        concat(col("customer_id"), lit("_"), col("salt"))
    )
    
    # Group by salted key (splits whale customer into 10 pieces)
    df_partial = df_salted.groupBy("customer_id_salted").count()
    
    # Remove salt and combine
    df_final = df_partial.withColumn(
        "customer_id",
        expr("substring(customer_id_salted, 1, length(customer_id_salted) - 2)")
    ).groupBy("customer_id").sum("count")
    
    Result:
    â€¢ Whale customer split across 10 partitions
    â€¢ Each partition: ~10 seconds (instead of 1 waiting for 100)
    â€¢ Total time: ~10 seconds
    â€¢ 10Ã— speedup!
    
    TIMING: Apply WHEN you detect skew (slow stages in Spark UI)
    """
    print("\n" + "=" * 80)
    print("BEST PRACTICE #4: HANDLE DATA SKEW")
    print("=" * 80)
    
    # âŒ WRONG: Ignore skew
    print("\nâŒ Anti-Pattern: Ignore Data Skew")
    print("   df.groupBy('customer_id').count()")
    print("   Problem: If C001 has 90% of data:")
    print("   â€¢ 1 partition: 90% of work (takes 100 seconds)")
    print("   â€¢ 9 partitions: 10% of work (takes 1 second each)")
    print("   â€¢ Total time: 100 seconds (limited by slowest!)")
    print("   â€¢ 99% of cluster idle waiting for that 1 partition\n")
    
    # âœ… CORRECT: Salt skewed keys
    print("âœ… Best Practice: Salt Skewed Keys")
    print("   WHEN: When joins/aggregations show uneven partition sizes")
    print("   WHY: Split hot key across multiple partitions")
    print("   HOW: Add random salt, group twice\n")
    
    # Simulate skewed data (C001 has 70% of transactions)
    skewed_data = [
        ("TXN_C001_" + str(i), "C001", "P001", "Electronics", 100.0, 1)
        for i in range(700)  # Customer C001: 700 transactions
    ] + [
        ("TXN_C002_" + str(i), "C002", "P002", "Books", 50.0, 1)
        for i in range(100)  # Customer C002: 100 transactions
    ] + [
        ("TXN_C003_" + str(i), "C003", "P003", "Clothing", 75.0, 1)
        for i in range(200)  # Customer C003: 200 transactions
    ]
    
    skewed_schema = StructType([
        StructField("transaction_id", StringType(), False),
        StructField("customer_id", StringType(), False),
        StructField("product_id", StringType(), False),
        StructField("category", StringType(), False),
        StructField("amount", DoubleType(), False),
        StructField("quantity", IntegerType(), False)
    ])
    
    df_skewed = spark.createDataFrame(skewed_data, schema=skewed_schema)
    
    print("ğŸ“Š Skewed Data Distribution:")
    df_skewed.groupBy("customer_id").count().orderBy(col("count").desc()).show()
    print("   âš ï¸  C001 has 70% of data (700/1000 transactions)")
    print("   This will cause skew in groupBy/join operations!\n")
    
    # STEP 1: Add salt (TIMING: Now, before expensive operations)
    print("STEP 1: Add Salt to Split Hot Keys")
    print("   WHY: rand() * 10 creates uniform distribution 0-9")
    print("   EFFECT: C001 becomes C001_0, C001_1, ..., C001_9\n")
    
    df_salted = df_skewed.withColumn(
        "salt",
        (rand() * 10).cast("int")  # Random 0-9
    ).withColumn(
        "customer_id_salted",
        concat(col("customer_id"), lit("_"), col("salt"))  # C001 â†’ C001_0, C001_1, etc.
    )
    
    print("âœ… Salt added:")
    df_salted.select("customer_id", "salt", "customer_id_salted").show(10)
    
    # STEP 2: Partial aggregation (with salt)
    print("\nSTEP 2: Partial Aggregation (with salt)")
    print("   WHY: Distribute C001's work across 10 partitions")
    print("   EFFECT: Each partition handles ~70 rows instead of 700\n")
    
    df_partial = df_salted.groupBy("customer_id", "customer_id_salted").agg(
        count("*").alias("partial_count"),
        spark_sum("amount").alias("partial_sum")
    )
    
    print("âœ… Partial aggregation (C001 split into 10 pieces):")
    df_partial.filter(col("customer_id") == "C001").show(10)
    
    # STEP 3: Final aggregation (remove salt)
    print("\nSTEP 3: Final Aggregation (remove salt)")
    print("   WHY: Combine the 10 partial results for C001")
    print("   EFFECT: C001_0 + C001_1 + ... + C001_9 = C001 total\n")
    
    df_final = df_partial.groupBy("customer_id").agg(
        spark_sum("partial_count").alias("total_transactions"),
        spark_sum("partial_sum").alias("total_revenue")
    )
    
    print("âœ… Final aggregation (salt removed):")
    df_final.orderBy(col("total_transactions").desc()).show()
    
    print("\nğŸ’¡ Performance Impact:")
    print("   âŒ Without salting:")
    print("      â€¢ C001 partition: 700 rows â†’ takes 100 seconds")
    print("      â€¢ Other partitions: done in 1 second")
    print("      â€¢ Total time: 100 seconds")
    print("      â€¢ Resource utilization: 10% (one partition working)")
    print("\n   âœ… With salting:")
    print("      â€¢ C001 split across 10 partitions: 70 rows each")
    print("      â€¢ Each partition: ~10 seconds")
    print("      â€¢ Total time: ~10 seconds")
    print("      â€¢ Resource utilization: 100% (all partitions working)")
    print("      â€¢ ğŸš€ Speedup: 10Ã— faster!")
    
    print("\nğŸ¯ When to Apply Salting:")
    print("   â€¢ Spark UI shows uneven partition sizes")
    print("   â€¢ One task takes much longer than others")
    print("   â€¢ Join with hot keys (few keys have lots of data)")
    print("   â€¢ GroupBy with skewed distribution")
    
    return df_final


def best_practice_5_cache_strategically(spark: SparkSession, df: "DataFrame"):
    """
    âœ… BEST PRACTICE #5: Cache Frequently-Used DataFrames
    
    WHEN: After expensive transformations, BEFORE multiple reuses
    WHY: Avoid recomputing from source multiple times
    HOW: Use .cache() or .persist() strategically
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_expensive = df.join(...).filter(...).groupBy(...)  # Expensive operations
    
    result1 = df_expensive.filter(condition1).count()     # Computes from source
    result2 = df_expensive.filter(condition2).count()     # Computes from source AGAIN
    result3 = df_expensive.filter(condition3).count()     # Computes from source AGAIN
    
    3 actions = 3 full recomputations of join + filter + groupBy!
    
    âœ… Best Practice:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_expensive = df.join(...).filter(...).groupBy(...)
    df_expensive.cache()  # TIMING: After expensive ops, BEFORE reuse!
    
    result1 = df_expensive.filter(condition1).count()     # Computes & caches
    result2 = df_expensive.filter(condition2).count()     # Reads from cache!
    result3 = df_expensive.filter(condition3).count()     # Reads from cache!
    
    1 computation + 2 cache reads = 3Ã— faster!
    
    TIMING:
    â€¢ TOO EARLY: Caching raw data (not expensive yet)
    â€¢ JUST RIGHT: After expensive transformations, before reuse
    â€¢ TOO LATE: After all uses (no benefit)
    
    Think: "I will use this DF multiple times, cache it NOW!"
    """
    print("\n" + "=" * 80)
    print("BEST PRACTICE #5: CACHE STRATEGICALLY")
    print("=" * 80)
    
    # âŒ WRONG: No caching (recomputes every time)
    print("\nâŒ Anti-Pattern: No Caching")
    print("   df_exp = df.join(...).filter(...).groupBy(...)")
    print("   result1 = df_exp.filter(cond1).count()  # Full computation")
    print("   result2 = df_exp.filter(cond2).count()  # Full computation AGAIN")
    print("   result3 = df_exp.filter(cond3).count()  # Full computation AGAIN")
    print("\n   Impact: 3 actions = 3 full recomputations = 3Ã— slower\n")
    
    # âœ… CORRECT: Cache after expensive ops, before reuse
    print("âœ… Best Practice: Cache After Expensive Transformations")
    print("   WHEN: After expensive ops, BEFORE multiple reuses")
    print("   WHY: Compute once, reuse many times\n")
    
    # Simulate expensive transformation
    print("STEP 1: Expensive Transformations (join + aggregation)")
    
    # Create a customers DataFrame for join
    customers_data = [
        ("C001", "Alice", "Gold", "US-WEST"),
        ("C002", "Bob", "Silver", "US-EAST"),
        ("C003", "Charlie", "Bronze", "EU")
    ]
    customers_schema = StructType([
        StructField("customer_id", StringType(), False),
        StructField("name", StringType(), False),
        StructField("tier", StringType(), False),
        StructField("region", StringType(), False)
    ])
    df_customers = spark.createDataFrame(customers_data, schema=customers_schema)
    
    # Expensive transformation (join + aggregation)
    df_expensive = df.join(
        df_customers,
        on="customer_id",
        how="inner"
    ).groupBy("customer_id", "name", "tier", "region").agg(
        count("*").alias("transaction_count"),
        spark_sum("amount").alias("total_spent"),
        avg("amount").alias("avg_transaction"),
        spark_max("amount").alias("max_transaction")
    )
    
    print("   âœ… Expensive DataFrame created (join + aggregation)")
    print("   This is expensive to compute!\n")
    
    # TIMING: Cache NOW (after expensive ops, before reuse)!
    print("STEP 2: Cache Now (Before Multiple Uses)")
    print("   â° TIMING: Right after expensive transformation")
    print("   â“ WHY NOW: About to use this DF 3 times\n")
    
    df_expensive.cache()  # Cache in memory
    
    print("   âœ… DataFrame cached")
    print("   First action will compute & cache")
    print("   Subsequent actions will read from cache\n")
    
    # Trigger caching with first action
    print("STEP 3: First Use (Computes and Caches)")
    print("   Query 1: Count Gold tier customers")
    start_time = time.time()
    count_gold = df_expensive.filter(col("tier") == "Gold").count()
    time_1 = time.time() - start_time
    print(f"   âœ… Gold customers: {count_gold} (computed in {time_1:.3f}s)")
    print("   â€¢ Ran full computation (join + aggregation)")
    print("   â€¢ Stored result in cache\n")
    
    # Second use (reads from cache)
    print("STEP 4: Second Use (Reads from Cache)")
    print("   Query 2: Count Silver tier customers")
    start_time = time.time()
    count_silver = df_expensive.filter(col("tier") == "Silver").count()
    time_2 = time.time() - start_time
    print(f"   âœ… Silver customers: {count_silver} (computed in {time_2:.3f}s)")
    print("   â€¢ Read from cache (no recomputation!)")
    print(f"   â€¢ {time_1/max(time_2, 0.001):.1f}Ã— faster than first query\n")
    
    # Third use (reads from cache)
    print("STEP 5: Third Use (Reads from Cache)")
    print("   Query 3: High spenders (total_spent > 1000)")
    start_time = time.time()
    high_spenders = df_expensive.filter(col("total_spent") > 1000)
    count_high = high_spenders.count()
    time_3 = time.time() - start_time
    print(f"   âœ… High spenders: {count_high} (computed in {time_3:.3f}s)")
    print("   â€¢ Read from cache (no recomputation!)")
    print(f"   â€¢ {time_1/max(time_3, 0.001):.1f}Ã— faster than first query\n")
    
    # Show results
    print("ğŸ“Š Customer Analysis Results:")
    df_expensive.show(truncate=False)
    
    print("\nğŸ’¡ Performance Impact:")
    print(f"   âŒ Without caching:")
    print(f"      â€¢ Query 1: {time_1:.3f}s (full computation)")
    print(f"      â€¢ Query 2: {time_1:.3f}s (full computation again)")
    print(f"      â€¢ Query 3: {time_1:.3f}s (full computation again)")
    print(f"      â€¢ Total: {time_1 * 3:.3f}s")
    print(f"\n   âœ… With caching:")
    print(f"      â€¢ Query 1: {time_1:.3f}s (compute & cache)")
    print(f"      â€¢ Query 2: {time_2:.3f}s (read from cache)")
    print(f"      â€¢ Query 3: {time_3:.3f}s (read from cache)")
    print(f"      â€¢ Total: {time_1 + time_2 + time_3:.3f}s")
    print(f"      â€¢ ğŸš€ Speedup: ~{(time_1 * 3) / max(time_1 + time_2 + time_3, 0.001):.1f}Ã— faster!")
    
    print("\nğŸ¯ When to Cache:")
    print("   âœ… DO cache when:")
    print("      â€¢ DataFrame will be reused multiple times")
    print("      â€¢ After expensive transformations (joins, aggregations)")
    print("      â€¢ Iterative algorithms (ML training)")
    print("\n   âŒ DON'T cache when:")
    print("      â€¢ DataFrame used only once")
    print("      â€¢ Data is too large (exceeds memory)")
    print("      â€¢ Source read is fast (e.g., already in Parquet)")
    
    print("\nğŸ§¹ Cleanup: Unpersist when done")
    df_expensive.unpersist()
    print("   âœ… Cache released (frees memory for other operations)")
    
    return df_expensive


def best_practice_6_use_spark_sql_not_udfs(spark: SparkSession, df: "DataFrame"):
    """
    âœ… BEST PRACTICE #6: Use Spark SQL Functions, Not UDFs
    
    WHEN: During transformation logic (any data manipulation)
    WHY: Native Spark functions are 10-100Ã— faster than UDFs
    HOW: Use functions from pyspark.sql.functions instead of UDFs
    
    âŒ Anti-Pattern:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def calculate_discount_udf(amount):
        if amount > 1000:
            return amount * 0.9
        elif amount > 500:
            return amount * 0.95
        else:
            return amount
    
    df.withColumn("discounted", udf_function(col("amount")))
    
    Problems:
    â€¢ Python UDF = serialize data â†’ Python process â†’ deserialize result
    â€¢ Row-by-row processing (can't vectorize)
    â€¢ No Catalyst optimization
    â€¢ GIL (Global Interpreter Lock) bottleneck
    â€¢ 10-100Ã— slower than native Spark
    
    âœ… Best Practice:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df.withColumn(
        "discounted",
        when(col("amount") > 1000, col("amount") * 0.9)
        .when(col("amount") > 500, col("amount") * 0.95)
        .otherwise(col("amount"))
    )
    
    Benefits:
    â€¢ Native Spark execution (Tungsten engine)
    â€¢ Vectorized operations (SIMD)
    â€¢ Catalyst query optimization
    â€¢ No serialization overhead
    â€¢ 10-100Ã— faster!
    
    TIMING: Use Spark SQL functions FIRST, UDFs only when absolutely necessary
    """
    print("\n" + "=" * 80)
    print("BEST PRACTICE #6: USE SPARK SQL FUNCTIONS, NOT UDFs")
    print("=" * 80)
    
    # âŒ WRONG: Use UDF for simple logic
    print("\nâŒ Anti-Pattern: UDF for Simple Logic")
    print("   def calculate_discount_udf(amount):")
    print("       if amount > 1000: return amount * 0.9")
    print("       elif amount > 500: return amount * 0.95")
    print("       else: return amount")
    print("\n   df.withColumn('discounted', udf_function(col('amount')))")
    print("\n   Impact:")
    print("   â€¢ Serialization: Spark â†’ Python â†’ Spark")
    print("   â€¢ Row-by-row processing (no vectorization)")
    print("   â€¢ No Catalyst optimization")
    print("   â€¢ 10-100Ã— slower than native Spark\n")
    
    # âœ… CORRECT: Use Spark SQL functions
    print("âœ… Best Practice: Use Native Spark SQL Functions")
    print("   WHEN: For any data transformation logic")
    print("   WHY: Native execution is 10-100Ã— faster\n")
    
    # Example 1: Conditional logic (when/otherwise)
    print("EXAMPLE 1: Conditional Logic")
    print("   Goal: Apply tiered discounts based on amount\n")
    
    df_with_discount = df.withColumn(
        "discount_rate",
        when(col("amount") > 1000, lit(0.10))
        .when(col("amount") > 500, lit(0.05))
        .otherwise(lit(0.0))
    ).withColumn(
        "discounted_amount",
        col("amount") * (1 - col("discount_rate"))
    )
    
    print("   âœ… Using when/otherwise (native Spark):")
    df_with_discount.select(
        "transaction_id", "amount", "discount_rate", "discounted_amount"
    ).show(5, truncate=False)
    
    # Example 2: String operations
    print("\nEXAMPLE 2: String Operations")
    print("   Goal: Create full product code from parts\n")
    
    from pyspark.sql.functions import concat_ws, upper, substring
    
    df_with_codes = df.withColumn(
        "product_code",
        concat_ws("-", 
                  upper(col("category")),
                  col("product_id"),
                  substring(col("region"), 1, 2))
    )
    
    print("   âœ… Using concat_ws/upper/substring (native Spark):")
    df_with_codes.select(
        "product_id", "category", "region", "product_code"
    ).show(5, truncate=False)
    
    # Example 3: Numerical operations
    print("\nEXAMPLE 3: Numerical Operations")
    print("   Goal: Calculate derived metrics\n")
    
    from pyspark.sql.functions import round as spark_round, sqrt, pow
    
    df_with_metrics = df.withColumn(
        "amount_squared", pow(col("amount"), 2)
    ).withColumn(
        "amount_sqrt", sqrt(col("amount"))
    ).withColumn(
        "amount_rounded", spark_round(col("amount"), 0)
    )
    
    print("   âœ… Using pow/sqrt/round (native Spark):")
    df_with_metrics.select(
        "amount", "amount_squared", "amount_sqrt", "amount_rounded"
    ).show(5, truncate=False)
    
    # Example 4: Date/time operations
    print("\nEXAMPLE 4: Date/Time Operations")
    print("   Goal: Extract date parts and calculate age\n")
    
    from pyspark.sql.functions import year, month, dayofweek, date_format, current_date, datediff
    
    df_with_dates = df.withColumn(
        "year", year(col("timestamp"))
    ).withColumn(
        "month", month(col("timestamp"))
    ).withColumn(
        "day_of_week", dayofweek(col("timestamp"))
    ).withColumn(
        "formatted_date", date_format(col("timestamp"), "yyyy-MM-dd")
    ).withColumn(
        "days_ago", datediff(current_date(), col("timestamp"))
    )
    
    print("   âœ… Using year/month/dayofweek/datediff (native Spark):")
    df_with_dates.select(
        "timestamp", "year", "month", "day_of_week", "days_ago"
    ).show(5, truncate=False)
    
    print("\nğŸ’¡ Performance Comparison:")
    print("   âŒ UDF approach:")
    print("      â€¢ Serialization: Java â†’ Python â†’ Java")
    print("      â€¢ Row-by-row processing")
    print("      â€¢ Python interpreter overhead")
    print("      â€¢ No Catalyst optimization")
    print("      â€¢ Time: 10-100 seconds")
    print("\n   âœ… Native Spark SQL:")
    print("      â€¢ No serialization (stays in JVM)")
    print("      â€¢ Vectorized operations (SIMD)")
    print("      â€¢ Catalyst query optimization")
    print("      â€¢ Tungsten execution engine")
    print("      â€¢ Time: 1 second")
    print("      â€¢ ğŸš€ Speedup: 10-100Ã— faster!")
    
    print("\nğŸ¯ When to Use UDFs:")
    print("   âœ… ONLY when:")
    print("      â€¢ Complex Python libraries needed (scikit-learn, nltk)")
    print("      â€¢ Business logic too complex for SQL")
    print("      â€¢ No equivalent Spark SQL function exists")
    print("\n   âŒ NEVER when:")
    print("      â€¢ Simple conditionals (use when/otherwise)")
    print("      â€¢ String operations (use string functions)")
    print("      â€¢ Math operations (use math functions)")
    print("      â€¢ Date operations (use date functions)")
    
    print("\nğŸ“š Common Spark SQL Functions:")
    print("   â€¢ Conditional: when, otherwise, coalesce, nvl")
    print("   â€¢ String: concat, substring, upper, lower, trim, regexp_replace")
    print("   â€¢ Math: round, sqrt, pow, abs, ceil, floor")
    print("   â€¢ Date: year, month, day, date_add, datediff, to_date")
    print("   â€¢ Aggregate: sum, avg, count, min, max, stddev")
    print("   â€¢ Window: row_number, rank, lag, lead, first, last")
    print("   â€¢ Array: explode, array_contains, size, sort_array")
    print("   â€¢ JSON: from_json, to_json, get_json_object")
    
    return df_with_metrics


def demonstrate_all_best_practices_together():
    """
    Demonstrate all 6 best practices in a realistic workflow.
    
    This shows the TIMING and ORDER of applying optimizations:
    1. Define schema (at load)
    2. Load data with schema
    3. Broadcast models (before operations)
    4. Use Spark SQL functions (during transformations)
    5. Handle skew (when detected)
    6. Cache strategically (before reuse)
    7. Avoid collect() (at output)
    """
    print("\n" + "ğŸ”· " * 40)
    print("COMPLETE WORKFLOW: ALL BEST PRACTICES TOGETHER")
    print("ğŸ”· " * 40)
    
    print("""
This section demonstrates THE CORRECT ORDER and TIMING:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTIMIZATION WORKFLOW (Proper Order)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. âœ… Define Schema          â†’ At data load               â”‚
â”‚ 2. âœ… Load with Schema        â†’ Read data once            â”‚
â”‚ 3. âœ… Broadcast Models        â†’ Before map/filter ops     â”‚
â”‚ 4. âœ… Use Spark SQL           â†’ During transformations    â”‚
â”‚ 5. âœ… Handle Skew             â†’ When joins/aggs are slow  â”‚
â”‚ 6. âœ… Cache Strategically     â†’ Before reuse              â”‚
â”‚ 7. âœ… Avoid collect()         â†’ At output stage           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Let's see this in action!
    """)
    
    spark = create_spark_session()
    
    print("\n" + "=" * 80)
    print("STEP-BY-STEP OPTIMIZED WORKFLOW")
    print("=" * 80)
    
    # Step 1: Define schema (TIMING: Before load)
    print("\nğŸ“‹ STEP 1: Define Schema (BEFORE loading)")
    df_sales = best_practice_1_explicit_schema(spark)
    
    # Step 2: Broadcast model (TIMING: Before operations that use it)
    print("\nğŸ”Š STEP 2: Broadcast Model (BEFORE map/filter operations)")
    df_with_prices, broadcast_model = best_practice_2_broadcast_models(spark, df_sales)
    
    # Step 3: Use Spark SQL (TIMING: During transformations)
    print("\nâš¡ STEP 3: Use Spark SQL Functions (DURING transformations)")
    df_enriched = best_practice_6_use_spark_sql_not_udfs(spark, df_with_prices)
    
    # Step 4: Handle skew (TIMING: When detected in slow operations)
    print("\nâš–ï¸  STEP 4: Handle Data Skew (WHEN joins/aggs are slow)")
    df_aggregated = best_practice_4_handle_data_skew(spark, df_sales)
    
    # Step 5: Cache (TIMING: Before reuse)
    print("\nğŸ’¾ STEP 5: Cache Strategically (BEFORE reuse)")
    df_cached = best_practice_5_cache_strategically(spark, df_enriched)
    
    # Step 6: Avoid collect() (TIMING: At output)
    print("\nğŸ“¤ STEP 6: Output Without collect() (AT output stage)")
    best_practice_3_avoid_collect(spark, df_cached)
    
    print("\n" + "=" * 80)
    print("âœ… COMPLETE WORKFLOW FINISHED")
    print("=" * 80)
    
    print("""
ğŸ“ KEY LESSONS - TIMING MATTERS!

1. Schema Definition: FIRST (at load, not later)
   â° When: Before reading data
   â“ Why: Read once, not twice
   
2. Broadcasting: BEFORE operations that need it
   â° When: After model creation, before map/filter
   â“ Why: Load once per executor, not per row
   
3. Spark SQL: DURING all transformations
   â° When: Any data manipulation
   â“ Why: 10-100Ã— faster than UDFs
   
4. Skew Handling: WHEN detected (slow stages)
   â° When: After seeing uneven partitions
   â“ Why: Balance work across cluster
   
5. Caching: AFTER expensive ops, BEFORE reuse
   â° When: Right before multiple uses
   â“ Why: Compute once, reuse many times
   
6. Avoid collect(): AT output stage
   â° When: Final results
   â“ Why: Driver has limited memory

ğŸš€ PERFORMANCE SUMMARY (Typical Improvements):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Explicit schema: 2-5Ã— faster load
â€¢ Broadcast: 10-100Ã— fewer model loads
â€¢ Spark SQL: 10-100Ã— faster than UDFs
â€¢ Skew handling: 5-50Ã— speedup on slow stages
â€¢ Caching: 3-10Ã— faster for reused DFs
â€¢ No collect(): Prevents OOM crashes

Combined: 100-1000Ã— overall speedup! ğŸ‰
    """)
    
    spark.stop()


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ”· " * 40)
    print("PYSPARK OPTIMIZATION BEST PRACTICES - COMPREHENSIVE GUIDE")
    print("ğŸ”· " * 40)
    
    # Run complete demonstration
    demonstrate_all_best_practices_together()
    
    print("\n" + "=" * 80)
    print("âœ… OPTIMIZATION GUIDE COMPLETE")
    print("=" * 80)
    
    print("""
ğŸ“š Summary of All 6 Best Practices:

1. âœ… Explicit Schema
   â€¢ WHEN: At data load
   â€¢ WHY: Fast, type-safe, memory-efficient
   â€¢ HOW: Define StructType before reading

2. âœ… Broadcast Models
   â€¢ WHEN: Before map/filter operations
   â€¢ WHY: Load once per executor, not per row
   â€¢ HOW: spark.sparkContext.broadcast()

3. âœ… Avoid collect()
   â€¢ WHEN: At output stage
   â€¢ WHY: Driver has limited memory
   â€¢ HOW: Use show(), write(), or aggregate first

4. âœ… Handle Skew
   â€¢ WHEN: When joins/aggs are slow
   â€¢ WHY: Balance work across cluster
   â€¢ HOW: Salt keys, repartition, increase parallelism

5. âœ… Cache Strategically
   â€¢ WHEN: After expensive ops, before reuse
   â€¢ WHY: Compute once, reuse many times
   â€¢ HOW: .cache() or .persist()

6. âœ… Use Spark SQL
   â€¢ WHEN: For all transformations
   â€¢ WHY: 10-100Ã— faster than UDFs
   â€¢ HOW: Use pyspark.sql.functions

ğŸ¯ Remember: TIMING is everything!
   Apply optimizations at the RIGHT stage in your workflow.

ğŸ”— Related Files:
   â€¢ src/optimization/01_join_strategies.py
   â€¢ src/cluster_computing/02_data_partitioning.py
   â€¢ src/cluster_computing/04_aggregations_at_scale.py
    """)


if __name__ == "__main__":
    main()
