"""
================================================================================
Spark Streaming - Read JSON Files From Directory
================================================================================

PURPOSE:
--------
Demonstrates file-based streaming in Spark Structured Streaming, where new
JSON files added to a directory are automatically detected and processed.

WHAT THIS DOES:
---------------
- Monitors directory for new JSON files
- Automatically processes files as they arrive
- Maintains checkpoint to track processed files
- Supports schema evolution and validation
- Enables exactly-once processing semantics

WHY FILE-BASED STREAMING:
-------------------------
- Simplest streaming source (no broker needed)
- Reliable (files are durable, replayable)
- Common in enterprise ETL (file drops from external systems)
- Easy to test and debug locally
- Natural fit for batch-to-streaming migration

HOW IT WORKS:
-------------
1. Spark monitors directory for new files (poll interval: 1 second)
2. New files detected based on modification time
3. Files read and parsed as JSON (schema applied)
4. Data flows through transformation pipeline
5. Checkpoint tracks processed files (prevents reprocessing)
6. Results written to sink (console, Parquet, Kafka, etc.)

REAL-WORLD USE CASES:
---------------------
- LOG FILE PROCESSING: Application logs dropped every minute
  Example: Web server logs ‚Üí Parse ‚Üí Extract metrics ‚Üí Alert on errors

- DATA INGESTION: External systems export files to watched directory
  Example: CRM exports ‚Üí Transform ‚Üí Load to data warehouse

- ETL PIPELINES: Replace batch jobs with streaming
  Example: Daily CSV files ‚Üí Continuous processing ‚Üí Real-time analytics

- FBI CJIS: Case files arrive from field offices
  Example: Incident reports ‚Üí Parse ‚Üí Enrich ‚Üí Load to central database

KEY CONCEPTS:
-------------

1. SCHEMA INFERENCE vs EXPLICIT SCHEMA:
   - Inference: Convenient but slow (reads sample files)
   - Explicit: 10-100x faster, type-safe, required for production

2. maxFilesPerTrigger (Rate Limiting):
   - Controls processing rate (e.g., 10 files per trigger)
   - Prevents overwhelming downstream systems
   - Enables graceful degradation under load

3. FILE SOURCE OPTIONS:
   - cleanSource: Archive or delete processed files
   - latestFirst: Process newest files first
   - maxFileAge: Ignore files older than threshold

4. CHECKPOINT MANAGEMENT:
   - Tracks processed files and offsets
   - Enables exactly-once processing
   - Required for fault tolerance
   - Stored in HDFS/S3/Local filesystem

PERFORMANCE CONSIDERATIONS:
---------------------------
- SCHEMA: Always use explicit schema in production (100x faster)
- RATE: Set maxFilesPerTrigger to match processing capacity
- PARTITIONING: Output should be partitioned for efficient querying
- COMPACTION: Small files hurt performance (use coalesce/repartition)

FAULT TOLERANCE:
----------------
- Checkpointing enables automatic recovery from failures
- Files tracked by path + modification time
- Failed batches automatically retried
- Exactly-once semantics guaranteed with proper sinks

WHEN TO USE:
------------
‚úì Files arrive periodically (seconds to hours)
‚úì Simple deployment (no broker infrastructure)
‚úì Need exactly-once processing
‚úì Files are immutable (no updates)
‚úó Sub-second latency required (use Kafka instead)
‚úó Need to replay arbitrary time ranges (use Kafka retention)

================================================================================
"""

import json
import os
import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, from_json, struct, to_json
from pyspark.sql.types import (
    DoubleType,
    IntegerType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)


def create_sample_json_files(directory):
    """
    Create sample JSON files to simulate streaming data.

    In production, these would be:
    - Application log files
    - Export files from other systems
    - Data dumps from external sources
    """
    print(f"\nüìÅ Creating sample JSON files in: {directory}")

    # Create directory if it doesn't exist
    os.makedirs(directory, exist_ok=True)

    # Sample data - simulating case files for law enforcement
    sample_data = [
        {
            "case_id": "2024-001",
            "timestamp": "2024-12-13T10:00:00",
            "case_type": "Fingerprint Match",
            "priority": "HIGH",
            "match_score": 0.95,
            "investigator_id": "INV-123",
        },
        {
            "case_id": "2024-002",
            "timestamp": "2024-12-13T10:05:00",
            "case_type": "Background Check",
            "priority": "MEDIUM",
            "match_score": 0.87,
            "investigator_id": "INV-456",
        },
        {
            "case_id": "2024-003",
            "timestamp": "2024-12-13T10:10:00",
            "case_type": "Database Query",
            "priority": "LOW",
            "match_score": 0.72,
            "investigator_id": "INV-789",
        },
    ]

    # Write each record to a separate file (simulating file drops)
    for i, record in enumerate(sample_data):
        filename = os.path.join(directory, f"case_{i+1}.json")
        with open(filename, "w") as f:
            json.dump(record, f, indent=2)
        print(f"   ‚úì Created: {filename}")

    return len(sample_data)


def demo_basic_json_streaming(spark, input_path):
    """
    Basic example: Read JSON files from directory with automatic schema inference.

    Spark will:
    1. Monitor the directory for new files
    2. Process new files as they arrive
    3. Maintain checkpoint to track processed files
    """
    print("\n" + "=" * 70)
    print("1. BASIC JSON FILE STREAMING")
    print("=" * 70)

    print("\nüìñ Reading JSON files from directory...")
    print(f"   Input path: {input_path}")
    print("   Mode: Streaming (auto-detect new files)")

    # Read streaming data from JSON files
    # Note: In production, always use explicit schema for performance
    streaming_df = (
        spark.readStream.format("json").option("maxFilesPerTrigger", 1).load(input_path)
    )

    print("\nüìä Schema detected:")
    streaming_df.printSchema()

    print("\nüí° Key Options:")
    print("   - maxFilesPerTrigger: Limit files per batch (rate limiting)")
    print("   - maxFileAge: Ignore files older than threshold")
    print("   - latestFirst: Process newest files first")
    print("   - cleanSource: Archive/delete processed files")

    return streaming_df


def demo_explicit_schema(spark, input_path):
    """
    Production example: Use explicit schema for better performance.

    Benefits of explicit schema:
    - Faster processing (no schema inference)
    - Type safety and validation
    - Better error handling
    - Required for some sources
    """
    print("\n" + "=" * 70)
    print("2. EXPLICIT SCHEMA (PRODUCTION PATTERN)")
    print("=" * 70)

    # Define explicit schema
    # This is CRITICAL for production - never rely on schema inference
    schema = StructType(
        [
            StructField("case_id", StringType(), False),  # NOT NULL
            StructField("timestamp", StringType(), True),
            StructField("case_type", StringType(), True),
            StructField("priority", StringType(), True),
            StructField("match_score", DoubleType(), True),
            StructField("investigator_id", StringType(), True),
        ]
    )

    print("\nüìã Defined explicit schema:")
    print("   - case_id: STRING (NOT NULL)")
    print("   - timestamp: STRING")
    print("   - case_type: STRING")
    print("   - priority: STRING")
    print("   - match_score: DOUBLE")
    print("   - investigator_id: STRING")

    # Read with explicit schema
    streaming_df = (
        spark.readStream.format("json")
        .schema(schema)
        .option("maxFilesPerTrigger", 1)
        .option("maxFileAge", "7d")
        .load(input_path)
    )

    print("\n‚úÖ Benefits of explicit schema:")
    print("   ‚úì 10-100x faster than schema inference")
    print("   ‚úì Type validation on read")
    print("   ‚úì Consistent schema across batches")
    print("   ‚úì Required for production systems")

    return streaming_df


def demo_with_transformations(spark, input_path):
    """
    Apply transformations to streaming data.

    Common patterns:
    - Filter high-priority cases
    - Parse timestamps
    - Add processing metadata
    - Enrich with additional data
    """
    print("\n" + "=" * 70)
    print("3. STREAMING WITH TRANSFORMATIONS")
    print("=" * 70)

    # Define schema
    schema = StructType(
        [
            StructField("case_id", StringType(), False),
            StructField("timestamp", StringType(), True),
            StructField("case_type", StringType(), True),
            StructField("priority", StringType(), True),
            StructField("match_score", DoubleType(), True),
            StructField("investigator_id", StringType(), True),
        ]
    )

    # Read and transform
    streaming_df = spark.readStream.format("json").schema(schema).load(input_path)

    print("\n‚öôÔ∏è  Applying transformations:")
    print("   1. Filter: Only HIGH priority cases")
    print("   2. Add: Processing timestamp")
    print("   3. Convert: String timestamp to TimestampType")
    print("   4. Add: Alert flag for high-confidence matches")

    # Transform the stream
    transformed_df = (
        streaming_df.filter(col("priority") == "HIGH")
        .withColumn("processing_time", current_timestamp())
        .withColumn("alert", col("match_score") > 0.9)
    )

    print("\nüìä Transformed schema:")
    transformed_df.printSchema()

    return transformed_df


def demo_write_to_console(streaming_df):
    """
    Write streaming results to console for debugging.

    Console sink is perfect for:
    - Development and testing
    - Debugging transformations
    - Monitoring data flow

    NOT for production (use Delta, Kafka, etc.)
    """
    print("\n" + "=" * 70)
    print("4. WRITE TO CONSOLE (DEBUGGING)")
    print("=" * 70)

    print("\nüì∫ Starting console output stream...")
    print("   Output mode: APPEND")
    print("   Trigger: ProcessingTime (2 seconds)")

    # Write to console
    query = (
        streaming_df.writeStream.format("console")
        .outputMode("append")
        .option("truncate", False)
        .trigger(processingTime="2 seconds")
        .start()
    )

    # Let it run for a bit
    print("\n‚è≥ Processing (will stop after 10 seconds)...")
    time.sleep(10)

    # Stop the query
    query.stop()
    print("‚úÖ Stream stopped")


def demo_write_to_parquet(streaming_df, output_path, checkpoint_path):
    """
    Write streaming results to Parquet files.

    Production pattern for:
    - Data lake storage
    - Long-term retention
    - Efficient querying
    - Audit trails (FBI CJIS requirement)
    """
    print("\n" + "=" * 70)
    print("5. WRITE TO PARQUET (PRODUCTION)")
    print("=" * 70)

    print(f"\nüíæ Writing to Parquet...")
    print(f"   Output: {output_path}")
    print(f"   Checkpoint: {checkpoint_path}")
    print("   Partitioning: By processing date")

    # Write to Parquet with partitioning
    query = (
        streaming_df.writeStream.format("parquet")
        .outputMode("append")
        .option("path", output_path)
        .option("checkpointLocation", checkpoint_path)
        .partitionBy("priority")
        .trigger(processingTime="5 seconds")
        .start()
    )

    print("\n‚è≥ Processing (will stop after 10 seconds)...")
    time.sleep(10)

    query.stop()
    print("‚úÖ Stream stopped")
    print(f"‚úÖ Data written to: {output_path}")


def main():
    """
    Main demo function.
    """
    # Create Spark session
    spark = (
        SparkSession.builder.appName("JSONFileStreamingDemo")
        .config("spark.sql.streaming.schemaInference", "true")
        .getOrCreate()
    )

    # Set log level to reduce noise
    spark.sparkContext.setLogLevel("WARN")

    try:
        print("‚ïî" + "=" * 68 + "‚ïó")
        print("‚ïë" + " " * 15 + "JSON FILE STREAMING DEMO" + " " * 29 + "‚ïë")
        print("‚ïö" + "=" * 68 + "‚ïù")

        # Setup paths
        base_path = "/tmp/streaming_json_demo"
        input_path = f"{base_path}/input"
        output_path = f"{base_path}/output"
        checkpoint_path = f"{base_path}/checkpoint"

        # Create sample files
        create_sample_json_files(input_path)

        # Demo 1: Basic streaming
        streaming_df = demo_basic_json_streaming(spark, input_path)
        demo_write_to_console(streaming_df)

        # Demo 2: Explicit schema
        streaming_df = demo_explicit_schema(spark, input_path)

        # Demo 3: With transformations
        transformed_df = demo_with_transformations(spark, input_path)

        # Demo 4: Write to Parquet
        demo_write_to_parquet(transformed_df, output_path, checkpoint_path)

        print("\n" + "=" * 70)
        print("PRODUCTION BEST PRACTICES")
        print("=" * 70)

        print("\n‚úÖ DO:")
        print("   ‚úì Always use explicit schema")
        print("   ‚úì Set maxFilesPerTrigger to control rate")
        print("   ‚úì Use checkpointing for fault tolerance")
        print("   ‚úì Partition output for efficient querying")
        print("   ‚úì Monitor file ages (maxFileAge option)")
        print("   ‚úì Clean up processed files (cleanSource option)")

        print("\n‚ùå DON'T:")
        print("   ‚úó Rely on schema inference in production")
        print("   ‚úó Skip checkpointing")
        print("   ‚úó Write to console sink in production")
        print("   ‚úó Process all files at once (no rate limit)")
        print("   ‚úó Forget to handle late data")

        print("\n" + "=" * 70)
        print("‚úÖ JSON FILE STREAMING DEMO COMPLETE")
        print("=" * 70)

    finally:
        spark.stop()


if __name__ == "__main__":
    main()
