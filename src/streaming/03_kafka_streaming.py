#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
KAFKA STREAMING - Real-Time Data Processing with Apache Kafka
================================================================================

MODULE OVERVIEW:
----------------
Apache Kafka is the industry-standard distributed streaming platform for
building real-time data pipelines and streaming applications. This module
demonstrates PySpark Structured Streaming integration with Kafka for:

â€¢ Real-time event processing
â€¢ Exactly-once semantics
â€¢ Stateful stream processing
â€¢ Window operations on streams
â€¢ Join streaming data with batch data
â€¢ Stream-to-stream joins
â€¢ Checkpointing and fault tolerance

WHY KAFKA + PYSPARK:
--------------------
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA + PYSPARK ARCHITECTURE                 â”‚
â”‚                                                                 â”‚
â”‚  Producers          Kafka Cluster         PySpark Consumers    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚IoT     â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Topic 1   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Streaming  â”‚       â”‚
â”‚  â”‚Sensors â”‚        â”‚  (events)  â”‚        â”‚ Queries    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚               â”‚
â”‚  â”‚Web Appsâ”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Topic 2   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶     â”‚               â”‚
â”‚  â”‚Logs    â”‚        â”‚  (clicks)  â”‚        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚Aggregations â”‚      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚Joins        â”‚      â”‚
â”‚  â”‚APIs    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Topic 3   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Windowing    â”‚      â”‚
â”‚  â”‚Events  â”‚        â”‚  (payments)â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚               â”‚
â”‚                                           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                           â”‚  Outputs   â”‚       â”‚
â”‚                                           â”‚ (Console,  â”‚       â”‚
â”‚                                           â”‚  Kafka,    â”‚       â”‚
â”‚                                           â”‚  Parquet)  â”‚       â”‚
â”‚                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KAFKA FUNDAMENTALS:
-------------------

Topics & Partitions:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Topic: "user-events" (3 partitions)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Partition 0   â”‚  Partition 1   â”‚  Partition 2   â”‚        â”‚
â”‚  â”‚ [msg0, msg3,   â”‚ [msg1, msg4,   â”‚ [msg2, msg5,   â”‚        â”‚
â”‚  â”‚  msg6, ...]    â”‚  msg7, ...]    â”‚  msg8, ...]    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                               â”‚
â”‚  â€¢ Messages distributed by key hash                          â”‚
â”‚  â€¢ Same key â†’ Same partition (ordering guaranteed)           â”‚
â”‚  â€¢ Multiple partitions = Parallelism                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kafka Guarantees:
â€¢ At-least-once delivery (default)
â€¢ Exactly-once semantics (with idempotent producer)
â€¢ Message ordering within partition
â€¢ Distributed, replicated, fault-tolerant

PYSPARK STRUCTURED STREAMING:
------------------------------

Stream Processing Model:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source (Kafka) â†’ Transformations â†’ Sink (Output)            â”‚
â”‚                                                               â”‚
â”‚  1. Read Stream:                                             â”‚
â”‚     spark.readStream.format("kafka")                         â”‚
â”‚                                                               â”‚
â”‚  2. Transform:                                               â”‚
â”‚     â€¢ select, filter, map, flatMap                           â”‚
â”‚     â€¢ groupBy, agg (with watermarks)                         â”‚
â”‚     â€¢ join (stream-stream, stream-batch)                     â”‚
â”‚     â€¢ window operations                                      â”‚
â”‚                                                               â”‚
â”‚  3. Write Stream:                                            â”‚
â”‚     .writeStream.format("kafka")                             â”‚
â”‚     .outputMode("append|complete|update")                    â”‚
â”‚     .option("checkpointLocation", "/path")                   â”‚
â”‚     .start()                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STREAMING CONCEPTS:
-------------------

1. Triggers:
   â€¢ ProcessingTime: Micro-batches at fixed intervals
   â€¢ Continuous: Low-latency continuous processing
   â€¢ AvailableNow: Process available data and stop

2. Output Modes:
   â€¢ Append: Only new rows (immutable)
   â€¢ Complete: Entire result table (aggregations)
   â€¢ Update: Only updated rows (stateful operations)

3. Watermarks:
   â€¢ Handle late data
   â€¢ Define "how late is too late"
   â€¢ Enable windowed aggregations with time bounds

4. Checkpointing:
   â€¢ Stores stream metadata & offsets
   â€¢ Enables fault tolerance
   â€¢ Required for production
   â€¢ Resume from failure point

REAL-WORLD USE CASES:
---------------------

1. IoT Sensor Monitoring:
   â€¢ Millions of sensors â†’ Kafka topics
   â€¢ Real-time anomaly detection
   â€¢ Aggregated metrics dashboards

2. Click Stream Analysis:
   â€¢ Web events â†’ Kafka
   â€¢ Session analytics
   â€¢ Real-time recommendations

3. Financial Transactions:
   â€¢ Payment events â†’ Kafka
   â€¢ Fraud detection
   â€¢ Real-time risk scoring

4. Log Aggregation:
   â€¢ Application logs â†’ Kafka
   â€¢ Real-time error alerting
   â€¢ Security monitoring

5. Supply Chain Tracking:
   â€¢ GPS/RFID data â†’ Kafka
   â€¢ Real-time inventory
   â€¢ Delivery predictions

PERFORMANCE CONSIDERATIONS:
---------------------------

Kafka:
â€¢ Partition count = Parallelism
â€¢ Consumer group for load balancing
â€¢ Replication factor for fault tolerance
â€¢ Compression (gzip, snappy, lz4)

PySpark:
â€¢ Trigger interval vs throughput
â€¢ Checkpoint location (HDFS recommended)
â€¢ State store size management
â€¢ Shuffle partitions tuning

DEPENDENCIES:
-------------
pip install kafka-python pyspark

Spark packages:
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 1.0.0 - Kafka Streaming Guide
UPDATED: 2024
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, to_json, struct, window, count, sum as _sum, avg,
    current_timestamp, expr, lit, concat, explode, split, regexp_extract
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
)
import time


def create_kafka_spark_session():
    """
    Create SparkSession with Kafka support.
    
    NOTE: Requires spark-sql-kafka package:
    spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
    """
    print("=" * 80)
    print("CREATING KAFKA-ENABLED SPARK SESSION")
    print("=" * 80)
    
    spark = SparkSession.builder \
        .appName("KafkaStreaming") \
        .master("local[*]") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
        .config("spark.sql.shuffle.partitions", "4") \
        .config("spark.streaming.kafka.maxRatePerPartition", "1000") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Spark {spark.version} with Kafka support")
    print(f"âœ… Streaming checkpoint: /tmp/checkpoint")
    print(f"âœ… Ready to process Kafka streams")
    
    return spark


def example_1_basic_kafka_read(spark):
    """
    Example 1: Basic Kafka stream reading.
    
    Reads from Kafka topic and displays to console.
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 1: BASIC KAFKA READ")
    print("=" * 80)
    
    print("""
ğŸ“– Reading from Kafka topic: "user-events"

Configuration:
â€¢ kafka.bootstrap.servers: localhost:9092
â€¢ subscribe: user-events
â€¢ startingOffsets: earliest (read from beginning)

Message Format:
â€¢ key: user_id (binary)
â€¢ value: JSON event data (binary)
â€¢ timestamp: Event timestamp
â€¢ partition: Kafka partition
â€¢ offset: Message offset in partition
    """)
    
    # Read from Kafka
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "user-events") \
        .option("startingOffsets", "earliest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    print("\nğŸ“Š Kafka DataFrame Schema:")
    kafka_df.printSchema()
    
    print("""
Kafka Record Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column     â”‚ Description                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ key        â”‚ Message key (binary)                        â”‚
â”‚ value      â”‚ Message payload (binary)                    â”‚
â”‚ topic      â”‚ Topic name                                  â”‚
â”‚ partition  â”‚ Partition number (0 to N-1)                 â”‚
â”‚ offset     â”‚ Message offset within partition             â”‚
â”‚ timestamp  â”‚ Message timestamp                           â”‚
â”‚ timestamp  â”‚ Type of timestamp (CreateTime/LogAppendTime)â”‚
â”‚ Type       â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ Important: key and value are BINARY!
   Must cast to string and parse JSON
    """)
    
    # Convert binary to string
    events_df = kafka_df.select(
        col("key").cast("string").alias("user_id"),
        col("value").cast("string").alias("event_json"),
        col("timestamp"),
        col("partition"),
        col("offset")
    )
    
    print("\nâ–¶ï¸  Starting stream query (press Ctrl+C to stop)...")
    print("   (This is a simulated example - requires running Kafka cluster)")
    
    # This would start the actual stream
    # query = events_df.writeStream \
    #     .format("console") \
    #     .option("truncate", "false") \
    #     .outputMode("append") \
    #     .trigger(processingTime="5 seconds") \
    #     .start()
    # 
    # query.awaitTermination(30)  # Run for 30 seconds
    
    print("âœ… Basic Kafka read example complete")


def example_2_json_parsing(spark):
    """
    Example 2: Parse JSON from Kafka messages.
    
    Demonstrates:
    â€¢ JSON schema definition
    â€¢ from_json() function
    â€¢ Extracting nested fields
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 2: JSON PARSING FROM KAFKA")
    print("=" * 80)
    
    print("""
ğŸ“– Parsing JSON events from Kafka

Sample Kafka Message:
{
  "user_id": "user_12345",
  "event_type": "page_view",
  "page": "/products/laptop",
  "timestamp": "2024-12-13T10:30:00Z",
  "session_id": "sess_xyz",
  "device": "mobile",
  "country": "US"
}
    """)
    
    # Define JSON schema
    event_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("event_type", StringType(), True),
        StructField("page", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        StructField("session_id", StringType(), True),
        StructField("device", StringType(), True),
        StructField("country", StringType(), True)
    ])
    
    print("\nğŸ“‹ Defined JSON Schema:")
    print(event_schema)
    
    # Simulated Kafka stream
    kafka_df = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", "10") \
        .load()
    
    # Simulate Kafka structure
    kafka_simulated = kafka_df.select(
        col("value").cast("string").alias("user_id"),
        to_json(struct(
            lit("user_12345").alias("user_id"),
            lit("page_view").alias("event_type"),
            lit("/products/laptop").alias("page"),
            current_timestamp().alias("timestamp"),
            lit("sess_xyz").alias("session_id"),
            lit("mobile").alias("device"),
            lit("US").alias("country")
        )).alias("value")
    )
    
    # Parse JSON
    parsed_events = kafka_simulated.select(
        col("user_id"),
        from_json(col("value"), event_schema).alias("event")
    ).select("user_id", "event.*")
    
    print("\nğŸ“Š Parsed Events Schema:")
    parsed_events.printSchema()
    
    print("""
âœ… JSON Parsing Benefits:
â€¢ Type safety (schema validation)
â€¢ Nested field access
â€¢ Null handling
â€¢ Performance optimization (predicate pushdown)

ğŸ’¡ Best Practices:
â€¢ Define schema explicitly (faster than inferSchema)
â€¢ Use SELECT to extract only needed fields
â€¢ Handle malformed JSON with try-catch or options
    """)
    
    print("\nâ–¶ï¸  Sample output (simulated):")
    # query = parsed_events.writeStream \
    #     .format("console") \
    #     .outputMode("append") \
    #     .trigger(processingTime="5 seconds") \
    #     .start()
    
    print("âœ… JSON parsing example complete")


def example_3_windowed_aggregations(spark):
    """
    Example 3: Windowed aggregations on streaming data.
    
    Demonstrates:
    â€¢ Tumbling windows
    â€¢ Sliding windows
    â€¢ Watermarks for late data
    â€¢ Stateful aggregations
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 3: WINDOWED AGGREGATIONS")
    print("=" * 80)
    
    print("""
ğŸ“– Aggregating events over time windows

Window Types:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TUMBLING WINDOW (10 minutes, non-overlapping)              â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  10:00    10:10    10:20    10:30    10:40    10:50         â”‚
â”‚                                                               â”‚
â”‚  SLIDING WINDOW (10 min window, 5 min slide, overlapping)   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                                 â”‚
â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                             â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                         â”‚
â”‚  10:00  10:05  10:10  10:15  10:20                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Watermark:
â€¢ Threshold for "how late is too late"
â€¢ Example: "10 minutes" â†’ drop data >10 min late
â€¢ Enables state cleanup (prevents unbounded state)
    """)
    
    # Simulated event stream with timestamps
    event_stream = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", "100") \
        .load() \
        .selectExpr(
            "value as event_id",
            "timestamp",
            "CAST((value % 5) AS STRING) as event_type",
            "CAST((value % 10) AS STRING) as country"
        )
    
    # Tumbling window aggregation (10 seconds)
    print("\nğŸ• Tumbling Window Aggregation (10 seconds):")
    tumbling_agg = event_stream \
        .withWatermark("timestamp", "30 seconds") \
        .groupBy(
            window(col("timestamp"), "10 seconds"),
            col("event_type")
        ) \
        .agg(
            count("*").alias("event_count"),
            expr("min(timestamp) as window_start"),
            expr("max(timestamp) as window_end")
        )
    
    print("""
   â€¢ Each 10-second bucket counted separately
   â€¢ Watermark: 30 seconds (handle late data up to 30s)
   â€¢ Output: event counts per 10-second window
    """)
    
    # Sliding window aggregation (10s window, 5s slide)
    print("\nï¿½ï¿½ Sliding Window Aggregation (10s window, 5s slide):")
    sliding_agg = event_stream \
        .withWatermark("timestamp", "30 seconds") \
        .groupBy(
            window(col("timestamp"), "10 seconds", "5 seconds"),
            col("country")
        ) \
        .agg(
            count("*").alias("event_count"),
            avg("event_id").alias("avg_event_id")
        )
    
    print("""
   â€¢ Windows overlap (more granular view)
   â€¢ 10s window slides every 5s
   â€¢ Output: event counts for overlapping windows
    """)
    
    print("""
âœ… Windowed Aggregation Benefits:
â€¢ Real-time metrics (events/minute, transactions/hour)
â€¢ Trend analysis
â€¢ Anomaly detection
â€¢ Session analytics

ğŸ’¡ Watermark Tuning:
â€¢ Too short: Drop valid late data
â€¢ Too long: Excessive state size
â€¢ Balance: Business latency requirements vs. resource usage

Example Metrics:
â€¢ Page views per minute
â€¢ Transactions per hour by region
â€¢ Error rate in 5-minute windows
â€¢ Active sessions in rolling 30-minute window
    """)
    
    print("âœ… Windowed aggregation example complete")


def example_4_stream_to_stream_join(spark):
    """
    Example 4: Join two Kafka streams.
    
    Demonstrates:
    â€¢ Stream-to-stream joins
    â€¢ Join with watermarks
    â€¢ Time-bounded joins
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 4: STREAM-TO-STREAM JOIN")
    print("=" * 80)
    
    print("""
ğŸ“– Joining two Kafka streams: Clicks + Purchases

Use Case:
â€¢ Stream 1: User clicks (browsing behavior)
â€¢ Stream 2: User purchases (transactions)
â€¢ Goal: Correlate clicks to purchases

Time-Bounded Join:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Clicks Stream:     [C1] [C2] [C3] [C4] [C5] [C6]            â”‚
â”‚  10:00             10:05 10:10 10:15 10:20 10:25             â”‚
â”‚                                                               â”‚
â”‚  Purchases Stream:      [P1] [P2]     [P3]                   â”‚
â”‚  10:00                 10:08 10:12    10:22                  â”‚
â”‚                                                               â”‚
â”‚  Join Window: 15 minutes                                     â”‚
â”‚  â€¢ C2 joins with P1 (3 min apart)                            â”‚
â”‚  â€¢ C3 joins with P2 (2 min apart)                            â”‚
â”‚  â€¢ C5 joins with P3 (2 min apart)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why Time-Bounded?
â€¢ Prevents unbounded state growth
â€¢ Defines "relevance window"
â€¢ Watermarks enable state cleanup
    """)
    
    # Simulate clicks stream
    clicks = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", "10") \
        .load() \
        .selectExpr(
            "CAST((value % 100) AS STRING) as user_id",
            "timestamp as click_time",
            "value as click_id"
        )
    
    # Simulate purchases stream
    purchases = spark.readStream \
        .format("rate") \
        .option("rowsPerSecond", "2") \
        .load() \
        .selectExpr(
            "CAST((value % 100) AS STRING) as user_id",
            "timestamp as purchase_time",
            "value as purchase_id",
            "CAST((value * 10) AS DOUBLE) as amount"
        )
    
    print("\nğŸ”— Stream-to-Stream Join Configuration:")
    print("   â€¢ Join key: user_id")
    print("   â€¢ Join type: inner")
    print("   â€¢ Time bound: 15 minutes")
    print("   â€¢ Watermark: 30 seconds")
    
    # Apply watermarks
    clicks_with_watermark = clicks.withWatermark("click_time", "30 seconds")
    purchases_with_watermark = purchases.withWatermark("purchase_time", "30 seconds")
    
    # Time-bounded join
    joined = clicks_with_watermark.join(
        purchases_with_watermark,
        expr("""
            user_id = user_id AND
            purchase_time >= click_time AND
            purchase_time <= click_time + interval 15 minutes
        """)
    )
    
    result = joined.select(
        "user_id",
        "click_time",
        "purchase_time",
        "purchase_id",
        "amount",
        expr("(unix_timestamp(purchase_time) - unix_timestamp(click_time)) / 60 as minutes_to_purchase")
    )
    
    print("""
âœ… Stream-to-Stream Join Benefits:
â€¢ Attribution analysis (clicks â†’ conversions)
â€¢ Funnel analytics
â€¢ Session stitching
â€¢ Real-time recommendations

ğŸ’¡ Performance Tips:
â€¢ Always use watermarks (required for stateful joins)
â€¢ Keep join window as small as reasonable
â€¢ Monitor state size in Spark UI
â€¢ Use checkpoint location on distributed storage (HDFS/S3)

Real-World Examples:
â€¢ Ad clicks + purchases (attribution)
â€¢ Login events + activity logs (session analysis)
â€¢ Sensor readings + alerts (anomaly correlation)
    """)
    
    print("âœ… Stream-to-stream join example complete")


def example_5_exactly_once_semantics(spark):
    """
    Example 5: Exactly-once processing with Kafka.
    
    Demonstrates:
    â€¢ Idempotent writes
    â€¢ Checkpointing
    â€¢ Kafka transactions
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 5: EXACTLY-ONCE SEMANTICS")
    print("=" * 80)
    
    print("""
ğŸ“– Guaranteeing exactly-once processing

Delivery Semantics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. AT-MOST-ONCE:                                             â”‚
â”‚    â€¢ Fastest, but data loss possible                         â”‚
â”‚    â€¢ Use: Non-critical logs, metrics                         â”‚
â”‚                                                               â”‚
â”‚ 2. AT-LEAST-ONCE:                                            â”‚
â”‚    â€¢ No data loss, but duplicates possible                   â”‚
â”‚    â€¢ Use: Most streaming applications (with deduplication)   â”‚
â”‚                                                               â”‚
â”‚ 3. EXACTLY-ONCE:                                             â”‚
â”‚    â€¢ No data loss, no duplicates                             â”‚
â”‚    â€¢ Use: Financial transactions, critical events            â”‚
â”‚    â€¢ Requires: Idempotent sinks, checkpointing, transactions â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Exactly-Once Requirements:
1. Checkpointing (offset tracking)
2. Idempotent sink (same input â†’ same output)
3. Kafka transactions (for Kafka sink)
4. Spark streaming state management
    """)
    
    print("""
ğŸ”§ Checkpoint Configuration:

.writeStream \\
  .format("kafka") \\
  .option("kafka.bootstrap.servers", "localhost:9092") \\
  .option("topic", "output-topic") \\
  .option("checkpointLocation", "/hdfs/checkpoints/app1") \\
  .option("kafka.transactional.id", "app1-txn") \\  # Transactions
  .outputMode("append") \\
  .start()

Checkpoint Directory Structure:
/hdfs/checkpoints/app1/
â”œâ”€â”€ commits/          # Batch commit metadata
â”œâ”€â”€ metadata          # Stream metadata
â”œâ”€â”€ offsets/          # Kafka offsets per batch
â”œâ”€â”€ sources/          # Source information
â””â”€â”€ state/            # Stateful operation state

ğŸ’¡ Checkpoint Best Practices:
â€¢ Use distributed storage (HDFS, S3, not local disk)
â€¢ Never change query logic without new checkpoint location
â€¢ Monitor checkpoint size
â€¢ Clean up old checkpoints periodically
    """)
    
    print("""
âœ… Exactly-Once Use Cases:
â€¢ Payment processing
â€¢ Account balance updates
â€¢ Order fulfillment
â€¢ Inventory management
â€¢ Financial transactions

âš ï¸  Caveats:
â€¢ Requires compatible sinks (not all support transactions)
â€¢ Performance overhead vs. at-least-once
â€¢ Complexity in error handling
â€¢ State management for large windows
    """)
    
    print("âœ… Exactly-once semantics example complete")


def example_6_production_monitoring(spark):
    """
    Example 6: Production monitoring and best practices.
    """
    print("\n" + "=" * 80)
    print("EXAMPLE 6: PRODUCTION MONITORING & BEST PRACTICES")
    print("=" * 80)
    
    print("""
ğŸ“Š Monitoring Kafka Streaming Applications

Key Metrics to Monitor:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                     â”‚ What to Watch                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Processing Rate            â”‚ Events/second throughput        â”‚
â”‚ Batch Duration             â”‚ Time to process each batch      â”‚
â”‚ Input Rate                 â”‚ Incoming events/second          â”‚
â”‚ Scheduling Delay           â”‚ Backlog indicator               â”‚
â”‚ End-to-End Latency         â”‚ Event â†’ Output delay            â”‚
â”‚ State Store Size           â”‚ Stateful operation memory       â”‚
â”‚ Kafka Consumer Lag         â”‚ Offset lag per partition        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Accessing Metrics in Spark UI:
â€¢ Navigate to: http://driver:4040/StreamingQuery/
â€¢ View: Progress reports, batch statistics, watermarks

Programmatic Monitoring:
    """)
    
    # Simulated query for monitoring example
    print("""
query = events_df.writeStream \\
    .format("console") \\
    .start()

# Monitor progress
while query.isActive:
    progress = query.lastProgress
    if progress:
        print(f"Batch: {progress['batchId']}")
        print(f"Input Rows: {progress['numInputRows']}")
        print(f"Processing Time: {progress['durationMs']['triggerExecution']}ms")
        print(f"Input Rate: {progress.get('inputRowsPerSecond', 0)} rows/sec")
        print(f"Process Rate: {progress.get('processedRowsPerSecond', 0)} rows/sec")
    
    time.sleep(10)
    """)
    
    print("""
ğŸš¨ Common Issues & Solutions:

1. Consumer Lag Growing:
   â€¢ Input rate > Processing rate
   â€¢ Solution: Scale up executors, optimize transformations
   
2. Out of Memory (State):
   â€¢ Stateful aggregations with no watermark
   â€¢ Solution: Add watermarks, tune state timeout
   
3. Slow Batches:
   â€¢ Complex transformations, skewed data
   â€¢ Solution: Optimize queries, add salting
   
4. Checkpoint Corruption:
   â€¢ Job restarted with incompatible code
   â€¢ Solution: Use new checkpoint location
   
5. Exactly-Once Failures:
   â€¢ Sink doesn't support transactions
   â€¢ Solution: Use compatible sink or implement idempotency

ğŸ’¡ Best Practices:

Performance:
âœ… Set appropriate trigger interval (balance latency vs. throughput)
âœ… Use Kafka partition count = Spark parallelism
âœ… Enable compression in Kafka
âœ… Tune fetch.min.bytes and fetch.max.wait.ms
âœ… Use Kryo serialization

Reliability:
âœ… Always use checkpointing in production
âœ… Add watermarks for stateful operations
âœ… Monitor consumer lag
âœ… Set up alerts for backlog
âœ… Test failure recovery

Security:
âœ… Enable Kafka SSL/TLS
âœ… Use SASL authentication
âœ… Encrypt checkpoint location
âœ… Rotate credentials regularly

Scalability:
âœ… Partition Kafka topics appropriately (10-100 per node)
âœ… Use dynamic allocation if workload varies
âœ… Archive old checkpoints
âœ… Clean up state periodically
    """)
    
    print("âœ… Production monitoring example complete")


def kafka_setup_guide():
    """
    Print Kafka setup guide.
    """
    print("\n" + "=" * 80)
    print("KAFKA SETUP GUIDE")
    print("=" * 80)
    
    print("""
ğŸ“¦ 1. Install Kafka:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Download Kafka
wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0


ğŸš€ 2. Start Kafka (Local Development):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Terminal 1: Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Terminal 2: Start Kafka Broker
bin/kafka-server-start.sh config/server.properties


ğŸ“ 3. Create Topics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Create topic with 3 partitions
bin/kafka-topics.sh --create \\
  --bootstrap-server localhost:9092 \\
  --replication-factor 1 \\
  --partitions 3 \\
  --topic user-events

# List topics
bin/kafka-topics.sh --list --bootstrap-server localhost:9092

# Describe topic
bin/kafka-topics.sh --describe \\
  --bootstrap-server localhost:9092 \\
  --topic user-events


ğŸ“¤ 4. Produce Test Messages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Console producer (manual testing)
bin/kafka-console-producer.sh \\
  --bootstrap-server localhost:9092 \\
  --topic user-events \\
  --property "parse.key=true" \\
  --property "key.separator=:"

# Then type messages:
user1:{"event":"page_view","page":"/home"}
user2:{"event":"purchase","amount":99.99}


ğŸ“¥ 5. Consume Messages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Console consumer (verify messages)
bin/kafka-console-consumer.sh \\
  --bootstrap-server localhost:9092 \\
  --topic user-events \\
  --from-beginning \\
  --property print.key=true \\
  --property key.separator=" : "


ğŸ 6. Python Producer (for testing):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from kafka import KafkaProducer
import json
import time

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Send events
for i in range(100):
    event = {
        "user_id": f"user_{i % 10}",
        "event_type": "page_view",
        "timestamp": time.time()
    }
    producer.send('user-events', value=event)
    time.sleep(0.1)

producer.flush()


âš™ï¸  7. Run PySpark with Kafka:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \\
  03_kafka_streaming.py


ğŸ³ 8. Docker Compose (Recommended for Development):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# docker-compose.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
  
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

# Start
docker-compose up -d

# Stop
docker-compose down
    """)


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ”¥ " * 40)
    print("KAFKA STREAMING WITH PYSPARK - COMPREHENSIVE GUIDE")
    print("ğŸ”¥ " * 40)
    
    spark = create_kafka_spark_session()
    
    # Run examples
    example_1_basic_kafka_read(spark)
    example_2_json_parsing(spark)
    example_3_windowed_aggregations(spark)
    example_4_stream_to_stream_join(spark)
    example_5_exactly_once_semantics(spark)
    example_6_production_monitoring(spark)
    
    # Setup guide
    kafka_setup_guide()
    
    print("\n" + "=" * 80)
    print("âœ… KAFKA STREAMING EXAMPLES COMPLETE")
    print("=" * 80)
    
    print("""
ğŸ“š Key Takeaways:

1. Kafka Integration:
   â€¢ readStream.format("kafka") for consuming
   â€¢ writeStream.format("kafka") for producing
   â€¢ Binary key/value â†’ cast to string and parse JSON

2. Windowed Aggregations:
   â€¢ Tumbling windows (non-overlapping)
   â€¢ Sliding windows (overlapping)
   â€¢ Watermarks for late data handling

3. Stream-to-Stream Joins:
   â€¢ Requires watermarks on both streams
   â€¢ Time-bounded joins prevent unbounded state
   â€¢ Inner, left outer, right outer join support

4. Exactly-Once Semantics:
   â€¢ Checkpointing required
   â€¢ Kafka transactions for sink
   â€¢ Idempotent operations

5. Production Best Practices:
   â€¢ Monitor consumer lag
   â€¢ Set appropriate trigger intervals
   â€¢ Use distributed checkpoint location
   â€¢ Add watermarks to stateful operations
   â€¢ Test failure recovery

ğŸ¯ Next Steps:
   â€¢ Start Kafka cluster (local or Docker)
   â€¢ Create test topics
   â€¢ Run examples with real Kafka
   â€¢ Build custom streaming pipeline
   â€¢ Deploy to production with monitoring

ğŸ”— Related Files:
   â€¢ 01_socket_streaming.py (basic streaming)
   â€¢ 02_file_streaming.py (file source)
   â€¢ cluster_computing/08_shuffle_optimization.py
   â€¢ optimization/01_join_strategies.py
    """)
    
    spark.stop()


if __name__ == "__main__":
    main()
