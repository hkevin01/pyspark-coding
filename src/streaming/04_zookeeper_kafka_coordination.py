#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
ZOOKEEPER & KAFKA - Distributed Coordination Deep Dive
================================================================================

MODULE OVERVIEW:
----------------
Apache ZooKeeper is a distributed coordination service that acts as the "central
nervous system" for distributed applications. In the context of Kafka, ZooKeeper
manages cluster metadata, leader election, and configuration.

This module explains:
â€¢ What ZooKeeper is and why it exists
â€¢ How Kafka uses ZooKeeper (legacy) and KRaft (new)
â€¢ Distributed coordination patterns
â€¢ Leader election mechanisms
â€¢ Configuration management
â€¢ Service discovery
â€¢ Production deployment strategies

WHAT IS ZOOKEEPER?
------------------

ZooKeeper is a distributed, hierarchical key-value store that provides:
1. **Coordination**: Synchronize actions across distributed nodes
2. **Configuration**: Centralized configuration management
3. **Naming**: Service discovery and registry
4. **Synchronization**: Distributed locks and barriers
5. **Leader Election**: Choose a leader from multiple nodes

Think of ZooKeeper as:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "A Filing System + Notification System for Distributed Apps"  â”‚
â”‚                                                                â”‚
â”‚  Filing System:                    Notification System:        â”‚
â”‚  â€¢ Hierarchical namespace          â€¢ Watch for changes        â”‚
â”‚  â€¢ Store small data (KB)           â€¢ Get notified instantly   â”‚
â”‚  â€¢ Atomic operations               â€¢ React to cluster events  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ZOOKEEPER DATA MODEL:
---------------------

ZooKeeper uses a hierarchical namespace similar to a file system:

```
/                              â† Root
â”œâ”€â”€ kafka                      â† Kafka cluster metadata
â”‚   â”œâ”€â”€ brokers                â† Broker information
â”‚   â”‚   â”œâ”€â”€ ids                â† Active broker IDs
â”‚   â”‚   â”‚   â”œâ”€â”€ 0              â† Broker 0 details
â”‚   â”‚   â”‚   â”œâ”€â”€ 1              â† Broker 1 details
â”‚   â”‚   â”‚   â””â”€â”€ 2              â† Broker 2 details
â”‚   â”‚   â””â”€â”€ topics             â† Topic metadata
â”‚   â”‚       â”œâ”€â”€ user-events    â† Topic configuration
â”‚   â”‚       â””â”€â”€ orders         â† Topic configuration
â”‚   â”œâ”€â”€ controller             â† Current controller broker ID
â”‚   â”œâ”€â”€ controller_epoch       â† Controller version number
â”‚   â””â”€â”€ config                 â† Cluster configuration
â”‚       â”œâ”€â”€ topics             â† Topic-specific configs
â”‚       â””â”€â”€ brokers            â† Broker configs
â”œâ”€â”€ consumers                  â† Consumer group metadata
â”‚   â””â”€â”€ my-consumer-group      â† Consumer group state
â”‚       â”œâ”€â”€ offsets            â† Committed offsets
â”‚       â””â”€â”€ owners             â† Partition ownership
â””â”€â”€ spark                      â† Spark cluster coordination
    â”œâ”€â”€ masters                â† Spark master nodes
    â””â”€â”€ workers                â† Spark worker registration
```

Each node (znode) can:
â€¢ Store data (up to 1MB, typically KB)
â€¢ Have children (like a directory)
â€¢ Be watched for changes
â€¢ Be ephemeral (disappears when client disconnects)
â€¢ Be sequential (auto-incrementing suffix)

ZOOKEEPER ARCHITECTURE:
-----------------------

```
ZooKeeper Ensemble (Cluster):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ZooKeeper Cluster                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Leader     â”‚  â”‚  Follower 1  â”‚  â”‚  Follower 2  â”‚       â”‚
â”‚  â”‚  (ZK Node 1) â”‚  â”‚  (ZK Node 2) â”‚  â”‚  (ZK Node 3) â”‚       â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚       â”‚
â”‚  â”‚  Handles     â”‚  â”‚  Forwards    â”‚  â”‚  Forwards    â”‚       â”‚
â”‚  â”‚  all writes  â”‚  â”‚  writes to   â”‚  â”‚  writes to   â”‚       â”‚
â”‚  â”‚              â”‚  â”‚  leader      â”‚  â”‚  leader      â”‚       â”‚
â”‚  â”‚  Handles     â”‚  â”‚  Handles     â”‚  â”‚  Handles     â”‚       â”‚
â”‚  â”‚  reads       â”‚  â”‚  reads       â”‚  â”‚  reads       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                   Quorum Protocol (Zab)                      â”‚
â”‚             (Majority must agree on writes)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Kafka Brokers     â”‚  â”‚  Spark Master  â”‚
         â”‚  (Clients)         â”‚  â”‚  (Client)      â”‚
         â”‚  â€¢ Register        â”‚  â”‚  â€¢ Register    â”‚
         â”‚  â€¢ Watch topics    â”‚  â”‚  â€¢ Elect leaderâ”‚
         â”‚  â€¢ Elect controllerâ”‚  â”‚  â€¢ Get config  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Properties:
â€¢ Odd number of nodes (3, 5, or 7 typical)
â€¢ Quorum = Majority (3 nodes â†’ need 2, 5 nodes â†’ need 3)
â€¢ Can tolerate (n-1)/2 failures (3 nodes â†’ 1 failure, 5 nodes â†’ 2 failures)
â€¢ All writes go through leader (consistency)
â€¢ Reads can be served by any node (scalability)
```

WHY KAFKA NEEDS ZOOKEEPER (Legacy Architecture):
-------------------------------------------------

Before Kafka 2.8, ZooKeeper was essential for:

1. **Broker Registration & Discovery**
   ```
   When Kafka broker starts:
   1. Broker connects to ZooKeeper
   2. Creates ephemeral znode: /brokers/ids/<broker-id>
   3. Stores broker metadata (host, port, topics)
   4. If broker crashes â†’ znode disappears automatically
   5. Other brokers detect the change via watches
   ```

2. **Controller Election**
   ```
   Controller = The boss broker that manages the cluster

   Election Process:
   1. All brokers watch /controller znode
   2. First broker to create /controller becomes controller
   3. Controller gets a unique epoch number
   4. If controller fails â†’ znode disappears
   5. New election triggered automatically
   6. New controller gets higher epoch number

   Controller Responsibilities:
   â€¢ Assign partitions to brokers
   â€¢ Monitor broker failures
   â€¢ Trigger partition leader elections
   â€¢ Manage partition replicas
   ```

3. **Topic Configuration**
   ```
   Topic metadata stored in ZooKeeper:
   /brokers/topics/<topic-name>
   {
     "version": 1,
     "partitions": {
       "0": [1, 2, 3],    # Partition 0: replicas on brokers 1,2,3
       "1": [2, 3, 1],    # Partition 1: replicas on brokers 2,3,1
       "2": [3, 1, 2]     # Partition 2: replicas on brokers 3,1,2
     }
   }

   â€¢ All brokers watch topic changes
   â€¢ Instant cluster-wide notification
   â€¢ Consistent view across cluster
   ```

4. **Partition Leader Election**
   ```
   Each partition has:
   â€¢ Leader: Handles all reads/writes
   â€¢ Followers: Replicate data from leader

   Leader Election (via ZooKeeper):
   /brokers/topics/<topic>/partitions/<partition>/state
   {
     "leader": 1,              # Broker 1 is leader
     "isr": [1, 2, 3],         # In-Sync Replicas
     "controller_epoch": 5,    # Controller version
     "leader_epoch": 12        # Leader version
   }

   If leader fails:
   1. Controller detects via ZooKeeper watch
   2. Selects new leader from ISR
   3. Updates partition state in ZooKeeper
   4. All brokers notified via watches
   ```

5. **Consumer Group Coordination** (Old Consumer)
   ```
   Consumer group metadata:
   /consumers/<group-id>/ids/<consumer-id>  # Active consumers
   /consumers/<group-id>/owners/<topic>/<partition>  # Who owns what
   /consumers/<group-id>/offsets/<topic>/<partition>  # Committed offsets

   Rebalancing Process:
   1. New consumer joins â†’ creates znode
   2. All consumers watch group membership
   3. Change detected â†’ trigger rebalance
   4. Partitions reassigned
   5. New ownership written to ZooKeeper
   ```

KAFKA WITHOUT ZOOKEEPER: KRaft MODE
------------------------------------

Starting Kafka 2.8+ (Production-ready in 3.3+), Kafka can run without ZooKeeper!

**KRaft (Kafka Raft)**: Kafka's own consensus protocol

```
Old Architecture (with ZooKeeper):          New Architecture (KRaft):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ZooKeeper   â”‚                           â”‚  Kafka Cluster   â”‚
â”‚   Cluster    â”‚                           â”‚  (Self-managed)  â”‚
â”‚  (3 nodes)   â”‚                           â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚                                   â”‚  â”‚ Controller â”‚  â”‚
       â”‚                                   â”‚  â”‚  Quorum    â”‚  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚ (3 nodes)  â”‚  â”‚
â”‚   Kafka Brokers       â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”      â”‚                  â”‚        â”‚         â”‚
â”‚  â”‚Brkr1â”‚ â”‚Brkr2â”‚      â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜      â”‚                  â”‚  â”‚  Brokers   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”â”‚  â”‚
                                           â”‚  â”‚ â”‚Br1 â”‚Br2 â”‚â”‚  â”‚
Total: 6 nodes (3 ZK + 3 Kafka)            â”‚  â”‚ â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜â”‚  â”‚
                                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          Total: 3 nodes only!

Benefits of KRaft:
âœ… Simpler deployment (no separate ZooKeeper cluster)
âœ… Faster metadata operations (no network hop)
âœ… Better scalability (millions of partitions)
âœ… Faster recovery (no ZooKeeper bottleneck)
âœ… Easier operations (one system to manage)
```

ZOOKEEPER OPERATIONS:
---------------------

Basic Operations (via Python kazoo library):

```python
from kazoo.client import KazooClient

# 1. Connect to ZooKeeper
zk = KazooClient(hosts='localhost:2181')
zk.start()

# 2. Create a node (znode)
zk.create("/myapp", b"my data")

# 3. Read data from node
data, stat = zk.get("/myapp")
print(f"Data: {data}, Version: {stat.version}")

# 4. Set data (update)
zk.set("/myapp", b"new data")

# 5. Create ephemeral node (disappears when client disconnects)
zk.create("/myapp/temp", b"temp data", ephemeral=True)

# 6. Create sequential node (auto-incrementing)
path = zk.create("/myapp/item-", b"data", sequence=True)
# Creates: /myapp/item-0000000001, /myapp/item-0000000002, etc.

# 7. Watch for changes
@zk.DataWatch("/myapp")
def watch_node(data, stat):
    print(f"Data changed: {data}")

# 8. Check if node exists
if zk.exists("/myapp"):
    print("Node exists")

# 9. List children
children = zk.get_children("/myapp")
print(f"Children: {children}")

# 10. Delete node
zk.delete("/myapp")

zk.stop()
```

DISTRIBUTED COORDINATION PATTERNS:
----------------------------------

1. **Leader Election Pattern**
   ```python
   from kazoo.recipe.election import Election

   # Multiple processes compete to be leader
   election = Election(zk, "/election", "candidate-1")

   # Block until this process becomes leader
   election.run(leader_function)

   def leader_function():
       print("I am the leader!")
       # Do leader work...
       # If process crashes, leadership automatically transfers
   ```

2. **Distributed Lock Pattern**
   ```python
   from kazoo.recipe.lock import Lock

   # Only one process can hold the lock
   lock = Lock(zk, "/locks/mylock")

   with lock:
       print("I have the lock!")
       # Critical section - only one process executes this
       # Atomic operation across cluster
   ```

3. **Service Discovery Pattern**
   ```python
   # Service registration
   service_path = "/services/my-service"
   zk.create(service_path, b'{"host": "10.0.0.1", "port": 8080}', ephemeral=True)

   # Service discovery
   @zk.ChildrenWatch("/services")
   def watch_services(children):
       print(f"Available services: {children}")
       for child in children:
           data, _ = zk.get(f"/services/{child}")
           print(f"Service {child}: {data}")
   ```

4. **Configuration Management Pattern**
   ```python
   # Centralized config
   config_path = "/config/myapp"
   zk.create(config_path, b'{"db": "mysql://localhost:3306"}')

   # All nodes watch for config changes
   @zk.DataWatch(config_path)
   def update_config(data, stat):
       config = json.loads(data)
       print(f"Config updated: {config}")
       # Reload application config
       reload_app_config(config)
   ```

PRODUCTION DEPLOYMENT:
----------------------

ZooKeeper Ensemble Sizing:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster Size           â”‚ Fault Toleranceâ”‚ Use Case     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 node (standalone)    â”‚ 0 failures    â”‚ Development  â”‚
â”‚ 3 nodes (ensemble)     â”‚ 1 failure     â”‚ Small prod   â”‚
â”‚ 5 nodes (ensemble)     â”‚ 2 failures    â”‚ Medium prod  â”‚
â”‚ 7 nodes (ensemble)     â”‚ 3 failures    â”‚ Large prod   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸  Never use even numbers! (2, 4, 6)
   â€¢ 3 nodes: Can tolerate 1 failure
   â€¢ 4 nodes: Can still only tolerate 1 failure (need 3 for quorum)
   â€¢ Result: Wasted resources
```

Best Practices:
```
âœ… DO:
â€¢ Use odd number of ZooKeeper nodes (3, 5, 7)
â€¢ Deploy ZooKeeper on separate machines from Kafka
â€¢ Use SSDs for ZooKeeper data directory
â€¢ Monitor ZooKeeper health (latency, connections)
â€¢ Set appropriate JVM heap (typically 1-4GB)
â€¢ Enable ZooKeeper authentication (SASL)
â€¢ Use separate ZooKeeper for each Kafka cluster
â€¢ Backup ZooKeeper data directory
â€¢ Set up monitoring and alerting

âŒ DON'T:
â€¢ Use even numbers (2, 4, 6 nodes)
â€¢ Collocate ZooKeeper with Kafka on same machine
â€¢ Use spinning disks (HDD) for ZooKeeper data
â€¢ Share ZooKeeper across multiple Kafka clusters
â€¢ Ignore ZooKeeper logs and metrics
â€¢ Run without authentication in production
â€¢ Store large data in ZooKeeper (>1MB)
â€¢ Perform frequent writes (ZooKeeper is for coordination, not storage)
```

MONITORING ZOOKEEPER:
---------------------

Key Metrics to Monitor:
```
1. Latency:
   â€¢ avg_latency: Average request latency
   â€¢ min_latency: Minimum latency
   â€¢ max_latency: Maximum latency
   â€¢ Target: < 10ms average

2. Connections:
   â€¢ num_alive_connections: Active client connections
   â€¢ Target: Stable count

3. Outstanding Requests:
   â€¢ outstanding_requests: Queued requests
   â€¢ Target: < 100

4. Znodes:
   â€¢ znode_count: Total nodes in namespace
   â€¢ Target: < 1 million

5. Data Size:
   â€¢ approximate_data_size: Total data stored
   â€¢ Target: < 1GB

6. Watch Count:
   â€¢ watch_count: Active watches
   â€¢ Target: Stable

7. Leadership:
   â€¢ leader_uptime: Time current leader has been stable
   â€¢ Target: Long uptime (no frequent elections)
```

Check ZooKeeper Status:
```bash
# Four-letter commands (telnet to port 2181)
echo stat | nc localhost 2181  # Server statistics
echo mntr | nc localhost 2181  # Monitoring data
echo conf | nc localhost 2181  # Configuration
echo cons | nc localhost 2181  # Connections
echo ruok | nc localhost 2181  # Health check (returns "imok")
```

MIGRATION PATH: ZOOKEEPER â†’ KRAFT:
-----------------------------------

```
Step-by-Step Migration:

1. Current State: Kafka with ZooKeeper
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ZooKeeper â”‚â”€â”€â”€â–¶â”‚   Kafka    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Upgrade Kafka to 3.3+ (supports both modes)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ZooKeeper â”‚â”€â”€â”€â–¶â”‚ Kafka 3.3+ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Create new KRaft cluster (parallel deployment)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ZooKeeper â”‚â”€â”€â”€â–¶â”‚  Old Kafka â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ KRaft Kafkaâ”‚ (New)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Migrate topics/data to KRaft cluster
   (Use MirrorMaker 2.0 for live migration)

5. Switch producers/consumers to KRaft cluster

6. Decommission old ZooKeeper-based cluster
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ KRaft Kafkaâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline: 2-6 weeks depending on cluster size
```

WHY ZOOKEEPER IS AWESOME:
-------------------------

```
Use Cases Beyond Kafka:

1. Distributed Locking:
   â€¢ Multiple processes need exclusive access
   â€¢ Example: Batch job coordination

2. Leader Election:
   â€¢ Multiple nodes, only one does work
   â€¢ Example: Active/passive database setup

3. Configuration Management:
   â€¢ Centralized config for distributed apps
   â€¢ Example: Feature flags, A/B testing

4. Service Discovery:
   â€¢ Dynamic service registration/discovery
   â€¢ Example: Microservices finding each other

5. Distributed Queues:
   â€¢ Implement priority queues
   â€¢ Example: Task scheduling systems

6. Barriers & Semaphores:
   â€¢ Coordinate multiple processes
   â€¢ Example: MapReduce-style computation

Companies Using ZooKeeper:
â€¢ Netflix (service discovery)
â€¢ Yahoo (coordination service)
â€¢ Twitter (distributed systems)
â€¢ LinkedIn (Kafka coordination)
â€¢ Uber (service mesh coordination)
```

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 1.0.0 - ZooKeeper & Kafka Coordination Guide
UPDATED: 2024
================================================================================
"""

import json
import time

from pyspark.sql import SparkSession


def explain_zookeeper_basics():
    """
    Explain ZooKeeper fundamentals with examples.
    """
    print("=" * 80)
    print("ZOOKEEPER FUNDAMENTALS")
    print("=" * 80)

    print(
        """
ğŸ”· What is ZooKeeper?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ZooKeeper is a centralized service for maintaining:
â€¢ Configuration information
â€¢ Naming
â€¢ Providing distributed synchronization
â€¢ Providing group services

Think of it as a "Coordination Kernel" for distributed systems.

ğŸ¯ Core Guarantees:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Sequential Consistency: Updates applied in order
2. Atomicity: Updates either succeed completely or fail
3. Single System Image: Clients see consistent view
4. Reliability: Changes persist once applied
5. Timeliness: Client view is up-to-date within time bound

ğŸ“Š Data Model:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hierarchical namespace (like a file system):

    /
    â”œâ”€â”€ kafka
    â”‚   â”œâ”€â”€ brokers
    â”‚   â”‚   â””â”€â”€ ids
    â”‚   â”‚       â”œâ”€â”€ 0  (Broker 0 info)
    â”‚   â”‚       â”œâ”€â”€ 1  (Broker 1 info)
    â”‚   â”‚       â””â”€â”€ 2  (Broker 2 info)
    â”‚   â”œâ”€â”€ controller  (Current controller broker)
    â”‚   â””â”€â”€ topics
    â”‚       â”œâ”€â”€ topic1
    â”‚       â””â”€â”€ topic2
    â””â”€â”€ myapp
        â”œâ”€â”€ config  (App configuration)
        â””â”€â”€ locks   (Distributed locks)

Each node (znode) can:
â€¢ Store data (up to 1MB, typically < 1KB)
â€¢ Have children
â€¢ Be watched for changes
â€¢ Be ephemeral (auto-deleted when client disconnects)
â€¢ Be persistent (survives client disconnect)
    """
    )

    print("\n" + "=" * 80)
    print("ZOOKEEPER OPERATIONS")
    print("=" * 80)

    print(
        """
Basic Operations (Python with kazoo library):

# 1. Installation
pip install kazoo

# 2. Connect to ZooKeeper
from kazoo.client import KazooClient
zk = KazooClient(hosts='localhost:2181')
zk.start()

# 3. Create a node
zk.create("/myapp", b"initial data")
zk.create("/myapp/config", b'{"timeout": 30}')

# 4. Read data
data, stat = zk.get("/myapp/config")
print(f"Data: {data}")
print(f"Version: {stat.version}")

# 5. Update data
zk.set("/myapp/config", b'{"timeout": 60}')

# 6. Create ephemeral node (disappears when client disconnects)
zk.create("/myapp/session-12345", ephemeral=True)

# 7. Watch for changes
@zk.DataWatch("/myapp/config")
def watch_config(data, stat):
    print(f"Config changed: {data}")
    # React to configuration changes

# 8. List children
children = zk.get_children("/myapp")
print(f"Children: {children}")

# 9. Delete node
zk.delete("/myapp/config")

# 10. Close connection
zk.stop()
    """
    )


def explain_kafka_zookeeper_relationship():
    """
    Explain how Kafka uses ZooKeeper.
    """
    print("\n" + "=" * 80)
    print("KAFKA + ZOOKEEPER RELATIONSHIP")
    print("=" * 80)

    print(
        """
ğŸ”· How Kafka Uses ZooKeeper (Legacy Architecture):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. BROKER REGISTRATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ When Kafka broker starts:                                â”‚
   â”‚                                                           â”‚
   â”‚ 1. Connects to ZooKeeper                                 â”‚
   â”‚ 2. Creates ephemeral znode: /brokers/ids/<broker-id>    â”‚
   â”‚ 3. Stores: host, port, endpoints, rack info              â”‚
   â”‚                                                           â”‚
   â”‚ Example: /brokers/ids/1                                  â”‚
   â”‚ {                                                         â”‚
   â”‚   "host": "kafka1.example.com",                          â”‚
   â”‚   "port": 9092,                                          â”‚
   â”‚   "rack": "rack-1"                                       â”‚
   â”‚ }                                                         â”‚
   â”‚                                                           â”‚
   â”‚ If broker crashes â†’ znode disappears automatically!      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. CONTROLLER ELECTION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Controller = "Boss Broker" that manages cluster          â”‚
   â”‚                                                           â”‚
   â”‚ Election Process:                                        â”‚
   â”‚ 1. All brokers try to create /controller znode          â”‚
   â”‚ 2. First one succeeds â†’ becomes controller              â”‚
   â”‚ 3. Others watch /controller for changes                 â”‚
   â”‚ 4. If controller fails â†’ znode deleted                  â”‚
   â”‚ 5. New election happens automatically                   â”‚
   â”‚                                                           â”‚
   â”‚ Controller Responsibilities:                             â”‚
   â”‚ â€¢ Manage partition leader elections                     â”‚
   â”‚ â€¢ Monitor broker failures                               â”‚
   â”‚ â€¢ Update metadata                                       â”‚
   â”‚ â€¢ Coordinate partition reassignments                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. TOPIC METADATA
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Path: /brokers/topics/<topic-name>                      â”‚
   â”‚                                                           â”‚
   â”‚ Example: /brokers/topics/user-events                    â”‚
   â”‚ {                                                         â”‚
   â”‚   "version": 1,                                          â”‚
   â”‚   "partitions": {                                        â”‚
   â”‚     "0": [1, 2, 3],  # Replicas for partition 0        â”‚
   â”‚     "1": [2, 3, 1],  # Replicas for partition 1        â”‚
   â”‚     "2": [3, 1, 2]   # Replicas for partition 2        â”‚
   â”‚   }                                                       â”‚
   â”‚ }                                                         â”‚
   â”‚                                                           â”‚
   â”‚ All brokers watch topics â†’ instant updates!             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. PARTITION LEADER ELECTION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Path: /brokers/topics/<topic>/partitions/<id>/state     â”‚
   â”‚                                                           â”‚
   â”‚ Example: /brokers/topics/orders/partitions/0/state      â”‚
   â”‚ {                                                         â”‚
   â”‚   "leader": 1,              # Broker 1 is leader        â”‚
   â”‚   "isr": [1, 2, 3],         # In-Sync Replicas         â”‚
   â”‚   "controller_epoch": 5,                                â”‚
   â”‚   "leader_epoch": 12                                    â”‚
   â”‚ }                                                         â”‚
   â”‚                                                           â”‚
   â”‚ If leader fails:                                        â”‚
   â”‚ 1. Controller detects via ZooKeeper                     â”‚
   â”‚ 2. Picks new leader from ISR                           â”‚
   â”‚ 3. Updates state in ZooKeeper                          â”‚
   â”‚ 4. All brokers notified via watches                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. CONSUMER GROUP COORDINATION (Old API)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Path: /consumers/<group-id>                              â”‚
   â”‚                                                           â”‚
   â”‚ Structure:                                               â”‚
   â”‚ /consumers/my-group/                                     â”‚
   â”‚   â”œâ”€â”€ ids/                  # Active consumers          â”‚
   â”‚   â”‚   â”œâ”€â”€ consumer-1                                    â”‚
   â”‚   â”‚   â””â”€â”€ consumer-2                                    â”‚
   â”‚   â”œâ”€â”€ owners/               # Partition ownership       â”‚
   â”‚   â”‚   â””â”€â”€ topic1/                                       â”‚
   â”‚   â”‚       â”œâ”€â”€ 0 â†’ consumer-1                           â”‚
   â”‚   â”‚       â””â”€â”€ 1 â†’ consumer-2                           â”‚
   â”‚   â””â”€â”€ offsets/              # Committed offsets         â”‚
   â”‚       â””â”€â”€ topic1/                                       â”‚
   â”‚           â”œâ”€â”€ 0 â†’ 12345                                â”‚
   â”‚           â””â”€â”€ 1 â†’ 67890                                â”‚
   â”‚                                                           â”‚
   â”‚ Note: New consumers use __consumer_offsets topic        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    )

    print("\n" + "=" * 80)
    print("KAFKA WITHOUT ZOOKEEPER: KRaft MODE")
    print("=" * 80)

    print(
        """
ğŸš€ KRaft (Kafka Raft): ZooKeeper Removal
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Starting Kafka 2.8+ (Production-ready 3.3+)

Old: Kafka + ZooKeeper (2 systems)          New: Kafka Only (1 system)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ZooKeeper Cluster         â”‚            â”‚   Kafka Cluster      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”          â”‚            â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ZK1 â”‚ZK2 â”‚ZK3 â”‚          â”‚            â”‚   â”‚ Controllers  â”‚   â”‚
â”‚   â””â”€â”¬â”€â”€â”´â”€â”¬â”€â”€â”´â”€â”¬â”€â”€â”˜          â”‚            â”‚   â”‚  (Quorum)    â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜             â”‚            â”‚   â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚ â”‚C1 â”‚C2 â”‚C3 â”‚â”‚   â”‚
          â”‚                                â”‚   â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜â”‚   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   Kafka Brokers             â”‚            â”‚          â”‚           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”          â”‚            â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚BR1 â”‚BR2 â”‚BR3 â”‚          â”‚            â”‚   â”‚   Brokers    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜          â”‚            â”‚   â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚ â”‚B1 â”‚B2 â”‚B3 â”‚â”‚   â”‚
                                           â”‚   â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜â”‚   â”‚
6 servers total                            â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
(3 ZK + 3 Kafka)                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           3 servers total
                                           (Kafka manages itself!)

âœ… Benefits of KRaft:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Simpler architecture (no ZooKeeper to manage)
â€¢ Faster metadata operations (no network hop)
â€¢ Better scalability (millions of partitions)
â€¢ Faster startup and recovery
â€¢ Reduced operational complexity
â€¢ Lower resource requirements

ğŸ”„ Migration Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Kafka 2.8 (2021): KRaft in early access
â€¢ Kafka 3.0 (2021): KRaft improvements
â€¢ Kafka 3.3 (2022): KRaft production-ready
â€¢ Kafka 4.0 (2024): ZooKeeper deprecated
â€¢ Future: ZooKeeper support removed

ğŸ’¡ Recommendation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ New deployments: Use KRaft mode
â€¢ Existing deployments: Plan migration to KRaft
â€¢ ZooKeeper mode: Still supported but legacy
    """
    )


def demonstrate_coordination_patterns():
    """
    Demonstrate distributed coordination patterns.
    """
    print("\n" + "=" * 80)
    print("DISTRIBUTED COORDINATION PATTERNS")
    print("=" * 80)

    print(
        """
ğŸ”· Common Patterns with ZooKeeper:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. LEADER ELECTION PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Multiple processes, only one should be active

from kazoo.recipe.election import Election

# Process 1
election = Election(zk, "/election/myapp")
election.run(do_leader_work)  # Blocks until elected

def do_leader_work():
    print("I am the leader!")
    while True:
        # Do work only leader should do
        time.sleep(1)
    # If this process crashes, another becomes leader

Real-World Examples:
â€¢ Kafka Controller (only one broker is controller)
â€¢ Active/Passive Database (only one is active)
â€¢ Batch Job Coordinator (only one runs the job)


2. DISTRIBUTED LOCK PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Ensure only one process accesses resource

from kazoo.recipe.lock import Lock

# Multiple processes compete for lock
lock = Lock(zk, "/locks/critical-resource")

with lock:
    print("I have exclusive access!")
    # Critical section - only one process here
    modify_shared_resource()
# Lock automatically released

Real-World Examples:
â€¢ Database migration (only one runs)
â€¢ File writing (prevent corruption)
â€¢ Resource allocation (assign work once)


3. SERVICE DISCOVERY PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Find available services dynamically

# Service Registration (by service)
service_info = {
    "host": "10.0.0.5",
    "port": 8080,
    "protocol": "http"
}
zk.create(
    "/services/api-server/instance-1",
    json.dumps(service_info).encode(),
    ephemeral=True  # Disappears if service crashes
)

# Service Discovery (by client)
@zk.ChildrenWatch("/services/api-server")
def watch_services(children):
    print(f"Available instances: {len(children)}")
    for child in children:
        data, _ = zk.get(f"/services/api-server/{child}")
        service = json.loads(data)
        print(f"  {child}: {service['host']}:{service['port']}")

Real-World Examples:
â€¢ Microservices finding each other
â€¢ Load balancer discovering backend servers
â€¢ Client finding available database replicas


4. CONFIGURATION MANAGEMENT PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Centralized config with instant updates

# Set configuration (by admin)
config = {
    "max_connections": 100,
    "timeout": 30,
    "cache_size": "1GB"
}
zk.ensure_path("/config/myapp")
zk.set("/config/myapp", json.dumps(config).encode())

# Watch configuration (by all app instances)
@zk.DataWatch("/config/myapp")
def update_config(data, stat):
    config = json.loads(data)
    print(f"Config updated (version {stat.version})")
    # Apply new configuration
    app.reload_config(config)

Real-World Examples:
â€¢ Feature flags (enable/disable features)
â€¢ A/B testing (route % of traffic)
â€¢ Database connection strings
â€¢ API rate limits


5. BARRIER PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Coordinate multiple processes to start together

from kazoo.recipe.barrier import Barrier

# All processes must reach barrier before any continue
barrier = Barrier(zk, "/barriers/start-processing")

# Wait for all participants
barrier.wait()  # Blocks until all processes call wait()

print("All processes ready - starting work!")
# Now all processes start simultaneously

Real-World Examples:
â€¢ Distributed testing (start all test runners together)
â€¢ MapReduce (wait for all mappers before reduce)
â€¢ Multi-stage pipelines (synchronize stages)


6. QUEUE PATTERN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Case: Distributed work queue

from kazoo.recipe.queue import Queue

# Producer
queue = Queue(zk, "/queues/tasks")
queue.put(b"task-1")
queue.put(b"task-2")

# Consumer
while True:
    task = queue.get()
    print(f"Processing: {task}")
    process_task(task)

Real-World Examples:
â€¢ Job scheduler
â€¢ Task distribution
â€¢ Message passing
    """
    )


def production_deployment_guide():
    """
    Guide for production ZooKeeper deployment.
    """
    print("\n" + "=" * 80)
    print("PRODUCTION DEPLOYMENT GUIDE")
    print("=" * 80)

    print(
        """
ğŸ”· ZooKeeper Production Best Practices:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. CLUSTER SIZING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nodes       â”‚ Fault Toleranceâ”‚ Quorum Needed â”‚ Use Case    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1           â”‚ 0             â”‚ 1              â”‚ Dev/Test    â”‚
â”‚ 3 (RECOMMENDED)â”‚ 1          â”‚ 2              â”‚ Prod (small)â”‚
â”‚ 5 (RECOMMENDED)â”‚ 2          â”‚ 3              â”‚ Prod (med)  â”‚
â”‚ 7           â”‚ 3             â”‚ 4              â”‚ Prod (large)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸  NEVER use even numbers (2, 4, 6)!
   â€¢ 3 nodes: Tolerates 1 failure, needs 2 for quorum
   â€¢ 4 nodes: Still tolerates 1 failure, needs 3 for quorum
   â€¢ Result: 4 nodes = same fault tolerance as 3 nodes (wasted!)


2. HARDWARE REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Minimum (Small Cluster):
â€¢ CPU: 2-4 cores
â€¢ RAM: 4-8 GB
â€¢ Disk: SSD with 100GB
â€¢ Network: 1 Gbps

Recommended (Medium Cluster):
â€¢ CPU: 4-8 cores
â€¢ RAM: 8-16 GB
â€¢ Disk: SSD with 200GB
â€¢ Network: 10 Gbps

Critical: USE SSDs!
â€¢ ZooKeeper writes transaction log to disk
â€¢ Latency-sensitive operations
â€¢ HDD = slow writes = cluster degradation


3. CONFIGURATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
zoo.cfg (ZooKeeper config file):

# Data directory (must be on SSD)
dataDir=/var/lib/zookeeper/data
dataLogDir=/var/lib/zookeeper/log  # Separate disk if possible

# Client port
clientPort=2181

# Cluster members (3-node ensemble)
server.1=zk1.example.com:2888:3888
server.2=zk2.example.com:2888:3888
server.3=zk3.example.com:2888:3888

# Performance tuning
tickTime=2000                    # Basic time unit (ms)
initLimit=10                     # Follower connect timeout (10 ticks)
syncLimit=5                      # Follower sync timeout (5 ticks)
maxClientCnxns=60                # Max clients per IP
autopurge.snapRetainCount=3      # Keep 3 snapshots
autopurge.purgeInterval=24       # Cleanup every 24 hours

# JVM Settings (zkEnv.sh or systemd service)
JVMFLAGS="-Xms4G -Xmx4G"         # Heap size (1-4GB typical)


4. MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key Metrics:

Health Checks:
echo ruok | nc localhost 2181     # Returns "imok" if healthy
echo stat | nc localhost 2181     # Server statistics
echo mntr | nc localhost 2181     # Detailed monitoring metrics

Critical Metrics:
â€¢ avg_latency < 10ms              # Average request latency
â€¢ outstanding_requests < 100      # Queued requests
â€¢ znode_count < 1,000,000        # Total znodes
â€¢ watch_count (stable)           # Active watches
â€¢ leader_uptime (high)           # Leadership stability

Alerting Thresholds:
ğŸ”´ CRITICAL:
   â€¢ avg_latency > 50ms
   â€¢ outstanding_requests > 1000
   â€¢ Node disconnected from ensemble

ğŸŸ¡ WARNING:
   â€¢ avg_latency > 20ms
   â€¢ outstanding_requests > 100
   â€¢ Frequent leader elections


5. BACKUP & RECOVERY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Backup Strategy:

# Automatic snapshots (configured in zoo.cfg)
autopurge.snapRetainCount=3
autopurge.purgeInterval=24

# Manual backup
cp -r /var/lib/zookeeper/data /backup/zookeeper-$(date +%Y%m%d)

# Recovery
1. Stop ZooKeeper
2. Restore data directory from backup
3. Start ZooKeeper
4. Verify cluster health


6. SECURITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Production Security Checklist:

âœ… Enable SASL Authentication:
# zoo.cfg
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider

âœ… Enable SSL/TLS:
secureClientPort=2182
ssl.keyStore.location=/path/to/keystore.jks
ssl.trustStore.location=/path/to/truststore.jks

âœ… Set ACLs (Access Control Lists):
from kazoo.security import make_digest_acl

acl = make_digest_acl("user", "password", all=True)
zk.create("/secure-node", b"data", acl=[acl])

âœ… Network Isolation:
â€¢ Firewall rules (only allow known IPs)
â€¢ Private network for ZooKeeper cluster
â€¢ No public internet access


7. OPERATIONAL TIPS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… DO:
â€¢ Deploy ZooKeeper on separate machines from Kafka
â€¢ Use dedicated ZooKeeper per Kafka cluster
â€¢ Monitor latency and request queue
â€¢ Set up alerting
â€¢ Regular backups
â€¢ Test failure scenarios
â€¢ Plan capacity (znodes, connections, throughput)

âŒ DON'T:
â€¢ Share ZooKeeper across multiple Kafka clusters
â€¢ Use spinning disks (HDD)
â€¢ Ignore monitoring
â€¢ Store large data in znodes (>1KB typical, 1MB max)
â€¢ Perform frequent writes
â€¢ Use even number of nodes
â€¢ Run without authentication in production


8. TROUBLESHOOTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Common Issues:

Issue: High Latency
â€¢ Check disk I/O (must use SSD)
â€¢ Check network latency between nodes
â€¢ Increase JVM heap if lots of znodes
â€¢ Check for disk space

Issue: Frequent Leader Elections
â€¢ Check network connectivity
â€¢ Check if nodes are overloaded
â€¢ Verify syncLimit is appropriate

Issue: Out of Memory
â€¢ Too many znodes (limit to < 1M)
â€¢ Too many watches (limit connections)
â€¢ Increase JVM heap (-Xmx)

Issue: Connection Timeouts
â€¢ Check maxClientCnxns limit
â€¢ Verify firewall rules
â€¢ Check client connection timeout settings
    """
    )


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ”· " * 40)
    print("ZOOKEEPER & KAFKA COORDINATION - COMPREHENSIVE GUIDE")
    print("ğŸ”· " * 40)

    # Explain ZooKeeper basics
    explain_zookeeper_basics()

    # Explain Kafka-ZooKeeper relationship
    explain_kafka_zookeeper_relationship()

    # Demonstrate coordination patterns
    demonstrate_coordination_patterns()

    # Production deployment guide
    production_deployment_guide()

    print("\n" + "=" * 80)
    print("âœ… ZOOKEEPER & KAFKA GUIDE COMPLETE")
    print("=" * 80)

    print(
        """
ğŸ“š Key Takeaways:

1. ZooKeeper Purpose:
   â€¢ Distributed coordination service
   â€¢ Manages metadata and configuration
   â€¢ Enables leader election and locking
   â€¢ Provides consistency guarantees

2. Kafka + ZooKeeper (Legacy):
   â€¢ Broker registration
   â€¢ Controller election
   â€¢ Topic/partition metadata
   â€¢ Consumer group coordination

3. Kafka without ZooKeeper (KRaft):
   â€¢ Simpler architecture
   â€¢ Better performance
   â€¢ Production-ready in Kafka 3.3+
   â€¢ Future of Kafka

4. Production Deployment:
   â€¢ Use 3, 5, or 7 nodes (odd numbers only)
   â€¢ Deploy on SSDs
   â€¢ Monitor latency and health
   â€¢ Enable authentication and encryption
   â€¢ Regular backups

5. Coordination Patterns:
   â€¢ Leader election
   â€¢ Distributed locks
   â€¢ Service discovery
   â€¢ Configuration management
   â€¢ Barriers and queues

ğŸ¯ Recommendation:
   â€¢ New projects: Use Kafka with KRaft (no ZooKeeper)
   â€¢ Existing Kafka: Plan migration to KRaft
   â€¢ Other distributed apps: ZooKeeper still excellent choice

ğŸ”— Related Files:
   â€¢ src/streaming/03_kafka_streaming.py (Kafka basics)
   â€¢ src/cluster_computing/* (Distributed coordination)
    """
    )


if __name__ == "__main__":
    main()
