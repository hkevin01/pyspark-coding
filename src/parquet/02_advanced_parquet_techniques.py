#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
ADVANCED PARQUET TECHNIQUES - Performance Optimization & Best Practices
================================================================================

üìñ OVERVIEW:
Advanced techniques for working with Parquet files including performance
tuning, statistics, metadata management, and optimization strategies.

üéØ TOPICS COVERED:
‚Ä¢ Row group sizing
‚Ä¢ Dictionary encoding
‚Ä¢ Statistics and metadata
‚Ä¢ Coalescing and repartitioning
‚Ä¢ Performance benchmarking

üöÄ RUN:
spark-submit 02_advanced_parquet_techniques.py
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime, timedelta
import time
import random


def create_spark_session():
    """Create optimized Spark session."""
    spark = SparkSession.builder \
        .appName("Advanced Parquet Techniques") \
        .config("spark.sql.files.maxPartitionBytes", "134217728") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .config("spark.sql.parquet.block.size", "134217728") \
        .config("spark.sql.parquet.page.size", "1048576") \
        .config("spark.sql.parquet.enableVectorizedReader", "true") \
        .config("spark.sql.parquet.recordLevelFilter.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def example_1_row_group_optimization(spark):
    """
    Example 1: Row Group Size Optimization
    
    üìù Demonstrates:
    ‚Ä¢ Controlling row group size
    ‚Ä¢ Impact on query performance
    ‚Ä¢ Memory vs I/O tradeoffs
    """
    print("=" * 80)
    print("EXAMPLE 1: Row Group Size Optimization")
    print("=" * 80)
    
    # Generate large dataset
    print("üìä Generating 1 million records...")
    
    data = [(i, f"user_{i}", random.uniform(0, 1000), random.choice(['A', 'B', 'C']))
            for i in range(1000000)]
    
    df = spark.createDataFrame(data, ['id', 'username', 'value', 'category'])
    
    # Write with different row group sizes
    configs = [
        ("small", 64 * 1024 * 1024),   # 64 MB
        ("medium", 128 * 1024 * 1024),  # 128 MB (default)
        ("large", 256 * 1024 * 1024)    # 256 MB
    ]
    
    print("\nüìù Writing with different row group sizes...")
    print()
    
    for name, size in configs:
        output_path = f"/tmp/pyspark_examples/parquet/rowgroup_{name}"
        
        start = time.time()
        
        df.write \
            .mode("overwrite") \
            .option("parquet.block.size", size) \
            .parquet(output_path)
        
        write_time = time.time() - start
        
        # Get file size
        import os
        total_size = sum(os.path.getsize(os.path.join(root, f))
                        for root, _, files in os.walk(output_path)
                        for f in files if f.endswith('.parquet'))
        
        print(f"   {name.upper()}:")
        print(f"      Row group size: {size / (1024*1024):.0f} MB")
        print(f"      Write time: {write_time:.2f}s")
        print(f"      Total size: {total_size / (1024*1024):.2f} MB")
        print()
    
    print("üí° Row Group Size Guidelines:")
    print("   ‚Ä¢ Small (64MB): Better for selective queries")
    print("   ‚Ä¢ Medium (128MB): Good general-purpose default")
    print("   ‚Ä¢ Large (256MB): Better for full scans")
    print("   ‚Ä¢ Consider: memory available, query patterns, cluster size")
    print()


def example_2_dictionary_encoding(spark):
    """
    Example 2: Dictionary Encoding
    
    üìù Demonstrates:
    ‚Ä¢ Automatic dictionary encoding for low-cardinality columns
    ‚Ä¢ Size savings from encoding
    ‚Ä¢ When encoding is beneficial
    """
    print("=" * 80)
    print("EXAMPLE 2: Dictionary Encoding")
    print("=" * 80)
    
    # Create dataset with varying cardinality
    print("üìä Creating datasets with different cardinality levels...")
    
    size = 100000
    
    # Low cardinality (perfect for dictionary encoding)
    data_low = [(i, random.choice(['A', 'B', 'C', 'D', 'E'])) for i in range(size)]
    df_low = spark.createDataFrame(data_low, ['id', 'category'])
    
    # High cardinality (poor for dictionary encoding)
    data_high = [(i, f"unique_value_{i}") for i in range(size)]
    df_high = spark.createDataFrame(data_high, ['id', 'category'])
    
    # Write both
    path_low = "/tmp/pyspark_examples/parquet/dict_low_cardinality"
    path_high = "/tmp/pyspark_examples/parquet/dict_high_cardinality"
    
    df_low.write.mode("overwrite").parquet(path_low)
    df_high.write.mode("overwrite").parquet(path_high)
    
    # Compare sizes
    import os
    
    size_low = sum(os.path.getsize(os.path.join(root, f))
                   for root, _, files in os.walk(path_low)
                   for f in files if f.endswith('.parquet'))
    
    size_high = sum(os.path.getsize(os.path.join(root, f))
                    for root, _, files in os.walk(path_high)
                    for f in files if f.endswith('.parquet'))
    
    print(f"\nÔøΩÔøΩ Comparison:")
    print(f"   Low cardinality (5 unique values):")
    print(f"      Size: {size_low / (1024*1024):.2f} MB")
    print(f"   High cardinality (100,000 unique values):")
    print(f"      Size: {size_high / (1024*1024):.2f} MB")
    print(f"   Ratio: {size_high / size_low:.2f}x larger")
    print()
    
    print("üí° Dictionary Encoding Best Practices:")
    print("   ‚Ä¢ Most effective for cardinality < 10,000")
    print("   ‚Ä¢ Automatically applied by Parquet writer")
    print("   ‚Ä¢ Great for: status codes, categories, countries")
    print("   ‚Ä¢ Poor for: IDs, timestamps, unique strings")
    print()


def example_3_statistics_and_metadata(spark):
    """
    Example 3: Parquet Statistics and Metadata
    
    üìù Demonstrates:
    ‚Ä¢ Column statistics (min, max, null count)
    ‚Ä¢ Using statistics for query optimization
    ‚Ä¢ Metadata inspection
    """
    print("=" * 80)
    print("EXAMPLE 3: Statistics and Metadata")
    print("=" * 80)
    
    # Create dataset with clear min/max values
    data = [
        (1, 'Alice', 25, 50000, '2024-01-15'),
        (2, 'Bob', 30, 75000, '2024-02-20'),
        (3, 'Charlie', 35, 100000, '2024-03-10'),
        (4, 'Diana', 28, 65000, '2024-04-05'),
        (5, 'Eve', 42, 120000, '2024-05-18')
    ]
    
    df = spark.createDataFrame(data, ['id', 'name', 'age', 'salary', 'hire_date'])
    
    path = "/tmp/pyspark_examples/parquet/with_statistics"
    
    print("üìù Writing data with statistics...")
    df.write.mode("overwrite").parquet(path)
    
    print("‚úÖ Statistics automatically collected:")
    print("   ‚Ä¢ min/max values for each column")
    print("   ‚Ä¢ null counts")
    print("   ‚Ä¢ total row counts")
    print()
    
    print("üîç Demonstrating statistics-based pruning...")
    print("   Query: salary > 150000")
    
    # This query should skip all row groups based on statistics
    df_filtered = spark.read.parquet(path).filter(col('salary') > 150000)
    
    count = df_filtered.count()
    print(f"   Result: {count} rows (Parquet skipped all row groups using stats)")
    print()
    
    print("ÔøΩÔøΩ Statistics Benefits:")
    print("   ‚Ä¢ Skip entire row groups without reading")
    print("   ‚Ä¢ Automatic for numeric, string, date columns")
    print("   ‚Ä¢ Enables min/max pruning")
    print("   ‚Ä¢ Crucial for large datasets")
    print()


def example_4_coalesce_optimization(spark):
    """
    Example 4: Coalesce and Repartition
    
    üìù Demonstrates:
    ‚Ä¢ Controlling output file count
    ‚Ä¢ coalesce() vs repartition()
    ‚Ä¢ Optimal file sizing
    """
    print("=" * 80)
    print("EXAMPLE 4: Coalesce and Repartition")
    print("=" * 80)
    
    # Create large dataset
    print("ÔøΩÔøΩ Creating 500,000 record dataset...")
    data = [(i, f"value_{i}", random.randint(1, 100)) for i in range(500000)]
    df = spark.createDataFrame(data, ['id', 'value', 'score'])
    
    print(f"   Initial partitions: {df.rdd.getNumPartitions()}")
    print()
    
    # Write without coalescing (many small files)
    path_many = "/tmp/pyspark_examples/parquet/many_files"
    print("üìù Writing without coalesce...")
    df.write.mode("overwrite").parquet(path_many)
    
    import os
    file_count_many = len([f for _, _, files in os.walk(path_many) 
                           for f in files if f.endswith('.parquet')])
    
    # Write with coalesce (fewer, larger files)
    path_coalesced = "/tmp/pyspark_examples/parquet/coalesced"
    print(f"üìù Writing with coalesce(4)...")
    df.coalesce(4).write.mode("overwrite").parquet(path_coalesced)
    
    file_count_coalesced = len([f for _, _, files in os.walk(path_coalesced)
                                for f in files if f.endswith('.parquet')])
    
    print(f"\nüìä Comparison:")
    print(f"   Without coalesce: {file_count_many} files")
    print(f"   With coalesce(4): {file_count_coalesced} files")
    print()
    
    print("ÔøΩÔøΩ File Sizing Guidelines:")
    print("   ‚Ä¢ Target: 128-512 MB per file")
    print("   ‚Ä¢ Too many small files: slow metadata operations")
    print("   ‚Ä¢ Too few large files: poor parallelism")
    print("   ‚Ä¢ Use coalesce() to reduce (no shuffle)")
    print("   ‚Ä¢ Use repartition() to increase (with shuffle)")
    print()
    
    # Calculate ideal partition count
    total_size_mb = sum(os.path.getsize(os.path.join(root, f))
                       for root, _, files in os.walk(path_many)
                       for f in files if f.endswith('.parquet')) / (1024*1024)
    
    target_file_size_mb = 256
    ideal_partitions = max(1, int(total_size_mb / target_file_size_mb))
    
    print(f"ÔøΩÔøΩ Calculation for this dataset:")
    print(f"   Total size: {total_size_mb:.2f} MB")
    print(f"   Target per file: {target_file_size_mb} MB")
    print(f"   Recommended partitions: {ideal_partitions}")
    print()


def example_5_performance_benchmark(spark):
    """
    Example 5: Performance Benchmarking
    
    üìù Demonstrates:
    ‚Ä¢ Comparing read/write performance
    ‚Ä¢ Impact of different configurations
    ‚Ä¢ Measuring query execution time
    """
    print("=" * 80)
    print("EXAMPLE 5: Performance Benchmarking")
    print("=" * 80)
    
    # Generate test dataset
    print("üìä Generating 1 million records for benchmark...")
    size = 1000000
    data = [(i, f"user_{i % 1000}", random.uniform(0, 1000), 
             random.choice(['A', 'B', 'C', 'D', 'E']),
             datetime(2024, 1, 1) + timedelta(days=random.randint(0, 365)))
            for i in range(size)]
    
    df = spark.createDataFrame(data, ['id', 'user', 'amount', 'category', 'date'])
    
    configs = [
        ("snappy", "true", 4),
        ("gzip", "true", 4),
        ("snappy", "false", 4),
        ("snappy", "true", 8)
    ]
    
    print("\nüìù Running benchmarks...")
    print()
    
    results = []
    
    for compression, vectorized, partitions in configs:
        path = f"/tmp/pyspark_examples/parquet/bench_{compression}_{vectorized}_{partitions}"
        
        # Configure
        spark.conf.set("spark.sql.parquet.compression.codec", compression)
        spark.conf.set("spark.sql.parquet.enableVectorizedReader", vectorized)
        
        # Write benchmark
        start = time.time()
        df.coalesce(partitions).write.mode("overwrite").parquet(path)
        write_time = time.time() - start
        
        # Read benchmark
        start = time.time()
        count = spark.read.parquet(path).count()
        read_time = time.time() - start
        
        # Query benchmark (filter + aggregation)
        start = time.time()
        result = spark.read.parquet(path) \
            .filter(col('category') == 'A') \
            .groupBy('user') \
            .agg(sum('amount').alias('total')) \
            .count()
        query_time = time.time() - start
        
        # Get size
        import os
        size_mb = sum(os.path.getsize(os.path.join(root, f))
                     for root, _, files in os.walk(path)
                     for f in files if f.endswith('.parquet')) / (1024*1024)
        
        results.append({
            'compression': compression,
            'vectorized': vectorized,
            'partitions': partitions,
            'write_time': write_time,
            'read_time': read_time,
            'query_time': query_time,
            'size_mb': size_mb
        })
    
    print("üìä Benchmark Results:")
    print("-" * 100)
    print(f"{'Compression':<12} {'Vectorized':<12} {'Parts':<8} {'Write(s)':<12} {'Read(s)':<12} {'Query(s)':<12} {'Size(MB)'}")
    print("-" * 100)
    
    for r in results:
        print(f"{r['compression']:<12} {r['vectorized']:<12} {r['partitions']:<8} "
              f"{r['write_time']:<12.2f} {r['read_time']:<12.2f} {r['query_time']:<12.2f} {r['size_mb']:.2f}")
    
    print("-" * 100)
    print()
    
    print("üí° Performance Insights:")
    print("   ‚Ä¢ snappy: Best balance of speed and compression")
    print("   ‚Ä¢ gzip: Slower but better compression ratio")
    print("   ‚Ä¢ Vectorized reader: Significant read performance boost")
    print("   ‚Ä¢ More partitions: Better parallelism for large datasets")
    print()


def example_6_nested_data_structures(spark):
    """
    Example 6: Nested and Complex Data Types
    
    üìù Demonstrates:
    ‚Ä¢ Struct, Array, Map types
    ‚Ä¢ Efficient storage of nested data
    ‚Ä¢ Querying nested structures
    """
    print("=" * 80)
    print("EXAMPLE 6: Nested Data Structures")
    print("=" * 80)
    
    # Create nested data
    data = [
        {
            'id': 1,
            'user': {'name': 'Alice', 'age': 30, 'email': 'alice@example.com'},
            'orders': [
                {'order_id': 'O1', 'amount': 100.0, 'items': ['laptop', 'mouse']},
                {'order_id': 'O2', 'amount': 50.0, 'items': ['keyboard']}
            ],
            'metadata': {'signup_date': '2024-01-01', 'premium': True}
        },
        {
            'id': 2,
            'user': {'name': 'Bob', 'age': 25, 'email': 'bob@example.com'},
            'orders': [
                {'order_id': 'O3', 'amount': 200.0, 'items': ['phone', 'case', 'charger']}
            ],
            'metadata': {'signup_date': '2024-02-15', 'premium': False}
        }
    ]
    
    df = spark.createDataFrame(data)
    
    print("üìã Schema with nested structures:")
    df.printSchema()
    
    path = "/tmp/pyspark_examples/parquet/nested_data"
    
    print("\nüìù Writing nested data to Parquet...")
    df.write.mode("overwrite").parquet(path)
    
    print("‚úÖ Nested structures preserved in Parquet")
    print()
    
    print("üìñ Reading and querying nested data...")
    df_read = spark.read.parquet(path)
    
    # Query nested fields
    df_read.select(
        col('id'),
        col('user.name').alias('user_name'),
        col('user.age').alias('user_age'),
        size(col('orders')).alias('order_count'),
        col('metadata.premium').alias('is_premium')
    ).show()
    
    print("üí° Nested Data Benefits:")
    print("   ‚Ä¢ Natural representation of complex objects")
    print("   ‚Ä¢ Preserves relationships without joins")
    print("   ‚Ä¢ Efficient storage (no duplication)")
    print("   ‚Ä¢ Column pruning works on nested fields")
    print()


def main():
    """Main execution function."""
    print("\n" + "üî• " * 40)
    print("ADVANCED PARQUET TECHNIQUES")
    print("üî• " * 40)
    print()
    
    spark = create_spark_session()
    
    try:
        example_1_row_group_optimization(spark)
        example_2_dictionary_encoding(spark)
        example_3_statistics_and_metadata(spark)
        example_4_coalesce_optimization(spark)
        example_5_performance_benchmark(spark)
        example_6_nested_data_structures(spark)
        
        print("=" * 80)
        print("‚úÖ ALL ADVANCED EXAMPLES COMPLETED")
        print("=" * 80)
        print()
        
        print("""
ÔøΩÔøΩ Advanced Parquet Optimization Summary:

1. Row Group Sizing:
   ‚Ä¢ Default: 128 MB
   ‚Ä¢ Tune based on query patterns and memory
   ‚Ä¢ Larger = better for scans, Smaller = better for selective queries

2. Dictionary Encoding:
   ‚Ä¢ Automatic for low-cardinality columns
   ‚Ä¢ Huge space savings (10x+ for categories)
   ‚Ä¢ Most effective for < 10,000 unique values

3. Statistics:
   ‚Ä¢ Automatically collected (min/max/null count)
   ‚Ä¢ Enable row group skipping
   ‚Ä¢ Critical for query performance

4. File Sizing:
   ‚Ä¢ Target: 128-512 MB per file
   ‚Ä¢ Use coalesce() to control file count
   ‚Ä¢ Balance: parallelism vs metadata overhead

5. Performance Tuning:
   ‚Ä¢ Enable vectorized reader
   ‚Ä¢ Use snappy compression (default)
   ‚Ä¢ Partition appropriately
   ‚Ä¢ Monitor and benchmark

6. Nested Data:
   ‚Ä¢ Efficiently stored in Parquet
   ‚Ä¢ Supports struct, array, map
   ‚Ä¢ Column pruning works on nested fields
        """)
    
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
