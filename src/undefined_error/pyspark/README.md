# PySpark Undefined Behavior & Pitfalls

Comprehensive collection of dangerous patterns, edge cases, and undefined behaviors in PySpark that can cause production failures.

## ğŸ¯ Purpose

This collection demonstrates **real-world PySpark pitfalls** that silently break production systems:
- Closure serialization failures
- Lazy evaluation gotchas
- Data skew causing OOM errors
- Type coercion data loss
- NULL handling bugs

Each example shows both the **dangerous pattern** (âŒ) and the **safe alternative** (âœ…).

## ğŸ“ Files Overview

| File | Lines | Topics Covered |
|------|-------|----------------|
| `01_closure_serialization.py` | 527 | Non-serializable objects, mutable state, instance methods, late binding |
| `02_lazy_evaluation.py` | 665 | Multiple recomputations, side effects, accumulator double-counting, random values |
| `03_data_skew_partitions.py` | 160 | Data skew, hot keys, partition imbalance, salting techniques |
| `04_type_coercion_null.py` | 210 | Implicit coercion, NULL propagation, division by zero, NaN vs NULL |

**Total: 1,562 lines of production-ready anti-patterns and solutions**

## ğŸ”¥ Top 10 Deadly PySpark Mistakes

### 1. **Non-Serializable Objects in Closures**
```python
# âŒ DANGER: File handles cannot be serialized
log_file = open('log.txt', 'w')

@udf(StringType())
def dangerous_udf(value):
    log_file.write(value)  # CRASH!
    return value

# âœ… SAFE: Create resources on executors
@udf(StringType())
def safe_udf(value):
    with open('log.txt', 'a') as f:  # Create inside UDF
        f.write(value)
    return value
```

### 2. **Mutable State Modifications**
```python
# âŒ DANGER: Counter modifications lost on executors
counter = [0]

@udf(IntegerType())
def increment(value):
    counter[0] += 1  # Lost! Executors have copies
    return counter[0]

# âœ… SAFE: Use Spark accumulators
counter = spark.sparkContext.accumulator(0)
```

### 3. **Multiple Recomputations Without Caching**
```python
# âŒ DANGER: Each action recomputes entire DAG
expensive_df = df.withColumn("expensive", expensive_computation())
expensive_df.count()  # Computation 1
expensive_df.sum()    # Computation 2 (full recompute!)

# âœ… SAFE: Cache before multiple actions
expensive_df.cache()
expensive_df.count()  # Computation 1
expensive_df.sum()    # Uses cache!
```

### 4. **Accumulator Double-Counting**
```python
# âŒ DANGER: Accumulator incremented multiple times
counter = spark.sparkContext.accumulator(0)

def increment(row):
    counter.add(1)
    return row

transformed = df.rdd.map(increment).toDF()
transformed.count()  # Counter = 100
transformed.count()  # Counter = 200 (WRONG!)

# âœ… SAFE: Cache to prevent recomputation
transformed.cache()
```

### 5. **Data Skew (Hot Keys)**
```python
# âŒ DANGER: 99% of data has same key
df.withColumn("key", lit("hot_key")).groupBy("key").count()
# One executor OOMs, others idle

# âœ… SAFE: Salting technique
df.withColumn("salt", (rand() * 10).cast("int")) \
  .withColumn("salted_key", concat(col("key"), col("salt"))) \
  .groupBy("salted_key").count()
```

### 6. **Type Coercion Data Loss**
```python
# âŒ DANGER: Invalid strings become NULL silently
df.withColumn("as_int", col("string_col").cast("int"))
# "123abc" â†’ NULL (no error!)

# âœ… SAFE: Validate before casting
df.withColumn("valid", col("string_col").rlike("^[0-9]+$")) \
  .withColumn("as_int", when(col("valid"), col("string_col").cast("int")))
```

### 7. **UDFs Not Handling NULLs**
```python
# âŒ DANGER: Crashes on NULL
@udf(IntegerType())
def multiply(value):
    return value * 2  # TypeError when value is None!

# âœ… SAFE: Explicit NULL check
@udf(IntegerType())
def multiply(value):
    if value is None:
        return None
    return value * 2
```

### 8. **Random Values Changing on Recomputation**
```python
# âŒ DANGER: Different results each time
df = df.withColumn("random", rand())
df.show()  # Random values X
df.show()  # Random values Y (DIFFERENT!)

# âœ… SAFE: Seed + cache
df = df.withColumn("random", rand(seed=42))
df.cache()
```

### 9. **Instance Methods in UDFs**
```python
# âŒ DANGER: Captures entire object (with locks!)
class Processor:
    def __init__(self):
        self.lock = threading.Lock()  # Non-serializable!
    
    def process(self, value):
        with self.lock:  # CRASH!
            return value * 2

processor = Processor()
udf_func = udf(processor.process)  # Tries to serialize lock!

# âœ… SAFE: Static methods
class Processor:
    @staticmethod
    def process(value):
        return value * 2  # No self capture
```

### 10. **Python Late Binding in Loops**
```python
# âŒ DANGER: All UDFs use final loop value
udfs = [udf(lambda x: x * i) for i in range(5)]
# All UDFs multiply by 4! (final i value)

# âœ… SAFE: Default argument for early binding
udfs = [udf(lambda x, i=i: x * i) for i in range(5)]
```

## ğŸƒ Running Examples

### Run Individual Files
```bash
# Closure serialization pitfalls
python3 01_closure_serialization.py

# Lazy evaluation issues
python3 02_lazy_evaluation.py

# Data skew problems
python3 03_data_skew_partitions.py

# Type coercion dangers
python3 04_type_coercion_null.py
```

### Run All Examples
```bash
# Execute all tests
./run_all.sh

# Or manually
for file in 0*.py; do python3 "$file"; done
```

## ğŸ“Š Pattern Categories

### Serialization Issues (File 01)
- Non-serializable objects (files, locks, sockets)
- Mutable state in closures
- Instance methods capturing self
- Global variable modifications
- Late binding in loops
- Broadcast variable misuse

### Lazy Evaluation (File 02)
- Multiple recomputations without cache
- Transformations without actions
- Side effects in transformations
- Accumulator double-counting
- Random/time-dependent operations
- Execution order assumptions
- Checkpoint vs persist confusion

### Data Distribution (File 03)
- Severe data skew (hot keys)
- Single partition bottlenecks
- Too many/too few partitions
- Unbalanced joins
- Salting techniques
- Partition sizing

### Type Safety (File 04)
- Implicit type coercion
- NULL propagation in UDFs
- NULL vs None confusion
- Division by zero
- NaN vs NULL differences
- String to number conversion

## ğŸ“ Learning Approach

Each file follows this structure:
1. **Dangerous Pattern** (âŒ) - Shows the bug
2. **Problem Explanation** - Why it fails
3. **Expected Result** - What goes wrong
4. **Safe Alternative** (âœ…) - Correct approach
5. **Key Takeaways** - Summary of lessons

## âš ï¸ Warning Signs

Watch for these red flags in production code:

```python
# ğŸš¨ RED FLAGS ğŸš¨
open('file.txt')              # File handles in closures
threading.Lock()              # Locks in UDF classes
counter[0] += 1               # Mutable state modification
processor.method              # Instance methods as UDFs
df.count(); df.count()        # Multiple actions without cache
.coalesce(1)                  # Single partition bottleneck
.cast("int")                  # Type coercion without validation
value * 2                     # UDF without NULL check
rand()                        # Random without seed + cache
lambda x: x * i               # Late binding in loops
```

## ğŸ“ˆ Performance Impact

| Pattern | Performance Impact | Severity |
|---------|-------------------|----------|
| No caching + multiple actions | 2-10x slower | ğŸ”´ Critical |
| Data skew | OOM crashes | ğŸ”´ Critical |
| Single partition | No parallelism | ğŸ”´ Critical |
| Too many partitions | 50-200% overhead | ğŸŸ  High |
| Regular UDF vs Pandas UDF | 10-100x slower | ğŸŸ  High |
| Accumulator double-count | Wrong results | ğŸ”´ Critical |
| Type coercion data loss | Silent corruption | ğŸ”´ Critical |

## ğŸ› ï¸ Defensive Patterns

### Always Do This
```python
# 1. Cache expensive repeated computations
expensive_df.cache()

# 2. Use static methods, not instance methods
@staticmethod
def process(value):
    ...

# 3. Handle NULLs explicitly in UDFs
if value is None:
    return None

# 4. Use Spark accumulators, not global variables
counter = spark.sparkContext.accumulator(0)

# 5. Use salting for hot keys
.withColumn("salt", (rand() * 10).cast("int"))

# 6. Validate before type casting
.withColumn("valid", col("str").rlike("^[0-9]+$"))

# 7. Use seed for reproducible random data
rand(seed=42)

# 8. Check partition counts
df.rdd.getNumPartitions()

# 9. Monitor data skew
df.groupBy("key").count().describe()

# 10. Use Pandas UDFs for vectorized operations
@pandas_udf(IntegerType())
def vectorized_udf(s: pd.Series) -> pd.Series:
    return s * 2
```

## ğŸ”— Related Resources

- [Spark Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [PySpark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)

## ğŸ¤ Contributing

Found another undefined behavior? Add it to the collection:
1. Create new file: `05_your_topic.py`
2. Follow the pattern: dangerous (âŒ) â†’ safe (âœ…)
3. Include 5-10 examples per file
4. Add comprehensive docstrings

## ğŸ“ License

Educational use. These are ANTI-PATTERNS - do NOT use dangerous patterns in production!
