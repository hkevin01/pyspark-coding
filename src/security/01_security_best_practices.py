#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
PYSPARK SECURITY BEST PRACTICES - Complete Guide
================================================================================

MODULE OVERVIEW:
----------------
This module provides comprehensive security best practices for Apache Spark
applications. Security is CRITICAL in production environments where you're
processing sensitive data, connecting to secure clusters, and deploying
applications that handle authentication and authorization.

Common security vulnerabilities in Spark applications include:
â€¢ Hardcoded credentials and API keys
â€¢ Unencrypted data transmission
â€¢ Missing authentication/authorization
â€¢ Insecure configuration settings
â€¢ Vulnerable dependencies
â€¢ Exposed Spark UI with sensitive data
â€¢ Insufficient access controls
â€¢ Logging sensitive information

PURPOSE:
--------
Learn how to:
1. Secure credentials and secrets management
2. Enable encryption (at-rest and in-transit)
3. Implement authentication and authorization
4. Configure secure Spark clusters
5. Protect sensitive data in logs and UI
6. Handle compliance requirements (GDPR, HIPAA, etc.)
7. Audit and monitor security events
8. Follow secure coding practices

TARGET AUDIENCE:
----------------
â€¢ Data engineers deploying production Spark applications
â€¢ Security engineers implementing data security policies
â€¢ DevOps teams managing Spark clusters
â€¢ Compliance officers ensuring regulatory compliance
â€¢ Anyone handling sensitive or PII data with Spark

================================================================================
SECURITY THREAT MODEL:
================================================================================

COMMON ATTACK VECTORS IN SPARK:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPARK SECURITY PERIMETER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. NETWORK LAYER                                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Client â†’ Driver â†’ Executors            â”‚                 â”‚
â”‚     â”‚ âš ï¸  Unencrypted traffic vulnerable      â”‚                 â”‚
â”‚     â”‚ âœ… Solution: Enable SSL/TLS             â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â”‚  2. AUTHENTICATION                                              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Who can submit Spark jobs?             â”‚                 â”‚
â”‚     â”‚ âš ï¸  No auth = anyone can run code       â”‚                 â”‚
â”‚     â”‚ âœ… Solution: Kerberos, OAuth, LDAP      â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â”‚  3. AUTHORIZATION                                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ What data can users access?            â”‚                 â”‚
â”‚     â”‚ âš ï¸  No ACLs = access to all data        â”‚                 â”‚
â”‚     â”‚ âœ… Solution: Ranger, ACLs, Row-level    â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â”‚  4. DATA ENCRYPTION                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ In-Transit: Network encryption         â”‚                 â”‚
â”‚     â”‚ At-Rest: Storage encryption            â”‚                 â”‚
â”‚     â”‚ âš ï¸  Plain text = data exposure          â”‚                 â”‚
â”‚     â”‚ âœ… Solution: AES-256, TLS 1.2+          â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â”‚  5. SECRETS MANAGEMENT                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Credentials, API keys, passwords       â”‚                 â”‚
â”‚     â”‚ âš ï¸  Hardcoded = Git history exposure    â”‚                 â”‚
â”‚     â”‚ âœ… Solution: Vault, KMS, Secrets Mgr    â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â”‚  6. SPARK UI EXPOSURE                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Shows queries, data samples, configs   â”‚                 â”‚
â”‚     â”‚ âš ï¸  Public UI = sensitive data leak     â”‚                 â”‚
â”‚     â”‚ âœ… Solution: Authentication, filtering  â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SECURITY LAYERS (Defense in Depth):
Layer 1: Network Security (firewalls, VPCs, security groups)
Layer 2: Authentication (prove who you are)
Layer 3: Authorization (prove what you can do)
Layer 4: Encryption (protect data confidentiality)
Layer 5: Auditing (track what happened)
Layer 6: Monitoring (detect anomalies)

================================================================================
BEST PRACTICE #1: SECURE SECRETS MANAGEMENT
================================================================================

âŒ WRONG - Hardcoded Credentials:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NEVER DO THIS!
spark = SparkSession.builder \\
    .config("spark.hadoop.fs.s3a.access.key", "AKIAIOSFODNN7EXAMPLE") \\
    .config("spark.hadoop.fs.s3a.secret.key", "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY") \\
    .getOrCreate()

# Database password in code
jdbc_url = "jdbc:postgresql://db.example.com:5432/mydb?user=admin&password=Secret123!"

WHY THIS IS DANGEROUS:
â€¢ Credentials committed to Git (visible in history forever!)
â€¢ Visible in Spark UI and logs
â€¢ Shared across all users of the code
â€¢ No credential rotation
â€¢ Security audit failures

âœ… CORRECT - Use Secrets Managers:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OPTION 1: Environment Variables (basic)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os

# Load from environment
aws_access_key = os.environ.get("AWS_ACCESS_KEY_ID")
aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY")

spark = SparkSession.builder \\
    .config("spark.hadoop.fs.s3a.access.key", aws_access_key) \\
    .config("spark.hadoop.fs.s3a.secret.key", aws_secret_key) \\
    .getOrCreate()

# Set environment variables in job submission:
# export AWS_ACCESS_KEY_ID=xxx
# export AWS_SECRET_ACCESS_KEY=yyy
# spark-submit app.py

OPTION 2: AWS Secrets Manager (production)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import boto3
import json

def get_secret(secret_name, region_name="us-east-1"):
    \"\"\"Retrieve secret from AWS Secrets Manager.\"\"\"
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )
    
    response = client.get_secret_value(SecretId=secret_name)
    return json.loads(response['SecretString'])

# Retrieve database credentials
db_creds = get_secret("prod/database/postgres")
jdbc_url = f"jdbc:postgresql://db.example.com:5432/mydb"

df = spark.read \\
    .format("jdbc") \\
    .option("url", jdbc_url) \\
    .option("dbtable", "users") \\
    .option("user", db_creds["username"]) \\
    .option("password", db_creds["password"]) \\
    .load()

OPTION 3: HashiCorp Vault
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import hvac

# Connect to Vault
client = hvac.Client(url='https://vault.example.com:8200')
client.auth.approle.login(
    role_id=os.environ['VAULT_ROLE_ID'],
    secret_id=os.environ['VAULT_SECRET_ID']
)

# Read secrets
secrets = client.secrets.kv.v2.read_secret_version(
    path='spark/prod/database'
)

db_password = secrets['data']['data']['password']

OPTION 4: Databricks Secrets
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Store secret: databricks secrets create-scope --scope prod
# Add secret: databricks secrets put --scope prod --key db_password

from pyspark.dbutils import DBUtils
dbutils = DBUtils(spark)

# Access secret (never appears in logs/UI)
db_password = dbutils.secrets.get(scope="prod", key="db_password")

jdbc_url = f"jdbc:postgresql://db.example.com:5432/mydb"
df = spark.read \\
    .format("jdbc") \\
    .option("url", jdbc_url) \\
    .option("user", "admin") \\
    .option("password", db_password) \\
    .load()

BEST PRACTICES:
âœ… Never commit secrets to Git
âœ… Use secrets managers (Vault, AWS Secrets Manager, Azure Key Vault)
âœ… Rotate credentials regularly
âœ… Use IAM roles instead of access keys when possible
âœ… Limit secret access with least privilege
âœ… Audit secret access

================================================================================
BEST PRACTICE #2: ENABLE ENCRYPTION
================================================================================

ENCRYPTION AT REST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Enable HDFS encryption zones (cluster admin)
hdfs crypto -createZone -keyName myKey -path /encrypted/data

# Enable S3 server-side encryption
spark.conf.set("spark.hadoop.fs.s3a.server-side-encryption-algorithm", "AES256")

# Enable disk encryption for shuffle and cache
spark = SparkSession.builder \\
    .config("spark.io.encryption.enabled", "true") \\
    .config("spark.io.encryption.keySizeBits", "256") \\
    .config("spark.io.encryption.keygen.algorithm", "HmacSHA256") \\
    .getOrCreate()

ENCRYPTION IN TRANSIT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Enable SSL/TLS for Spark internal communication
spark = SparkSession.builder \\
    .config("spark.ssl.enabled", "true") \\
    .config("spark.ssl.protocol", "TLSv1.2") \\
    .config("spark.ssl.keyStore", "/path/to/keystore.jks") \\
    .config("spark.ssl.keyStorePassword", keystore_password) \\
    .config("spark.ssl.trustStore", "/path/to/truststore.jks") \\
    .config("spark.ssl.trustStorePassword", truststore_password) \\
    .getOrCreate()

# Enable SSL for Spark UI
spark.conf.set("spark.ui.https.enabled", "true")
spark.conf.set("spark.ui.https.keyStore", "/path/to/keystore.jks")
spark.conf.set("spark.ui.https.keyStorePassword", keystore_password)

# JDBC over SSL
jdbc_url = "jdbc:postgresql://db.example.com:5432/mydb?ssl=true&sslmode=require"

ENCRYPTION CHECKLIST:
âœ… Enable encryption for shuffle files
âœ… Enable encryption for RDD cache
âœ… Use SSL/TLS for all network communication
âœ… Encrypt data at rest (HDFS, S3, databases)
âœ… Use TLS 1.2 or higher (not SSL 3.0 or TLS 1.0)
âœ… Regularly rotate encryption keys

================================================================================
BEST PRACTICE #3: AUTHENTICATION & AUTHORIZATION
================================================================================

KERBEROS AUTHENTICATION (Hadoop clusters):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Kerberos configuration
spark = SparkSession.builder \\
    .config("spark.security.credentials.hbase.enabled", "true") \\
    .config("spark.security.credentials.hive.enabled", "true") \\
    .config("spark.yarn.principal", "user@REALM.COM") \\
    .config("spark.yarn.keytab", "/path/to/user.keytab") \\
    .getOrCreate()

# Submit with Kerberos
# kinit -kt /path/to/user.keytab user@REALM.COM
# spark-submit --principal user@REALM.COM --keytab /path/to/user.keytab app.py

SPARK UI AUTHENTICATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Enable Spark UI authentication
spark.conf.set("spark.ui.filters", "org.apache.spark.ui.AclsFilter")
spark.conf.set("spark.acls.enable", "true")
spark.conf.set("spark.admin.acls", "admin_user")
spark.conf.set("spark.ui.view.acls", "user1,user2")

# Enable HTTP authentication
spark.conf.set("spark.authenticate", "true")
spark.conf.set("spark.authenticate.secret", shared_secret)

AUTHORIZATION (Apache Ranger):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Ranger provides fine-grained access control
# - Table-level access
# - Column-level access
# - Row-level filtering
# - Data masking

# Example: Read only allowed columns
df = spark.read.table("sensitive_data")  
# Ranger automatically filters columns based on user permissions
# User sees: id, name (allowed)
# User doesn't see: ssn, credit_card (denied)

LEAST PRIVILEGE PRINCIPLE:
âœ… Users only get minimum required permissions
âœ… Service accounts with specific roles
âœ… Regular permission audits
âœ… Remove unused accounts

================================================================================
BEST PRACTICE #4: SECURE SPARK UI
================================================================================

PROBLEM: Spark UI exposes sensitive information
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The Spark UI (port 4040) shows:
â€¢ SQL queries (may contain sensitive predicates)
â€¢ Job configurations (may include credentials)
â€¢ Environment variables
â€¢ Data samples
â€¢ File paths
â€¢ Executor details

SOLUTION 1: Enable Authentication
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark = SparkSession.builder \\
    .config("spark.ui.filters", "org.apache.spark.ui.AclsFilter") \\
    .config("spark.acls.enable", "true") \\
    .config("spark.admin.acls", "admin_user") \\
    .config("spark.ui.view.acls", "user1,user2") \\
    .config("spark.modify.acls", "admin_user") \\
    .getOrCreate()

SOLUTION 2: Redact Sensitive Data
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Redact sensitive values in UI
spark.conf.set("spark.redaction.regex", "(?i)(password|secret|token|key)")

# Example: This appears as ********** in UI
spark.conf.set("spark.my.api.key", "AKIAIOSFODNN7EXAMPLE")

SOLUTION 3: Disable Data Preview
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Disable showing data samples in SQL tab
spark.conf.set("spark.sql.ui.explainMode", "simple")

SOLUTION 4: Use Reverse Proxy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Put Spark UI behind authenticated reverse proxy (nginx, Apache)
# proxy_pass http://spark-ui:4040
# require valid-user

SOLUTION 5: Restrict Network Access
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Bind to localhost only (not 0.0.0.0)
spark.conf.set("spark.ui.host", "localhost")

# Use firewall rules to restrict access
# iptables -A INPUT -p tcp --dport 4040 -s trusted_ip -j ACCEPT

================================================================================
BEST PRACTICE #5: SECURE LOGGING
================================================================================

âŒ WRONG - Logging Sensitive Data:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# NEVER DO THIS!
logger.info(f"Connecting with password: {password}")
logger.debug(f"API key: {api_key}")
df.show()  # May display PII

âœ… CORRECT - Secure Logging:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Mask sensitive values
def mask_sensitive(text):
    \"\"\"Mask sensitive data in logs.\"\"\"
    import re
    # Mask credit cards
    text = re.sub(r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', 
                  'XXXX-XXXX-XXXX-XXXX', text)
    # Mask SSN
    text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', 'XXX-XX-XXXX', text)
    # Mask emails
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', 
                  '***@***.***', text)
    return text

logger.info(mask_sensitive(f"Processing user data: {data}"))

# Set log level to avoid verbose output
spark.sparkContext.setLogLevel("WARN")

# Redact in Spark logs
spark.conf.set("spark.redaction.regex", 
               "(?i)(password|pwd|secret|token|key|credit|ssn)")

# Don't log full DataFrames with sensitive data
# df.show()  # BAD
logger.info(f"Processed {df.count()} records")  # GOOD

================================================================================
BEST PRACTICE #6: INPUT VALIDATION & SQL INJECTION PREVENTION
================================================================================

âŒ WRONG - SQL Injection Vulnerable:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# String interpolation = SQL injection risk!
user_input = request.get("user_id")  # From web request
query = f"SELECT * FROM users WHERE user_id = {user_input}"
df = spark.sql(query)

# Attacker sends: user_id = "1 OR 1=1"
# Resulting query: SELECT * FROM users WHERE user_id = 1 OR 1=1
# Result: Returns ALL users!

âœ… CORRECT - Parameterized Queries:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Use DataFrame API (safe by default)
user_input = request.get("user_id")
df = spark.table("users").filter(col("user_id") == user_input)

# Or use parameterized SQL (Spark 3.4+)
df = spark.sql(
    "SELECT * FROM users WHERE user_id = :user_id",
    args={"user_id": user_input}
)

# Validate and sanitize inputs
def validate_user_id(user_id):
    \"\"\"Validate user ID format.\"\"\"
    if not user_id.isdigit():
        raise ValueError("Invalid user_id format")
    return int(user_id)

safe_user_id = validate_user_id(user_input)
df = spark.table("users").filter(col("user_id") == safe_user_id)

INPUT VALIDATION CHECKLIST:
âœ… Validate all external inputs
âœ… Use DataFrame API instead of raw SQL strings
âœ… Never concatenate user input into SQL
âœ… Whitelist allowed values when possible
âœ… Use type checking and bounds checking

================================================================================
BEST PRACTICE #7: SECURE DEPENDENCIES
================================================================================

VULNERABILITY SCANNING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Scan Python dependencies for vulnerabilities
pip install safety
safety check

# Scan requirements.txt
safety check -r requirements.txt

# Example output:
# â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••
# â”‚ VULNERABILITY FOUND: pyyaml < 5.4                    â”‚
# â”‚ Severity: HIGH                                       â”‚
# â”‚ CVE-2020-14343: Arbitrary code execution             â”‚
# â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›

DEPENDENCY MANAGEMENT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# requirements.txt - Pin versions!
pyspark==3.5.0  # Not: pyspark>=3.0.0 (unpredictable)
pandas==2.1.0
numpy==1.24.0

# Use virtual environments
python -m venv spark_env
source spark_env/bin/activate
pip install -r requirements.txt

# Regularly update dependencies
pip list --outdated
pip install --upgrade pyspark

SECURE PACKAGE SOURCES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Use trusted package repositories
pip install --index-url https://pypi.org/simple pyspark

# Verify package signatures (when available)
pip install --require-hashes -r requirements-hashes.txt

================================================================================
BEST PRACTICE #8: COMPLIANCE (GDPR, HIPAA, SOC 2)
================================================================================

GDPR REQUIREMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1. Data Minimization - Only collect necessary data
df_minimal = df.select("user_id", "order_date", "amount")  # Not all columns

# 2. Right to Erasure - Delete user data on request
user_id_to_delete = "12345"
df_anonymized = df.filter(col("user_id") != user_id_to_delete)

# 3. Data Anonymization
from pyspark.sql.functions import sha2, concat_ws

df_anonymized = df.withColumn(
    "user_id_hash",
    sha2(concat_ws("_", col("user_id"), lit("salt")), 256)
).drop("user_id")

# 4. Audit Trail - Log all data access
logger.info(f"User {current_user} accessed table users at {timestamp}")

HIPAA REQUIREMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1. Encryption (covered above)
# 2. Access Controls (covered above)
# 3. Audit Logging

def log_phi_access(user, table, action):
    \"\"\"Log PHI (Protected Health Information) access.\"\"\"
    audit_log = {
        "timestamp": datetime.now().isoformat(),
        "user": user,
        "table": table,
        "action": action,
        "ip_address": get_client_ip()
    }
    logger.info(f"PHI_ACCESS: {json.dumps(audit_log)}")

log_phi_access("doctor_smith", "patient_records", "READ")

# 4. De-identification
df_deidentified = df.drop("name", "ssn", "address", "phone")

================================================================================
COMMON SECURITY MISTAKES & FIXES:
================================================================================

MISTAKE #1: Exposed Cloud Storage Buckets
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Public S3 bucket: s3a://my-public-bucket/data
âœ… Private bucket with IAM roles

MISTAKE #2: Overly Permissive IAM Policies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Action: "s3:*", Resource: "*"
âœ… Action: ["s3:GetObject"], Resource: "arn:aws:s3:::my-bucket/data/*"

MISTAKE #3: Weak Passwords
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ password = "admin123"
âœ… password = generate_secure_password(length=32)

MISTAKE #4: Missing TLS Certificate Validation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ requests.get(url, verify=False)
âœ… requests.get(url, verify=True)

MISTAKE #5: Logging Stack Traces with Secrets
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ logger.exception(f"Error: {e}")  # May expose secrets
âœ… logger.error("Database connection failed")  # Generic message

MISTAKE #6: Unpatched Systems
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Running Spark 2.4.0 (EOL, known vulnerabilities)
âœ… Running Spark 3.5.0 (latest stable with security patches)

MISTAKE #7: Default Credentials
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Username: admin, Password: admin
âœ… Force password change on first login

================================================================================
SECURITY CHECKLIST:
================================================================================

BEFORE DEPLOYMENT:
â˜ No hardcoded credentials in code
â˜ Secrets stored in secrets manager
â˜ Encryption enabled (at-rest and in-transit)
â˜ Authentication configured
â˜ Authorization rules implemented
â˜ Spark UI authentication enabled
â˜ Sensitive data redacted in logs/UI
â˜ Input validation implemented
â˜ Dependencies scanned for vulnerabilities
â˜ Compliance requirements met (GDPR, HIPAA, etc.)
â˜ Security testing completed
â˜ Incident response plan documented

MONITORING:
â˜ Log all authentication attempts
â˜ Alert on failed authentications
â˜ Monitor for unusual data access patterns
â˜ Track configuration changes
â˜ Review audit logs regularly

MAINTENANCE:
â˜ Regular security patches
â˜ Credential rotation schedule
â˜ Access permission reviews
â˜ Security training for team
â˜ Vulnerability scanning (weekly/monthly)

================================================================================
RELATED RESOURCES:
================================================================================

Spark Security Documentation:
  https://spark.apache.org/docs/latest/security.html

OWASP Top 10:
  https://owasp.org/www-project-top-ten/

CIS Benchmarks:
  https://www.cisecurity.org/benchmark/apache_spark

AWS Security Best Practices:
  https://docs.aws.amazon.com/whitepapers/latest/aws-security-best-practices/

Databricks Security:
  https://docs.databricks.com/security/index.html

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 1.0.0 - Security Best Practices Guide
CREATED: 2024
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, sha2, concat_ws
import os
import logging

# Configure secure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_secure_spark_session():
    """
    Create a SparkSession with security best practices enabled.
    
    This demonstrates proper security configuration.
    """
    print("=" * 80)
    print("CREATING SECURE SPARK SESSION")
    print("=" * 80)
    
    # Load credentials from environment (not hardcoded!)
    aws_access_key = os.environ.get("AWS_ACCESS_KEY_ID", "")
    aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")
    
    spark = SparkSession.builder \
        .appName("SecureSparkApp") \
        .config("spark.io.encryption.enabled", "true") \
        .config("spark.io.encryption.keySizeBits", "256") \
        .config("spark.authenticate", "true") \
        .config("spark.redaction.regex", "(?i)(password|secret|token|key)") \
        .config("spark.hadoop.fs.s3a.access.key", aws_access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", aws_secret_key) \
        .getOrCreate()
    
    # Set appropriate log level
    spark.sparkContext.setLogLevel("WARN")
    
    print("âœ… Secure Spark session created")
    print(f"   Encryption enabled: {spark.conf.get('spark.io.encryption.enabled')}")
    print(f"   Redaction enabled: {spark.conf.get('spark.redaction.regex')}")
    
    return spark


def demonstrate_data_anonymization():
    """Demonstrate GDPR-compliant data anonymization."""
    print("\n" + "=" * 80)
    print("DATA ANONYMIZATION (GDPR Compliance)")
    print("=" * 80)
    
    spark = create_secure_spark_session()
    
    # Sample sensitive data
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    
    schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
        StructField("age", IntegerType(), True)
    ])
    
    data = [
        ("U001", "John Doe", "john@example.com", 30),
        ("U002", "Jane Smith", "jane@example.com", 25),
        ("U003", "Bob Johnson", "bob@example.com", 35)
    ]
    
    df = spark.createDataFrame(data, schema)
    
    print("\nğŸ”’ Original Data (SENSITIVE):")
    df.show(truncate=False)
    
    # Anonymize with hash
    df_anonymized = df \
        .withColumn("user_id_hash", sha2(col("user_id"), 256)) \
        .withColumn("email_hash", sha2(col("email"), 256)) \
        .drop("user_id", "name", "email")
    
    print("\nâœ… Anonymized Data (SAFE):")
    df_anonymized.show(truncate=False)
    
    spark.stop()


def main():
    """Run security demonstrations."""
    print("\n" + "ğŸ”’" * 40)
    print("PYSPARK SECURITY BEST PRACTICES")
    print("ğŸ”’" * 40)
    
    create_secure_spark_session()
    demonstrate_data_anonymization()
    
    print("\n" + "=" * 80)
    print("âœ… SECURITY GUIDE COMPLETE")
    print("=" * 80)
    
    print("\nğŸ” Key Takeaways:")
    print("   1. Never hardcode credentials")
    print("   2. Enable encryption everywhere")
    print("   3. Implement authentication & authorization")
    print("   4. Secure the Spark UI")
    print("   5. Redact sensitive data in logs")
    print("   6. Validate all inputs")
    print("   7. Keep dependencies updated")
    print("   8. Comply with regulations (GDPR, HIPAA)")


if __name__ == "__main__":
    main()
