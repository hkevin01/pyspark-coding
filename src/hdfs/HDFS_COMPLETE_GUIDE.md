# HDFS Complete Guide
## Comprehensive Reference for Hadoop Distributed File System

**Table of Contents**
1. [HDFS Components and Metadata](#hdfs-components-and-metadata)
2. [HDFS Blocks and Replication](#hdfs-blocks-and-replication)
3. [Rack Awareness](#rack-awareness)
4. [HDFS Read Mechanism](#hdfs-read-mechanism)
5. [HDFS Write Mechanism](#hdfs-write-mechanism)
6. [HDFS CLI Commands](#hdfs-cli-commands)
7. [File Permissions](#file-permissions)
8. [Best Practices](#best-practices)

---

## HDFS Components and Metadata

### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HDFS Cluster Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              NameNode (Master)                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  FSImage (Filesystem Metadata on Disk)           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Complete namespace snapshot                    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Block to file mapping                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - File permissions, owner, timestamps            â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  EditLog (Transaction Log)                       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Recent filesystem changes                      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Create, delete, rename operations             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Appended in real-time                          â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  In-Memory Metadata                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  /user/data/file.txt â†’ [Block1, Block2, Block3] â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Block1 â†’ [DN1, DN3, DN5] (locations)           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Block2 â†’ [DN2, DN4, DN6]                        â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â”‚ Heartbeat (3s) + Block Reports (1h) â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    DataNodes (Workers)                   â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ DataNode1â”‚    â”‚ DataNode2â”‚    â”‚ DataNode3â”‚    ...   â”‚   â”‚
â”‚  â”‚  â”‚ Rack 1   â”‚    â”‚ Rack 1   â”‚    â”‚ Rack 2   â”‚          â”‚   â”‚
â”‚  â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚          â”‚   â”‚
â”‚  â”‚  â”‚ [Block1] â”‚    â”‚ [Block2] â”‚    â”‚ [Block1] â”‚  Stores  â”‚   â”‚
â”‚  â”‚  â”‚ [Block3] â”‚    â”‚ [Block4] â”‚    â”‚ [Block2] â”‚  actual  â”‚   â”‚
â”‚  â”‚  â”‚ [Block5] â”‚    â”‚ [Block6] â”‚    â”‚ [Block4] â”‚  data    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Secondary NameNode (Checkpoint Helper)                â”‚   â”‚
â”‚  â”‚   - Merges FSImage + EditLog                            â”‚   â”‚
â”‚  â”‚   - Reduces NameNode restart time                        â”‚   â”‚
â”‚  â”‚   - NOT a backup! (common misconception)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. NameNode (Master)

**Responsibilities:**
- Manages filesystem namespace (directory tree)
- Stores metadata in memory for fast access
- Tracks which blocks belong to which files
- Knows locations of all block replicas
- Handles client requests for file operations

**Metadata Storage:**

```
FSImage (Persistent on Disk):
â”œâ”€ Filesystem namespace
â”œâ”€ File â†’ Block mapping
â”œâ”€ File attributes (owner, permissions, timestamps)
â””â”€ Checkpoint of metadata at specific time

EditLog (Transaction Log):
â”œâ”€ All changes since last FSImage
â”œâ”€ Create file: /user/data.txt
â”œâ”€ Delete file: /tmp/old.dat
â”œâ”€ Rename: /old â†’ /new
â””â”€ Append-only for durability

In-Memory (Fast Access):
/user/data/file.txt:
  - Size: 384 MB
  - Blocks: [blk_1001, blk_1002, blk_1003]
  - blk_1001 â†’ [DataNode1, DataNode3, DataNode5]
  - blk_1002 â†’ [DataNode2, DataNode4, DataNode6]
  - blk_1003 â†’ [DataNode1, DataNode2, DataNode4]
```

**Memory Requirements:**
```
Rule of thumb: ~150 bytes per block

Example:
1 million files Ã— 3 blocks avg = 3 million blocks
3M blocks Ã— 150 bytes = 450 MB RAM

1 billion files = 450 GB RAM needed!
This is why small files are a problem.
```

### 2. DataNodes (Workers)

**Responsibilities:**
- Store actual data blocks on local disks
- Serve read/write requests from clients
- Send heartbeats to NameNode (every 3 seconds)
- Report block inventory (every hour)
- Execute block operations (create, delete, replicate)

**Heartbeat Protocol:**
```
Every 3 seconds:
DataNode â†’ NameNode: "I'm alive! My disk usage: 45%"

If NameNode doesn't receive heartbeat for 10 minutes:
â†’ DataNode marked as dead
â†’ Blocks on dead node need re-replication
â†’ NameNode instructs other DataNodes to replicate
```

**Block Report:**
```
Every hour:
DataNode â†’ NameNode: "I have these blocks:"
  - blk_1001: 128 MB, checksum: 0x4f3a2b1c
  - blk_1002: 128 MB, checksum: 0x9e8d7c6b
  - blk_1003: 64 MB, checksum: 0x2a1b0c9d

NameNode verifies:
âœ… All blocks accounted for?
âœ… Replication factor met?
âŒ Missing replicas? â†’ Trigger replication
âŒ Excess replicas? â†’ Delete extras
```

### 3. Secondary NameNode

**NOT A BACKUP!** (Common Misconception)

**Actual Purpose:** Checkpoint Helper

```
Process:
1. Download FSImage + EditLog from NameNode
2. Merge them into new FSImage
3. Upload new FSImage back to NameNode
4. NameNode switches to new FSImage, empties EditLog

Why needed?
- EditLog grows indefinitely without checkpointing
- Large EditLog â†’ slow NameNode restart
- Checkpointing reduces restart time

Checkpoint Frequency:
- Every hour (default)
- Or when EditLog reaches 1M transactions
```

**Checkpoint Process:**
```
Time: 00:00 - FSImage.001 + EditLog (1M entries)
Time: 01:00 - Secondary NN triggers checkpoint
  â†’ Download FSImage.001 + EditLog
  â†’ Merge locally
  â†’ Create FSImage.002
  â†’ Upload to NameNode
  â†’ NameNode renames FSImage.002 â†’ FSImage.current
  â†’ New empty EditLog starts
Time: 02:00 - Repeat...
```

---

## HDFS Blocks and Replication

### Block Fundamentals

```
File: 640 MB
Block Size: 128 MB
Number of Blocks: 5

File Split:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Original File (640 MB)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ Split into blocks
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Block 1  â”‚ â”‚ Block 2  â”‚ â”‚ Block 3  â”‚ â”‚ Block 4  â”‚ â”‚ Block 5  â”‚
â”‚ 128 MB   â”‚ â”‚ 128 MB   â”‚ â”‚ 128 MB   â”‚ â”‚ 128 MB   â”‚ â”‚ 128 MB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why 128 MB Blocks?

**Optimization Trade-off:**

```
Small Blocks (e.g., 4 KB like Linux):
âœ… Less wasted space for small files
âŒ More metadata (1M blocks = 150 MB RAM)
âŒ More network requests
âŒ More disk seeks

Large Blocks (e.g., 128 MB):
âœ… Less metadata (fewer blocks)
âœ… Sequential I/O (fast!)
âœ… Fewer network requests
âŒ Wasted space for small files

Calculation for 1 TB file:
4 KB blocks:  262 million blocks (39 GB metadata!)
128 MB blocks: 8,000 blocks (1.2 MB metadata) âœ…
```

**Seek Time vs Transfer Time:**
```
Disk specs:
- Seek time: 10 ms
- Transfer rate: 100 MB/s

For 4 KB block:
- Seek: 10 ms
- Transfer: 0.04 ms
- Total: 10.04 ms (99% seeking!)

For 128 MB block:
- Seek: 10 ms  
- Transfer: 1280 ms
- Total: 1290 ms (99% transferring!) âœ…

Large blocks = efficient sequential I/O
```

### Replication Strategy

**Default Replication Factor: 3**

```
Original Block â†’ 3 Replicas

Primary:   DataNode1 (same rack as client)
Replica 1: DataNode3 (same rack, different node)
Replica 2: DataNode5 (different rack)

Why 3 replicas?
- Tolerates 2 node failures
- Good balance: reliability vs storage cost
- Can read from any of 3 locations (load balancing)
```

**Replica Placement Strategy:**

```
Goal: Maximize reliability AND data locality

Placement Rules:
1st replica: Same rack as client (data locality)
2nd replica: Same rack, different node (local redundancy)
3rd replica: Different rack (rack failure tolerance)

Example:
Client on Rack 1, Node 2

Replica Placement:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Rack 1         â”‚  â”‚      Rack 2         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”‚
â”‚  â”‚ N1 â”‚   â”‚ N2 â”‚   â”‚  â”‚  â”‚ N5 â”‚   â”‚ N6 â”‚   â”‚
â”‚  â”‚    â”‚   â”‚ 1stâ”‚   â”‚  â”‚  â”‚ 3rdâ”‚   â”‚    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”‚  â”‚                     â”‚
â”‚  â”‚ N3 â”‚   â”‚ N4 â”‚   â”‚  â”‚                     â”‚
â”‚  â”‚ 2ndâ”‚   â”‚    â”‚   â”‚  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â”‚  â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Fast write: 2/3 replicas local (same rack)
âœ… Rack failure: 1 replica survives
âœ… Read locality: Can read from local rack
```

### Block States

```
State Machine:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CREATED    â”‚  Block allocated, not yet written
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Write pipeline completes
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UNDER_CONST  â”‚  Being written, not finalized
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Close file
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMITTED   â”‚  Write complete, not yet reported
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Block report received
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   COMPLETE   â”‚  Fully replicated, available for reads
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Error States:
CORRUPT     â†’ Checksum mismatch
MISSING     â†’ Expected replica not found
EXCESS      â†’ Too many replicas
```

---

## Rack Awareness

### Data Center Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Center                            â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Rack 1      â”‚  â”‚     Rack 2      â”‚  â”‚   Rack 3    â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”      â”‚ â”‚
â”‚  â”‚  â”‚DN1â”‚  â”‚DN2â”‚   â”‚  â”‚  â”‚DN5â”‚  â”‚DN6â”‚   â”‚  â”‚  â”‚DN9â”‚ ...  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”˜      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”   â”‚  â”‚             â”‚ â”‚
â”‚  â”‚  â”‚DN3â”‚  â”‚DN4â”‚   â”‚  â”‚  â”‚DN7â”‚  â”‚DN8â”‚   â”‚  â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜   â”‚  â”‚             â”‚ â”‚
â”‚  â”‚      â†“          â”‚  â”‚       â†“         â”‚  â”‚      â†“       â”‚ â”‚
â”‚  â”‚  [Switch 1]     â”‚  â”‚  [Switch 2]     â”‚  â”‚  [Switch 3]  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚          â”‚                      â”‚                  â”‚         â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                         â”‚                                     â”‚
â”‚                   [Core Switch]                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Network Bandwidth:
- Within rack: 10 Gbps (fast!)
- Between racks: 1 Gbps (10x slower)
- Rack switch failure â†’ entire rack offline
```

### Rack Awareness Benefits

**1. Replica Placement:**
```
Without Rack Awareness (Random):
All 3 replicas might be on Rack 1
â†’ Rack 1 fails â†’ All replicas lost! âŒ

With Rack Awareness:
Replica 1: Rack 1
Replica 2: Rack 1 (different node)
Replica 3: Rack 2
â†’ Rack 1 fails â†’ Replica 3 survives âœ…
```

**2. Read Performance:**
```
File has replicas on:
- Rack 1, Node 2
- Rack 1, Node 4  
- Rack 2, Node 6

Client on Rack 1:
â†’ Reads from Rack 1 (10 Gbps, local)
âœ… Fast!

Client on Rack 3:
â†’ Must cross core switch (1 Gbps)
â†’ Reads from nearest replica
âš ï¸ Slower, but still works
```

**3. Write Pipeline:**
```
Write 1 block with replication=3:

Step 1: Client â†’ DataNode2 (Rack 1) - fast local write
Step 2: DataNode2 â†’ DataNode4 (Rack 1) - fast within rack
Step 3: DataNode4 â†’ DataNode6 (Rack 2) - slower cross-rack

Pipeline:
Client â”€â”€â”€â”€â”€â”€> DN2 â”€â”€â”€â”€â”€â”€> DN4 â”€â”€â”€â”€â”€â”€> DN6
         (local)     (same rack)  (different rack)
         10 Gbps       10 Gbps        1 Gbps

Total time optimized by pipelining!
```

### Configuring Rack Awareness

**rack-awareness.sh:**
```bash
#!/bin/bash
# Map IP addresses to rack names

if [ $# -eq 0 ]; then
    echo "Usage: rack-awareness.sh <ip-address>"
    exit 1
fi

IP=$1

# Define rack mapping
case $IP in
    10.0.1.*) echo "/rack1" ;;
    10.0.2.*) echo "/rack2" ;;
    10.0.3.*) echo "/rack3" ;;
    *)        echo "/default-rack" ;;
esac
```

**hdfs-site.xml:**
```xml
<property>
  <name>net.topology.script.file.name</name>
  <value>/etc/hadoop/conf/rack-awareness.sh</value>
</property>
```

---

## HDFS Read Mechanism

### Read Flow Architecture

```
Step-by-Step Read Process:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚  "Read /user/data/file.txt"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ 1. Open file
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    NameNode     â”‚  "Here are block locations:"
â”‚  (Metadata Only)â”‚  Block1 â†’ [DN1, DN3, DN5]
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Block2 â†’ [DN2, DN4, DN6]
     â”‚ 2. Returns block locations (sorted by distance)
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚  3. Opens connection to closest DataNode
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     (DN1 for Block1)
     â”‚
     â”‚ 4. Read Block1 data
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DN1    â”‚  5. Stream data to client
â”‚ (Rack 1)â”‚     Verify checksum
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ 6. Block1 complete
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚  7. Read Block2 from DN2
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ 8. Stream data
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DN2    â”‚  9. Complete read
â”‚ (Rack 1)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Read Steps

**1. Client Opens File:**
```python
# Using HDFS API
fs = hdfs.FileSystem()
file_handle = fs.open('/user/data/large_file.dat')

# Client contacts NameNode:
# "Give me metadata for /user/data/large_file.dat"
```

**2. NameNode Returns Metadata:**
```python
# NameNode response:
{
  "blocks": [
    {
      "blockId": "blk_1001",
      "size": 134217728,  # 128 MB
      "locations": [
        {"host": "dn1.example.com", "rack": "/rack1"},
        {"host": "dn3.example.com", "rack": "/rack1"},
        {"host": "dn5.example.com", "rack": "/rack2"}
      ]
    },
    {
      "blockId": "blk_1002",
      "size": 134217728,
      "locations": [
        {"host": "dn2.example.com", "rack": "/rack1"},
        {"host": "dn4.example.com", "rack": "/rack1"},
        {"host": "dn6.example.com", "rack": "/rack2"}
      ]
    }
  ]
}

# Locations sorted by:
# 1. Same node (if exists)
# 2. Same rack
# 3. Different rack
```

**3. Client Reads from DataNode:**
```python
# Client selects closest DataNode
# Reads Block1 from dn1.example.com

# Read process:
while not end_of_block:
    chunk = datanode.read(64 KB)  # Read in chunks
    verify_checksum(chunk)  # Verify integrity
    process(chunk)  # Use data
    
# If checksum fails:
# â†’ Try next replica (dn3)
# â†’ Report corrupt block to NameNode
```

**4. Data Locality:**
```
Best Case: Data on same node
  Read time: ~10 ms (disk seek) + transfer time
  
Good Case: Data on same rack  
  Read time: ~10 ms + network time (10 Gbps)
  
Worst Case: Data on different rack
  Read time: ~10 ms + network time (1 Gbps, slower)

HDFS + Spark optimization:
â†’ Schedule computation on nodes with data
â†’ Minimize network transfer
â†’ 10-100x faster processing!
```

### Error Handling

```python
# Checksum verification
def read_block_with_retry(block_locations):
    for location in block_locations:
        try:
            data = read_from_datanode(location)
            if verify_checksum(data):
                return data  # Success!
            else:
                report_corrupt_block(location)
                continue  # Try next replica
        except NetworkError:
            continue  # Try next replica
    
    raise Exception("All replicas failed!")

# NameNode notified of corruption:
# â†’ Marks block as corrupt
# â†’ Triggers re-replication from good replica
# â†’ Eventually deletes corrupt replica
```

---

## HDFS Write Mechanism

### Write Flow Architecture

```
Write Process:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚  "Write /user/data/new_file.txt"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ 1. Create file request
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    NameNode     â”‚  "OK, write to these DataNodes:"
â”‚  (Metadata)     â”‚  Pipeline: [DN1 â†’ DN3 â†’ DN5]
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ 2. Returns write pipeline
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚  3. Establish pipeline
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     DN1 â†’ DN3 â†’ DN5
     â”‚
     â”‚ 4. Write data packets
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DN1    â”‚â”€â”€â”€â”€>â”‚  DN3    â”‚â”€â”€â”€â”€>â”‚  DN5    â”‚
â”‚ (Rack1) â”‚ 5.  â”‚ (Rack1) â”‚ 6.  â”‚ (Rack2) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                7. ACK packets flow back
                     â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Client  â”‚  8. Close file
               â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                    â”‚ 9. Finalize
                    â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    NameNode     â”‚  10. Mark complete
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Write Steps

**1. Create File:**
```python
# Client requests file creation
fs = hdfs.FileSystem()
output = fs.create('/user/data/output.txt', replication=3)

# NameNode checks:
# âœ… File doesn't exist?
# âœ… Parent directory exists?
# âœ… Client has write permission?
# â†’ Create metadata entry (size=0, no blocks yet)
```

**2. Request Block:**
```python
# When client has data to write:
# Client â†’ NameNode: "Allocate block for me"

# NameNode response:
{
  "blockId": "blk_2001",
  "pipeline": [
    "dn2.example.com:50010",  # Primary
    "dn4.example.com:50010",  # Replica 1
    "dn6.example.com:50010"   # Replica 2
  ]
}

# NameNode selection criteria:
# - DataNodes with available space
# - Rack awareness (2 same rack, 1 different)
# - Load balancing
```

**3. Write Pipeline:**
```python
# Client establishes pipeline
# Client â†’ DN2, DN2 â†’ DN4, DN4 â†’ DN6

# Write in packets (64 KB each)
for packet in data_packets:
    # Client sends to DN2
    dn2.write(packet)
    
    # DN2 forwards to DN4
    dn4.write(packet)
    
    # DN4 forwards to DN6
    dn6.write(packet)
    
    # ACKs flow back
    dn6 â†’ dn4: ACK
    dn4 â†’ dn2: ACK  
    dn2 â†’ client: ACK
    
    # Client waits for ACK before sending next packet
```

**4. Pipeline Diagram:**
```
Time â†’

t0: Client sends Packet1 to DN2
t1: DN2 writes Packet1, forwards to DN4
t2: DN4 writes Packet1, forwards to DN6
    Client sends Packet2 to DN2
t3: DN6 writes Packet1, sends ACK
    DN2 writes Packet2, forwards to DN4
t4: DN4 sends ACK to DN2
    DN4 writes Packet2, forwards to DN6
t5: DN2 sends ACK to Client
    Client sends Packet3
...

Pipelining = parallelism = faster writes!
```

**5. Handling Failures:**
```python
# Scenario: DN4 fails during write

Pipeline: [DN2, DN4, DN6]
                â†“ FAILS

# Client action:
1. Remove DN4 from pipeline
2. New pipeline: [DN2, DN6]
3. Continue writing
4. Notify NameNode of partial replication

# NameNode action:
1. Mark block as under-replicated (2/3 replicas)
2. Schedule re-replication later
3. Choose DN8 for 3rd replica
4. DN2 â†’ DN8: replicate block

Result: Block eventually has 3 replicas âœ…
```

---

## HDFS CLI Commands

### Basic Commands

```bash
# Help
hdfs dfs -help
hdfs dfs -help ls

# List files
hdfs dfs -ls /user/data
hdfs dfs -ls -h /user/data  # Human-readable sizes
hdfs dfs -ls -R /user/data  # Recursive

# Create directory
hdfs dfs -mkdir /user/data
hdfs dfs -mkdir -p /user/data/year/month/day  # Create parents

# Upload to HDFS
hdfs dfs -put /local/file.txt /user/data/
hdfs dfs -put -f /local/file.txt /user/data/  # Force overwrite
hdfs dfs -copyFromLocal /local/file.txt /user/data/  # Same as put

# Download from HDFS
hdfs dfs -get /user/data/file.txt /local/
hdfs dfs -copyToLocal /user/data/file.txt /local/  # Same as get
hdfs dfs -getmerge /user/data/parts/* /local/merged.txt  # Merge files

# View files
hdfs dfs -cat /user/data/file.txt
hdfs dfs -head /user/data/file.txt  # First 1KB
hdfs dfs -tail /user/data/file.txt  # Last 1KB
hdfs dfs -cat /user/data/*.txt | grep "ERROR"  # Pipeline with grep

# File info
hdfs dfs -stat "%n %b %r" /user/data/file.txt  # name, size, replication
hdfs dfs -du /user/data  # Disk usage
hdfs dfs -du -h /user/data  # Human-readable
hdfs dfs -df -h /  # Filesystem space

# Copy/Move
hdfs dfs -cp /user/data/file.txt /user/backup/
hdfs dfs -mv /user/data/old.txt /user/data/new.txt

# Delete
hdfs dfs -rm /user/data/file.txt
hdfs dfs -rm -r /user/data/directory  # Recursive
hdfs dfs -rm -skipTrash /user/data/file.txt  # Skip trash

# Permissions
hdfs dfs -chmod 755 /user/data/file.txt
hdfs dfs -chown user:group /user/data/file.txt
hdfs dfs -chgrp group /user/data/file.txt

# Block information
hdfs fsck /user/data/file.txt -files -blocks -locations
```

### Advanced Commands

```bash
# Replication
hdfs dfs -setrep 5 /user/data/important.txt  # Set replication to 5
hdfs dfs -setrep -R 3 /user/data/  # Recursive

# Disk usage
hdfs dfs -count /user/data  # DIR_COUNT FILE_COUNT CONTENT_SIZE
hdfs dfs -count -q /user/data  # With quota info

# File checksum
hdfs dfs -checksum /user/data/file.txt

# Test
hdfs dfs -test -e /user/data/file.txt && echo "exists"
hdfs dfs -test -z /user/data/file.txt && echo "zero length"
hdfs dfs -test -d /user/data && echo "is directory"

# Append (HDFS 2.x+)
hdfs dfs -appendToFile /local/append.txt /user/data/file.txt

# Snapshot (if enabled)
hdfs dfs -createSnapshot /user/data snapshot1
hdfs dfs -renameSnapshot /user/data snapshot1 snapshot_backup
hdfs dfs -deleteSnapshot /user/data snapshot1
```

### HDFS Admin Commands

```bash
# Cluster health
hdfs dfsadmin -report  # Cluster summary
hdfs dfsadmin -safemode get  # Check safemode status

# Balancer (redistribute blocks evenly)
hdfs balancer -threshold 10  # Balance if deviation > 10%

# File system check
hdfs fsck /  # Check entire filesystem
hdfs fsck /user/data -files -blocks -locations -racks

# Block operations
hdfs fsck /user/data/file.txt -files -blocks -delete  # Delete corrupt
hdfs debug verifyMeta -block blk_1001  # Verify block checksum
```

### Practical Examples

```bash
# Example 1: Upload large dataset
hdfs dfs -mkdir -p /datasets/logs/2024
hdfs dfs -put -f /local/logs/*.gz /datasets/logs/2024/

# Example 2: Check file replication
hdfs dfs -stat "Replication: %r" /datasets/logs/2024/app.log.gz

# Example 3: Find large files
hdfs dfs -du -h /user/data | sort -h | tail -10

# Example 4: Count files by extension
hdfs dfs -ls -R /user/data | grep "\.csv$" | wc -l

# Example 5: Compress and upload
tar -czf - /local/data | hdfs dfs -put - /datasets/data.tar.gz

# Example 6: Download and process
hdfs dfs -cat /logs/*.log | grep "ERROR" > /local/errors.log

# Example 7: Copy between clusters
hdfs dfs -cp hdfs://cluster1:9000/data hdfs://cluster2:9000/data

# Example 8: Check block corruption
hdfs fsck / | grep -i corrupt

# Example 9: Monitor disk usage
watch -n 60 'hdfs dfs -df -h'

# Example 10: Backup critical data
hdfs dfs -get /user/critical/* /backup/$(date +%Y%m%d)/
```

---

## File Permissions

### Permission Model

```
HDFS permissions similar to POSIX (Linux):

Format: drwxrwxrwx owner group

Example:
-rw-r--r--  1 john  engineers  1048576  Dec 13 10:00  data.txt
â”‚â”‚â”‚â”‚â”‚â”‚â”‚â”‚â”‚  â”‚  â”‚     â”‚          â”‚        â”‚            â”‚
â”‚â”‚â”‚â”‚â”‚â”‚â”‚â”‚â””â”€ other execute (0)   â”‚        â”‚            â”‚
â”‚â”‚â”‚â”‚â”‚â”‚â”‚â””â”€â”€ other write (0)     â”‚        â”‚            â””â”€ filename
â”‚â”‚â”‚â”‚â”‚â”‚â””â”€â”€â”€ other read (1)      â”‚        â””â”€ modification time
â”‚â”‚â”‚â”‚â”‚â””â”€â”€â”€â”€ group execute (0)   â””â”€ size in bytes
â”‚â”‚â”‚â”‚â””â”€â”€â”€â”€â”€ group write (0)
â”‚â”‚â”‚â””â”€â”€â”€â”€â”€â”€ group read (1)
â”‚â”‚â””â”€â”€â”€â”€â”€â”€â”€ owner execute (0)
â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€ owner write (1)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ owner read (1)

Permissions:
r (4) = read
w (2) = write  
x (1) = execute

755 = rwxr-xr-x
644 = rw-r--r--
777 = rwxrwxrwx
```

### Setting Permissions

```bash
# Change mode
hdfs dfs -chmod 755 /user/data/script.sh
hdfs dfs -chmod -R 644 /user/data/*.txt
hdfs dfs -chmod u+x,g+x /user/data/script.sh
hdfs dfs -chmod o-w /user/data/public.txt

# Change owner
hdfs dfs -chown john /user/data/file.txt
hdfs dfs -chown john:engineers /user/data/file.txt
hdfs dfs -chown -R john:engineers /user/data/

# Change group
hdfs dfs -chgrp engineers /user/data/project/*

# View permissions
hdfs dfs -ls /user/data
```

### Permission Checking

```
Access Check:
1. If user is superuser â†’ Allow
2. If user is owner â†’ Check owner permissions
3. If user in group â†’ Check group permissions
4. Otherwise â†’ Check other permissions

Example:
File: -rw-r----- john engineers data.txt

User: john   â†’ Can read, write
User: jane (in engineers) â†’ Can read only
User: bob (not in engineers) â†’ No access
User: hdfs (superuser) â†’ Full access
```

### ACLs (Access Control Lists)

```bash
# Set ACL
hdfs dfs -setfacl -m user:jane:rw- /user/data/file.txt
hdfs dfs -setfacl -m group:analysts:r-- /user/data/file.txt

# View ACL
hdfs dfs -getfacl /user/data/file.txt

# Remove ACL
hdfs dfs -setfacl -x user:jane /user/data/file.txt

# Remove all ACLs
hdfs dfs -setfacl -b /user/data/file.txt

# Recursive ACL
hdfs dfs -setfacl -R -m user:jane:rwx /user/data/directory

# Default ACL (for new files in directory)
hdfs dfs -setfacl -m default:user:jane:rw- /user/data/directory
```

---

## Best Practices

### 1. File Size Optimization

```
âŒ Bad: 1 billion Ã— 1KB files = 150 GB NameNode RAM
âœ… Good: Combine into larger files (100MB+)

Solutions:
- Use sequence files or Parquet (columnar storage)
- Batch small files before uploading
- Use compression (gzip, snappy)
- Consider HBase for small records
```

### 2. Replication Strategy

```
Default: replication = 3

When to adjust:
- Critical data: replication = 4 or 5
- Temporary data: replication = 1 or 2
- Hot data: Higher replication for read load balancing
- Cold data: Lower replication to save space

Example:
hdfs dfs -setrep 1 /tmp/staging/*  # Temporary files
hdfs dfs -setrep 5 /critical/financial/*  # Critical data
```

### 3. Directory Organization

```
âœ… Good structure:
/data/
  â”œâ”€ raw/          # Raw ingested data
  â”œâ”€ processed/    # Cleaned data
  â”œâ”€ aggregated/   # Summary tables
  â”œâ”€ archive/      # Old data
  â””â”€ tmp/          # Temporary (replication=1)

/logs/
  â”œâ”€ year=2024/
  â”‚   â”œâ”€ month=01/
  â”‚   â”‚   â”œâ”€ day=01/
  â”‚   â”‚   â””â”€ day=02/
  â”‚   â””â”€ month=02/

Partitioning enables:
- Efficient pruning
- Easy deletion of old data
- Clear data lifecycle
```

### 4. Performance Tips

```bash
# Use compression
gzip large_file.txt
hdfs dfs -put large_file.txt.gz /data/

# Parallel uploads
ls /local/*.gz | xargs -P 10 -I {} hdfs dfs -put {} /data/

# Optimize block size for workload
hdfs dfs -D dfs.blocksize=256M -put huge_file.dat /data/

# Use distcp for large transfers
hadoop distcp /source /destination
hadoop distcp hdfs://cluster1/data hdfs://cluster2/data

# Monitor before large operations
hdfs dfsadmin -report  # Check available space
```

### 5. Data Integrity

```bash
# Always verify after upload
md5sum /local/file.txt
hdfs dfs -cat /hdfs/file.txt | md5sum

# Regular filesystem checks
hdfs fsck / -files -blocks

# Check specific file
hdfs fsck /important/data.txt -files -blocks -locations

# Enable checksums (default: ON)
hdfs dfs -Ddfs.client.use.datanode.hostname=true -put file.txt /data/
```

### 6. Monitoring

```bash
# Monitor NameNode
# Web UI: http://namenode:9870

# Monitor DataNodes
hdfs dfsadmin -report

# Check under-replicated blocks
hdfs fsck / -blocks | grep "Under replicated"

# Monitor space usage
hdfs dfs -du -h / | sort -h | tail -20

# Set quotas
hdfs dfsadmin -setSpaceQuota 1T /user/john
hdfs dfsadmin -setQuota 1000000 /user/john  # 1M files
```

---

## Summary

### HDFS Strengths âœ…
- Scales to petabytes
- Fault-tolerant through replication
- High throughput for large files
- Cost-effective (commodity hardware)
- Data locality optimization

### HDFS Limitations âŒ
- Not for small files (metadata overhead)
- High latency (not real-time)
- No random writes (append-only)
- NameNode is single point of failure (use HA)
- Not POSIX compliant

### Key Concepts Mastered ğŸ¯
1. NameNode stores metadata, DataNodes store blocks
2. Default block size: 128 MB
3. Default replication: 3 copies
4. Rack awareness for reliability and performance
5. Write pipeline for efficient replication
6. Read optimization through data locality

### Next Steps
- âœ… HDFS fundamentals (COMPLETE)
- ğŸ“š Practice CLI commands
- ğŸ”§ Set up local HDFS cluster (Docker)
- ğŸ’» Integrate with Spark for big data processing
- ğŸ“Š Monitor and optimize production clusters

**Remember**: HDFS is the storage foundation for the entire Hadoop/Spark ecosystem. Master it to understand how your data is actually stored and processed! ğŸš€
