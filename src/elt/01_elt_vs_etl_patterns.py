"""
ELT vs ETL Patterns in PySpark

This module demonstrates the difference between ETL and ELT patterns,
when to use each approach, and practical examples.

WHAT IS ELT?
============
ELT = Extract â†’ Load â†’ Transform

You use ELT when:
â€¢ You have a powerful data warehouse (BigQuery, Snowflake, Redshift, Databricks)
â€¢ It's faster to LOAD raw data first and then do transformations INSIDE the warehouse
â€¢ You want to keep raw data for future re-processing
â€¢ Your data warehouse can handle complex transformations efficiently

WHY ELT?
========
1. Speed: Modern data warehouses are optimized for large-scale transformations
2. Flexibility: Raw data is available for different transformation logic later
3. Simplicity: Less moving parts, transformations happen in one place
4. Cost: Separation of compute and storage (cloud warehouses)

WHEN TO USE ELT:
===============
âœ… Cloud data warehouses (BigQuery, Snowflake, Redshift)
âœ… Large datasets (TB to PB scale)
âœ… Need to preserve raw data
âœ… Multiple teams need different views of same data
âœ… Schema changes frequently
âœ… Powerful query engine available

WHEN TO USE ETL (Transform BEFORE Load):
=======================================
âœ… Limited warehouse capacity
âœ… Need to cleanse/filter before storage (reduce size)
âœ… Legacy systems
âœ… Data quality issues must be fixed before storage
âœ… Compliance requirements (PII removal)

Author: PySpark Learning Series
Date: December 2024
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time


def create_spark_session():
    """Create Spark session for ELT examples."""
    print("\n" + "=" * 70)
    print("CREATING SPARK SESSION FOR ELT PATTERNS")
    print("=" * 70)
    
    spark = SparkSession.builder \
        .appName("ELT vs ETL Patterns") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    print("âœ… Spark session created")
    return spark


def example_1_etl_pattern_traditional(spark):
    """
    ETL Pattern: Extract â†’ Transform â†’ Load
    
    Transform data BEFORE loading into warehouse.
    Use when: Storage is expensive or limited.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 1: ETL PATTERN (Traditional)")
    print("=" * 70)
    
    print("""
    ETL FLOW:
    ========
    1. EXTRACT: Read raw data from source
    2. TRANSFORM: Clean, filter, aggregate (BEFORE loading)
    3. LOAD: Write transformed data to warehouse
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SOURCE    â”‚
    â”‚  (Raw Data) â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ EXTRACT
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  TRANSFORM  â”‚ â† All transformations happen HERE
    â”‚  (Clean,    â”‚   (filter, aggregate, join)
    â”‚   Filter)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ LOAD
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  WAREHOUSE  â”‚ â† Only clean data stored
    â”‚ (Clean Data)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Advantages:
    â€¢ Less storage (only clean data)
    â€¢ Faster queries (pre-aggregated)
    
    Disadvantages:
    â€¢ Raw data lost
    â€¢ Can't re-process with different logic
    """)
    
    # Sample raw sales data
    raw_data = [
        (1, "Alice", "Electronics", 1200, "2024-01-15", "NY"),
        (2, "Bob", "Electronics", -50, "2024-01-16", "NY"),  # Invalid (negative)
        (3, None, "Clothing", 300, "2024-01-17", "CA"),      # Invalid (no name)
        (4, "Charlie", "Electronics", 800, "2024-01-18", "TX"),
        (5, "Diana", "Clothing", 450, "2024-01-19", "CA"),
        (6, "Eve", "Electronics", 1500, "2024-01-20", "NY"),
        (7, "Frank", "Furniture", 0, "2024-01-21", "TX"),    # Invalid (zero)
    ]
    
    raw_df = spark.createDataFrame(raw_data, 
        ["id", "customer", "category", "amount", "date", "state"])
    
    print("\nğŸ“Š RAW DATA (Before ETL):")
    raw_df.show()
    print(f"Total records: {raw_df.count()}")
    
    # TRANSFORM: Clean and filter BEFORE loading
    print("\nğŸ”§ TRANSFORMING DATA (Before Load)...")
    
    cleaned_df = raw_df \
        .filter(col("customer").isNotNull()) \
        .filter(col("amount") > 0) \
        .withColumn("year", year(col("date"))) \
        .withColumn("month", month(col("date"))) \
        .select("id", "customer", "category", "amount", "state", "year", "month")
    
    print("\nâœ… TRANSFORMED DATA (After Cleaning):")
    cleaned_df.show()
    print(f"Total records after cleaning: {cleaned_df.count()}")
    print(f"Records removed: {raw_df.count() - cleaned_df.count()}")
    
    # LOAD: Save only cleaned data
    print("\nğŸ’¾ LOADING to warehouse (only clean data)...")
    
    # In real scenario: cleaned_df.write.mode("overwrite").parquet("/warehouse/sales")
    print("âœ… Loaded to warehouse: /warehouse/sales")
    print("âŒ Raw data NOT saved (lost forever)")
    
    print("\n" + "=" * 70)
    print("ETL PATTERN COMPLETE")
    print("Result: Only clean data in warehouse, raw data discarded")
    print("=" * 70)


def example_2_elt_pattern_modern(spark):
    """
    ELT Pattern: Extract â†’ Load â†’ Transform
    
    Load raw data first, transform INSIDE warehouse.
    Use when: Have powerful data warehouse (BigQuery, Snowflake, etc.)
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 2: ELT PATTERN (Modern)")
    print("=" * 70)
    
    print("""
    ELT FLOW:
    ========
    1. EXTRACT: Read raw data from source
    2. LOAD: Load raw data to warehouse (NO transformation)
    3. TRANSFORM: Transform INSIDE warehouse (using SQL/Spark)
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SOURCE    â”‚
    â”‚  (Raw Data) â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ EXTRACT
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  WAREHOUSE  â”‚
    â”‚  RAW LAYER  â”‚ â† Load raw data FIRST
    â”‚  (No Clean) â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ TRANSFORM (inside warehouse)
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  WAREHOUSE  â”‚
    â”‚ CLEAN LAYER â”‚ â† Transform AFTER loading
    â”‚ (Cleaned)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Advantages:
    â€¢ Raw data preserved for re-processing
    â€¢ Flexibility to apply different transformations
    â€¢ Leverage warehouse compute power
    
    Disadvantages:
    â€¢ More storage (raw + clean)
    â€¢ Initial load faster but query might be slower
    """)
    
    # Same raw data
    raw_data = [
        (1, "Alice", "Electronics", 1200, "2024-01-15", "NY"),
        (2, "Bob", "Electronics", -50, "2024-01-16", "NY"),
        (3, None, "Clothing", 300, "2024-01-17", "CA"),
        (4, "Charlie", "Electronics", 800, "2024-01-18", "TX"),
        (5, "Diana", "Clothing", 450, "2024-01-19", "CA"),
        (6, "Eve", "Electronics", 1500, "2024-01-20", "NY"),
        (7, "Frank", "Furniture", 0, "2024-01-21", "TX"),
    ]
    
    raw_df = spark.createDataFrame(raw_data, 
        ["id", "customer", "category", "amount", "date", "state"])
    
    # LOAD: Save raw data FIRST (no transformation)
    print("\nğŸ’¾ LOADING raw data to warehouse (NO cleaning)...")
    print("âœ… Loaded to warehouse: /warehouse/sales_raw")
    
    # Create temporary view for SQL transformation
    raw_df.createOrReplaceTempView("sales_raw")
    
    print("\nğŸ“Š RAW DATA LOADED:")
    raw_df.show()
    print(f"Total records in raw layer: {raw_df.count()}")
    
    # TRANSFORM: Now transform INSIDE warehouse using SQL
    print("\nğŸ”§ TRANSFORMING DATA (Inside Warehouse using SQL)...")
    
    cleaned_df = spark.sql("""
        SELECT 
            id,
            customer,
            category,
            amount,
            state,
            YEAR(date) as year,
            MONTH(date) as month
        FROM sales_raw
        WHERE customer IS NOT NULL
          AND amount > 0
    """)
    
    print("\nâœ… TRANSFORMED DATA (Created from raw):")
    cleaned_df.show()
    print(f"Total records in clean layer: {cleaned_df.count()}")
    
    # Save cleaned view
    print("\nğŸ’¾ SAVING cleaned view...")
    print("âœ… Created view: /warehouse/sales_clean")
    
    print("\nğŸ”„ KEY DIFFERENCE:")
    print("â€¢ RAW data still available: /warehouse/sales_raw")
    print("â€¢ CLEAN data also available: /warehouse/sales_clean")
    print("â€¢ Can re-transform raw data anytime with different logic!")
    
    # Example: Different transformation on same raw data
    print("\nğŸ”„ CREATING DIFFERENT VIEW (Same raw data, different logic):")
    
    summary_df = spark.sql("""
        SELECT 
            category,
            state,
            COUNT(*) as total_orders,
            SUM(amount) as total_sales,
            AVG(amount) as avg_sale
        FROM sales_raw
        WHERE amount > 0
        GROUP BY category, state
        ORDER BY total_sales DESC
    """)
    
    print("\nğŸ“Š SUMMARY VIEW (Different transformation):")
    summary_df.show()
    
    print("\n" + "=" * 70)
    print("ELT PATTERN COMPLETE")
    print("Result: Raw data preserved, multiple views created")
    print("=" * 70)


def example_3_elt_use_case_bigquery_style(spark):
    """
    Real-world ELT pattern similar to BigQuery/Snowflake.
    
    Multi-layer architecture:
    â€¢ Bronze: Raw data (as-is)
    â€¢ Silver: Cleaned data
    â€¢ Gold: Aggregated/business metrics
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 3: REAL-WORLD ELT (Multi-Layer Architecture)")
    print("=" * 70)
    
    print("""
    MULTI-LAYER ELT ARCHITECTURE:
    ============================
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         BRONZE LAYER (Raw)                â”‚
    â”‚  â€¢ Load data as-is                        â”‚
    â”‚  â€¢ No transformations                     â”‚
    â”‚  â€¢ Preserve original format               â”‚
    â”‚  â€¢ Source of truth                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         SILVER LAYER (Cleaned)            â”‚
    â”‚  â€¢ Data quality checks                    â”‚
    â”‚  â€¢ Standardization                        â”‚
    â”‚  â€¢ Type conversions                       â”‚
    â”‚  â€¢ Deduplication                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         GOLD LAYER (Business Metrics)     â”‚
    â”‚  â€¢ Aggregations                           â”‚
    â”‚  â€¢ KPIs and metrics                       â”‚
    â”‚  â€¢ Ready for BI tools                     â”‚
    â”‚  â€¢ Optimized for queries                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Used by: BigQuery, Snowflake, Databricks, Redshift
    """)
    
    # Simulate raw data with quality issues
    raw_data = [
        (1, "alice@email.com", "Electronics", "1200", "2024-01-15", "new york"),
        (2, "bob@EMAIL.COM", "ELECTRONICS", "850", "2024-01-16", "NEW YORK"),
        (1, "alice@email.com", "Electronics", "1200", "2024-01-15", "new york"),  # Duplicate
        (3, "charlie@email", "Clothing", "abc", "2024-01-17", "Los Angeles"),    # Bad amount
        (4, "DIANA@email.com", "clothing", "450", "2024-01-18", "los angeles"),
        (5, "", "Furniture", "1500", "2024-01-19", "Chicago"),                   # Missing email
    ]
    
    # BRONZE LAYER: Load as-is
    print("\nğŸ¥‰ BRONZE LAYER: Loading raw data...")
    bronze_df = spark.createDataFrame(raw_data, 
        ["id", "email", "category", "amount_str", "date", "city"])
    
    bronze_df.createOrReplaceTempView("bronze_sales")
    print("\nğŸ“Š BRONZE DATA (Raw, as-is):")
    bronze_df.show(truncate=False)
    print(f"Total records: {bronze_df.count()}")
    
    # SILVER LAYER: Clean and standardize
    print("\nğŸ¥ˆ SILVER LAYER: Cleaning and standardizing...")
    
    silver_df = spark.sql("""
        SELECT 
            id,
            LOWER(TRIM(email)) as email,
            INITCAP(category) as category,
            CAST(amount_str AS DOUBLE) as amount,
            TO_DATE(date) as date,
            INITCAP(TRIM(city)) as city
        FROM bronze_sales
        WHERE email != ''
          AND email LIKE '%@%'
          AND amount_str RLIKE '^[0-9]+$'
    """).dropDuplicates(["id", "email", "date"])
    
    silver_df.createOrReplaceTempView("silver_sales")
    print("\nğŸ“Š SILVER DATA (Cleaned):")
    silver_df.show(truncate=False)
    print(f"Total records: {silver_df.count()}")
    print(f"Records cleaned: {bronze_df.count() - silver_df.count()}")
    
    # GOLD LAYER: Business metrics
    print("\nğŸ¥‡ GOLD LAYER: Creating business metrics...")
    
    gold_df = spark.sql("""
        SELECT 
            category,
            city,
            COUNT(DISTINCT id) as unique_customers,
            COUNT(*) as total_orders,
            SUM(amount) as total_revenue,
            AVG(amount) as avg_order_value,
            MIN(date) as first_order,
            MAX(date) as last_order
        FROM silver_sales
        GROUP BY category, city
        ORDER BY total_revenue DESC
    """)
    
    print("\nğŸ“Š GOLD DATA (Business Metrics):")
    gold_df.show(truncate=False)
    
    print("\n" + "=" * 70)
    print("MULTI-LAYER ELT SUMMARY:")
    print("=" * 70)
    print("ğŸ¥‰ Bronze: 6 raw records (with duplicates and errors)")
    print("ğŸ¥ˆ Silver: 3 clean records (deduplicated, validated)")
    print("ğŸ¥‡ Gold: 2 metric rows (aggregated for business)")
    print("\nâœ… All layers preserved - can re-process anytime!")
    print("=" * 70)


def example_4_when_to_use_elt(spark):
    """
    Decision guide: When to use ELT vs ETL.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 4: WHEN TO USE ELT vs ETL")
    print("=" * 70)
    
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    USE ELT WHEN:                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ âœ… Cloud data warehouse (BigQuery, Snowflake, Redshift)   â”‚
    â”‚ âœ… Storage is cheap (cloud storage)                        â”‚
    â”‚ âœ… Compute is powerful (MPP database)                      â”‚
    â”‚ âœ… Need to preserve raw data                               â”‚
    â”‚ âœ… Schema evolves frequently                               â”‚
    â”‚ âœ… Multiple teams need different views                     â”‚
    â”‚ âœ… Large datasets (TB to PB scale)                         â”‚
    â”‚ âœ… Want to re-process with different logic                 â”‚
    â”‚ âœ… Data lake architecture                                  â”‚
    â”‚ âœ… Modern cloud-native stack                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    USE ETL WHEN:                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ âœ… Storage is expensive/limited                            â”‚
    â”‚ âœ… Must filter out sensitive data (PII, compliance)        â”‚
    â”‚ âœ… Data quality issues must be fixed BEFORE storage        â”‚
    â”‚ âœ… Target system is not powerful (legacy)                  â”‚
    â”‚ âœ… Small datasets (GB scale)                               â”‚
    â”‚ âœ… Transformation logic is stable (won't change)           â”‚
    â”‚ âœ… Network bandwidth is limited                            â”‚
    â”‚ âœ… On-premise data centers                                 â”‚
    â”‚ âœ… Traditional data warehouses (Oracle, SQL Server)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    REAL-WORLD EXAMPLES:
    ===================
    
    ELT Use Cases:
    â€¢ Google BigQuery: Load CSV/JSON â†’ Transform with SQL
    â€¢ Snowflake: Load data into stages â†’ Transform in warehouse
    â€¢ Databricks: Bronze â†’ Silver â†’ Gold (Medallion Architecture)
    â€¢ AWS Redshift: S3 â†’ Redshift â†’ Transform with Spectrum
    
    ETL Use Cases:
    â€¢ Apache Spark: Read â†’ Transform â†’ Write to Hive
    â€¢ Legacy systems: SSIS, Informatica, Talend
    â€¢ On-premise: Transform before loading to limited storage
    â€¢ Compliance: Remove PII before storing
    """)
    
    # Practical example: Cost comparison
    print("\nğŸ’° COST COMPARISON:")
    print("=" * 70)
    
    cost_data = [
        ("ETL (Traditional)", "Small", "Low storage cost", "High compute cost", "Data lost"),
        ("ELT (Modern)", "Large", "Medium storage cost", "Low compute cost", "Data preserved"),
    ]
    
    cost_df = spark.createDataFrame(cost_data, 
        ["Pattern", "Data Volume", "Storage", "Compute", "Raw Data"])
    
    cost_df.show(truncate=False)
    
    print("\nğŸ“Š TREND: Industry moving towards ELT")
    print("Reason: Cloud storage is cheap, compute is scalable")
    print("=" * 70)


def main():
    """Run all ELT vs ETL examples."""
    spark = create_spark_session()
    
    try:
        # Run all examples
        example_1_etl_pattern_traditional(spark)
        example_2_elt_pattern_modern(spark)
        example_3_elt_use_case_bigquery_style(spark)
        example_4_when_to_use_elt(spark)
        
        print("\n" + "=" * 70)
        print("KEY TAKEAWAYS:")
        print("=" * 70)
        print("""
        1. ELT = Extract â†’ Load â†’ Transform (inside warehouse)
        2. ETL = Extract â†’ Transform â†’ Load (before warehouse)
        
        3. Use ELT when:
           â€¢ Powerful data warehouse (BigQuery, Snowflake)
           â€¢ Need to preserve raw data
           â€¢ Storage is cheap
        
        4. Use ETL when:
           â€¢ Limited storage
           â€¢ Must cleanse before storing
           â€¢ Legacy systems
        
        5. Modern trend: ELT (cloud data warehouses)
        
        6. Multi-layer architecture:
           Bronze (raw) â†’ Silver (clean) â†’ Gold (metrics)
        """)
        
    finally:
        spark.stop()
        print("\nâœ… Spark session stopped")


if __name__ == "__main__":
    main()
