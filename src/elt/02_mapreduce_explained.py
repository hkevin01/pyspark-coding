"""
MapReduce Programming Model Explained

WHAT IS MAPREDUCE?
==================
MapReduce is a programming model with two main steps:

1. MAP: Break the job into many small tasks processed in parallel
2. REDUCE: Combine all those results into one final output

Example: Count words
â€¢ MAP: Break text into chunks and count locally
â€¢ REDUCE: Sum all counts together

MapReduce was originally developed by Google (2004) for processing large datasets
across distributed clusters. It's the foundation of Hadoop and inspired Spark.

Author: PySpark Learning Series
Date: December 2024
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *


def create_spark_session():
    """Create Spark session for MapReduce examples."""
    print("\n" + "=" * 70)
    print("CREATING SPARK SESSION FOR MAPREDUCE EXAMPLES")
    print("=" * 70)
    
    spark = SparkSession.builder \
        .appName("MapReduce Explained") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    print("âœ… Spark session created")
    return spark


def example_1_word_count_mapreduce(spark):
    """
    Classic MapReduce example: Word count
    
    Problem: Count how many times each word appears in a large text.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 1: WORD COUNT (Classic MapReduce)")
    print("=" * 70)
    
    print("""
    MAPREDUCE FLOW FOR WORD COUNT:
    ==============================
    
    INPUT TEXT:
    "hello world hello spark spark is fast"
    
    STEP 1: MAP (Split and emit key-value pairs)
    ============================================
    Mapper 1: "hello world"
      â†’ (hello, 1)
      â†’ (world, 1)
    
    Mapper 2: "hello spark"
      â†’ (hello, 1)
      â†’ (spark, 1)
    
    Mapper 3: "spark is fast"
      â†’ (spark, 1)
      â†’ (is, 1)
      â†’ (fast, 1)
    
    STEP 2: SHUFFLE & SORT (Group by key)
    =====================================
    hello: [1, 1]
    world: [1]
    spark: [1, 1]
    is: [1]
    fast: [1]
    
    STEP 3: REDUCE (Sum the values)
    ===============================
    Reducer 1: hello â†’ [1, 1] â†’ 2
    Reducer 2: world â†’ [1] â†’ 1
    Reducer 3: spark â†’ [1, 1] â†’ 2
    Reducer 4: is â†’ [1] â†’ 1
    Reducer 5: fast â†’ [1] â†’ 1
    
    FINAL OUTPUT:
    (hello, 2)
    (world, 1)
    (spark, 2)
    (is, 1)
    (fast, 1)
    """)
    
    # Sample data
    text_data = [
        ("hello world hello spark"),
        ("spark is fast"),
        ("hello pyspark"),
        ("spark spark pyspark")
    ]
    
    df = spark.createDataFrame(text_data, ["text"])
    
    print("\nğŸ“Š INPUT DATA:")
    df.show(truncate=False)
    
    # MapReduce with PySpark
    print("\nğŸ”§ APPLYING MAPREDUCE:")
    
    # MAP: Split text into words and emit (word, 1)
    words_df = df.select(explode(split(col("text"), " ")).alias("word"))
    mapped_df = words_df.withColumn("count", lit(1))
    
    print("\n1ï¸âƒ£ MAP PHASE (word, 1):")
    mapped_df.show(truncate=False)
    
    # REDUCE: Group by word and sum counts
    result_df = mapped_df.groupBy("word").agg(sum("count").alias("total_count"))
    
    print("\n2ï¸âƒ£ REDUCE PHASE (Group and Sum):")
    result_df.orderBy(desc("total_count")).show(truncate=False)
    
    print("\n" + "=" * 70)
    print("âœ… MAPREDUCE COMPLETE")
    print("=" * 70)


def example_2_mapreduce_with_rdd(spark):
    """
    Word count using RDD (lower-level MapReduce API).
    
    RDD = Resilient Distributed Dataset
    This is how Spark originally implemented MapReduce before DataFrames.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 2: MAPREDUCE WITH RDD (Lower-Level)")
    print("=" * 70)
    
    print("""
    RDD API: More explicit MapReduce operations
    ==========================================
    
    1. map(): Transform each element
    2. flatMap(): Transform each element to 0 or more elements
    3. reduceByKey(): Reduce values for each key
    4. groupByKey(): Group values for each key
    5. sortByKey(): Sort by key
    """)
    
    # Create RDD
    text_data = [
        "hello world hello spark",
        "spark is fast",
        "hello pyspark",
        "spark spark pyspark"
    ]
    
    rdd = spark.sparkContext.parallelize(text_data)
    
    print("\nğŸ“Š INPUT RDD:")
    print(rdd.collect())
    
    # MAP: Split into words
    print("\n1ï¸âƒ£ MAP (flatMap to split words):")
    words_rdd = rdd.flatMap(lambda line: line.split(" "))
    print(words_rdd.collect())
    
    # MAP: Emit (word, 1) pairs
    print("\n2ï¸âƒ£ MAP (Create key-value pairs):")
    pairs_rdd = words_rdd.map(lambda word: (word, 1))
    print(pairs_rdd.collect())
    
    # REDUCE: Sum counts by word
    print("\n3ï¸âƒ£ REDUCE (reduceByKey - sum counts):")
    counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)
    print(counts_rdd.collect())
    
    # Sort by count
    print("\n4ï¸âƒ£ SORT (Sort by count descending):")
    sorted_rdd = counts_rdd.sortBy(lambda x: x[1], ascending=False)
    result = sorted_rdd.collect()
    
    for word, count in result:
        print(f"  {word}: {count}")
    
    print("\n" + "=" * 70)
    print("âœ… RDD MAPREDUCE COMPLETE")
    print("=" * 70)


def example_3_sales_aggregation_mapreduce(spark):
    """
    Real-world example: Sales aggregation using MapReduce pattern.
    
    Problem: Calculate total sales per category.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 3: SALES AGGREGATION (Real-World MapReduce)")
    print("=" * 70)
    
    print("""
    PROBLEM: Calculate total sales per category
    ===========================================
    
    INPUT: Sales transactions
    (product, category, amount)
    
    MAP PHASE:
    â€¢ Emit (category, amount) for each transaction
    
    SHUFFLE:
    â€¢ Group all amounts by category
    
    REDUCE PHASE:
    â€¢ Sum all amounts for each category
    
    OUTPUT: (category, total_sales)
    """)
    
    # Sample sales data
    sales_data = [
        ("iPhone", "Electronics", 1200),
        ("MacBook", "Electronics", 2500),
        ("Shirt", "Clothing", 45),
        ("TV", "Electronics", 800),
        ("Pants", "Clothing", 60),
        ("Shoes", "Clothing", 80),
        ("iPad", "Electronics", 900),
    ]
    
    df = spark.createDataFrame(sales_data, ["product", "category", "amount"])
    
    print("\nğŸ“Š INPUT DATA (Sales transactions):")
    df.show(truncate=False)
    
    # MAP: Emit (category, amount)
    print("\n1ï¸âƒ£ MAP PHASE (Emit category-amount pairs):")
    mapped_df = df.select("category", "amount")
    mapped_df.show(truncate=False)
    
    # REDUCE: Group by category and sum
    print("\n2ï¸âƒ£ REDUCE PHASE (Group and sum by category):")
    result_df = mapped_df.groupBy("category").agg(
        sum("amount").alias("total_sales"),
        count("*").alias("num_products"),
        avg("amount").alias("avg_price")
    ).orderBy(desc("total_sales"))
    
    result_df.show(truncate=False)
    
    print("\n" + "=" * 70)
    print("âœ… SALES AGGREGATION COMPLETE")
    print("=" * 70)


def example_4_distributed_processing_visual(spark):
    """
    Visualize how MapReduce distributes work across cluster.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 4: DISTRIBUTED PROCESSING (Visual)")
    print("=" * 70)
    
    print("""
    MAPREDUCE ON A DISTRIBUTED CLUSTER:
    ===================================
    
    DATA: 1 TB of log files
    CLUSTER: 100 machines
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              INPUT DATA (HDFS)                   â”‚
    â”‚          1 TB split into 1000 blocks            â”‚
    â”‚         (each block = 1 GB = 128 MB)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ DISTRIBUTE TO WORKERS
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              MAP PHASE (Parallel)               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Worker 1   â”‚  Worker 2   â”‚  ...  â”‚  Worker 100 â”‚
    â”‚  Process    â”‚  Process    â”‚       â”‚  Process    â”‚
    â”‚  Blocks     â”‚  Blocks     â”‚       â”‚  Blocks     â”‚
    â”‚  1-10       â”‚  11-20      â”‚       â”‚  991-1000   â”‚
    â”‚             â”‚             â”‚       â”‚             â”‚
    â”‚  Emit       â”‚  Emit       â”‚       â”‚  Emit       â”‚
    â”‚  (key,val)  â”‚  (key,val)  â”‚       â”‚  (key,val)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ SHUFFLE & SORT (Network Transfer)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           GROUP BY KEY (Shuffle)                â”‚
    â”‚  â€¢ key1: [val1, val2, val3, ...]               â”‚
    â”‚  â€¢ key2: [val1, val2, val3, ...]               â”‚
    â”‚  â€¢ key3: [val1, val2, val3, ...]               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ DISTRIBUTE TO REDUCERS
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            REDUCE PHASE (Parallel)              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Reducer 1  â”‚  Reducer 2  â”‚  ...  â”‚  Reducer 100â”‚
    â”‚  Process    â”‚  Process    â”‚       â”‚  Process    â”‚
    â”‚  key1       â”‚  key2       â”‚       â”‚  key1000    â”‚
    â”‚             â”‚             â”‚       â”‚             â”‚
    â”‚  Aggregate  â”‚  Aggregate  â”‚       â”‚  Aggregate  â”‚
    â”‚  Results    â”‚  Results    â”‚       â”‚  Results    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              FINAL OUTPUT (HDFS)                â”‚
    â”‚           (key1, result1)                       â”‚
    â”‚           (key2, result2)                       â”‚
    â”‚           ...                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    KEY BENEFITS:
    â€¢ Parallel processing: 100 machines work simultaneously
    â€¢ Data locality: Process data where it's stored
    â€¢ Fault tolerance: Re-run failed tasks automatically
    â€¢ Scalability: Add more machines = faster processing
    
    EXAMPLE: Word count on 1 TB of text
    â€¢ Without MapReduce: 1 machine, 10 hours
    â€¢ With MapReduce (100 machines): ~6 minutes!
    """)
    
    print("\n" + "=" * 70)
    print("âœ… DISTRIBUTED PROCESSING EXPLAINED")
    print("=" * 70)


def example_5_mapreduce_vs_spark(spark):
    """
    Comparison: Traditional MapReduce vs Spark.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 5: MAPREDUCE vs SPARK")
    print("=" * 70)
    
    print("""
    HADOOP MAPREDUCE (Traditional):
    ==============================
    â€¢ Write intermediate results to disk after each stage
    â€¢ Slower for iterative algorithms (machine learning)
    â€¢ Batch-only processing
    â€¢ Java API primarily
    
    APACHE SPARK (Modern):
    =====================
    â€¢ Keep intermediate results in memory (RDDs)
    â€¢ 10-100x faster for iterative algorithms
    â€¢ Supports batch, streaming, SQL, ML, graph processing
    â€¢ Python, Scala, Java, R APIs
    
    EXAMPLE: Iterative Algorithm (PageRank)
    =======================================
    
    Hadoop MapReduce:
    Iteration 1: Read HDFS â†’ Map â†’ Reduce â†’ Write HDFS
    Iteration 2: Read HDFS â†’ Map â†’ Reduce â†’ Write HDFS
    Iteration 3: Read HDFS â†’ Map â†’ Reduce â†’ Write HDFS
    ... (many disk I/O operations)
    
    Apache Spark:
    Load data â†’ Cache in memory
    Iteration 1: Map â†’ Reduce (in memory)
    Iteration 2: Map â†’ Reduce (in memory)
    Iteration 3: Map â†’ Reduce (in memory)
    ... (no disk I/O)
    Write final result â†’ HDFS
    
    RESULT: Spark is 10-100x faster!
    """)
    
    comparison_data = [
        ("Storage", "Disk (HDFS)", "Memory (RAM)"),
        ("Speed", "Slow", "Fast (10-100x)"),
        ("Iteration", "Disk I/O each time", "In-memory"),
        ("API", "Java", "Python, Scala, Java, R"),
        ("Use Case", "Batch only", "Batch, Streaming, SQL, ML"),
        ("Ease", "Complex", "Simple (SQL, DataFrames)"),
    ]
    
    comparison_df = spark.createDataFrame(comparison_data, 
        ["Feature", "Hadoop MapReduce", "Apache Spark"])
    
    print("\nğŸ“Š COMPARISON TABLE:")
    comparison_df.show(truncate=False)
    
    print("\n" + "=" * 70)
    print("âœ… SPARK IS THE MODERN MAPREDUCE")
    print("=" * 70)


def main():
    """Run all MapReduce examples."""
    spark = create_spark_session()
    
    try:
        example_1_word_count_mapreduce(spark)
        example_2_mapreduce_with_rdd(spark)
        example_3_sales_aggregation_mapreduce(spark)
        example_4_distributed_processing_visual(spark)
        example_5_mapreduce_vs_spark(spark)
        
        print("\n" + "=" * 70)
        print("KEY TAKEAWAYS:")
        print("=" * 70)
        print("""
        1. MapReduce = MAP (split work) + REDUCE (combine results)
        
        2. MAP phase:
           â€¢ Break job into small parallel tasks
           â€¢ Emit key-value pairs
        
        3. SHUFFLE phase:
           â€¢ Group values by key
           â€¢ Network transfer
        
        4. REDUCE phase:
           â€¢ Aggregate values for each key
           â€¢ Produce final output
        
        5. Benefits:
           â€¢ Parallel processing across cluster
           â€¢ Fault tolerance
           â€¢ Scalability
        
        6. Spark improves MapReduce:
           â€¢ In-memory processing (faster)
           â€¢ High-level APIs (DataFrames, SQL)
           â€¢ Unified engine (batch, streaming, ML)
        """)
        
    finally:
        spark.stop()
        print("\nâœ… Spark session stopped")


if __name__ == "__main__":
    main()
