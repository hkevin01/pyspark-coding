#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
DATA PARTITIONING - Mastering Distribution for Performance
================================================================================

MODULE OVERVIEW:
----------------
Data partitioning is the foundation of distributed computing performance in
Spark. A partition is a logical chunk of data that resides on a single executor.
Understanding partitioning is crucial because it directly impacts:
â€¢ Parallelism: More partitions = more parallel tasks
â€¢ Network shuffle: Bad partitioning = excessive data movement
â€¢ Memory usage: Partition size affects memory pressure
â€¢ Performance: Balanced partitions = balanced execution

This module covers:
1. How Spark partitions data by default
2. When and how to repartition data
3. Difference between repartition() and coalesce()
4. Partitioning strategies (random, hash, range)
5. Handling data skew
6. Production best practices

PURPOSE:
--------
Learn to:
â€¢ Optimize partition count for your data size
â€¢ Choose the right partitioning strategy
â€¢ Balance workload across executors
â€¢ Minimize shuffle operations
â€¢ Handle data skew effectively
â€¢ Tune for joins and aggregations

PARTITIONING FUNDAMENTALS:
--------------------------

What is a Partition?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DATAFRAME (Logical)                       â”‚
â”‚                  1 Million Rows Total                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Divided into Partitions                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Partition 0 â”‚Partition 1 â”‚Partition 2 â”‚Partition 3 â”‚         â”‚
â”‚  â”‚ 250K rows  â”‚ 250K rows  â”‚ 250K rows  â”‚ 250K rows  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚        â†“            â†“            â†“            â†“                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Executor 1  â”‚Executor 2  â”‚Executor 3  â”‚Executor 4  â”‚         â”‚
â”‚  â”‚ Task 1     â”‚ Task 2     â”‚ Task 3     â”‚ Task 4     â”‚         â”‚
â”‚  â”‚ Processes  â”‚ Processes  â”‚ Processes  â”‚ Processes  â”‚         â”‚
â”‚  â”‚ 250K rows  â”‚ 250K rows  â”‚ 250K rows  â”‚ 250K rows  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â”‚  Key: Each partition processed by one task on one executor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Partition Size Guidelines:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition Size           â”‚ Status        â”‚ Impact               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 10 MB                  â”‚ âŒ Too Small  â”‚ Task overhead        â”‚
â”‚ 10 MB - 128 MB           â”‚ âš ï¸  Small     â”‚ More tasks needed    â”‚
â”‚ 128 MB - 256 MB          â”‚ âœ… Optimal    â”‚ Best performance     â”‚
â”‚ 256 MB - 1 GB            â”‚ âœ… Good       â”‚ Acceptable           â”‚
â”‚ 1 GB - 2 GB              â”‚ âš ï¸  Large     â”‚ Memory pressure      â”‚
â”‚ > 2 GB                   â”‚ âŒ Too Large  â”‚ OOM, slow tasks      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HOW SPARK CREATES PARTITIONS:
------------------------------

1. Reading Data:
   â€¢ HDFS/S3: One partition per file block (typically 128MB)
   â€¢ Parquet: Based on row group size and schema
   â€¢ JDBC: Based on partitionColumn, lowerBound, upperBound
   â€¢ CSV: Based on spark.sql.files.maxPartitionBytes (128MB default)

   Example:
   10GB file on HDFS (128MB blocks)
   â†’ 10,000MB / 128MB = ~78 partitions

2. Transformations:
   â€¢ Narrow transformations: Preserve parent partitioning
     (map, filter, union)
   â€¢ Wide transformations: Shuffle to new partitioning
     (groupBy, join, distinct)

3. Default Parallelism:
   spark.default.parallelism:
   â€¢ Local mode: Number of cores on local machine
   â€¢ Cluster: Total cores across all executors
   
   spark.sql.shuffle.partitions (default: 200):
   â€¢ Used after shuffle operations (join, groupBy)
   â€¢ Often needs tuning based on data size

PARTITIONING OPERATIONS:
------------------------

repartition(n) - Full Shuffle:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE: 4 partitions            AFTER: 8 partitions       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”          â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”         â”‚
â”‚  â”‚ P0 â”‚ P1 â”‚ P2 â”‚ P3 â”‚          â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜          â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜         â”‚
â”‚         â†“                               â†‘                   â”‚
â”‚     Full Shuffle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚     (All data redistributed)                               â”‚
â”‚                                                            â”‚
â”‚  â€¢ Can increase OR decrease partitions                    â”‚
â”‚  â€¢ Data redistributed randomly (or by column)             â”‚
â”‚  â€¢ Expensive: Full network shuffle                        â”‚
â”‚  â€¢ Use for: Increasing parallelism, rebalancing skew      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

coalesce(n) - Optimized Reduction:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE: 8 partitions            AFTER: 4 partitions       â”‚
â”‚  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”              â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”     â”‚
â”‚  â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚              â”‚ P0 â”‚ P1 â”‚ P2 â”‚ P3 â”‚     â”‚
â”‚  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜              â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜     â”‚
â”‚   â””â”€â”´ â””â”€â”´ â””â”€â”´ â””â”€â”´                                         â”‚
â”‚    Merge   Merge   Merge   Merge                          â”‚
â”‚    (No shuffle)                                            â”‚
â”‚                                                            â”‚
â”‚  â€¢ Only decreases partitions                              â”‚
â”‚  â€¢ Merges adjacent partitions                             â”‚
â”‚  â€¢ Cheap: No network shuffle                              â”‚
â”‚  â€¢ Use for: Reducing output files, final optimization     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PARTITIONING STRATEGIES:
------------------------

1. Random Partitioning (Default):
   df.repartition(10)
   
   â€¢ Data distributed randomly
   â€¢ Balanced distribution (usually)
   â€¢ No data locality
   â€¢ Use for: General purpose, balanced workload

2. Hash Partitioning (By Column):
   df.repartition(10, "user_id")
   
   â€¢ Same key â†’ Same partition
   â€¢ Deterministic (consistent hashing)
   â€¢ Enables partition-wise joins
   â€¢ Use for: groupBy, joins on same column
   
   Example:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  user_id=123 â†’ hash(123) % 10 â†’ Partition 3         â”‚
   â”‚  user_id=456 â†’ hash(456) % 10 â†’ Partition 6         â”‚
   â”‚  user_id=789 â†’ hash(789) % 10 â†’ Partition 9         â”‚
   â”‚                                                      â”‚
   â”‚  All operations on user_id=123 go to Partition 3    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Range Partitioning:
   df.repartitionByRange(10, "timestamp")
   
   â€¢ Data sorted into ranges
   â€¢ Maintains sort order
   â€¢ Can have skew if data not uniform
   â€¢ Use for: Sorting, range queries, time-series
   
   Example:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Partition 0: timestamp < 2024-01-01                 â”‚
   â”‚  Partition 1: 2024-01-01 â‰¤ timestamp < 2024-02-01    â”‚
   â”‚  Partition 2: 2024-02-01 â‰¤ timestamp < 2024-03-01    â”‚
   â”‚  ...                                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CALCULATING OPTIMAL PARTITIONS:
--------------------------------

Formula:
optimal_partitions = (total_data_size_MB / target_partition_size_MB)

Where:
â€¢ target_partition_size_MB: 128-256 MB (sweet spot)
â€¢ Minimum: num_executors * executor_cores
â€¢ Maximum: No hard limit, but overhead increases

Examples:

Example 1: Small Data (1 GB)
data_size = 1,000 MB
target_size = 200 MB
optimal_partitions = 1,000 / 200 = 5 partitions

Example 2: Medium Data (50 GB)
data_size = 50,000 MB
target_size = 200 MB
optimal_partitions = 50,000 / 200 = 250 partitions

Example 3: Large Data (1 TB)
data_size = 1,000,000 MB
target_size = 256 MB
optimal_partitions = 1,000,000 / 256 = 3,906 partitions

Alternative Formula (Core-based):
partitions = num_executors * executor_cores * (2 to 4)

Example: 10 executors Ã— 4 cores = 40 cores
Recommended: 80-160 partitions (2-4x cores)

DATA SKEW PROBLEM:
------------------

What is Data Skew?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           BALANCED (Good)         SKEWED (Bad)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”¬â”€â”¬â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 25%â”‚ 25%â”‚ 25%â”‚ 25%â”‚      â”‚ 5% â”‚5â”‚5â”‚    85%       â”‚   â”‚
â”‚  â”‚    â”‚    â”‚    â”‚    â”‚      â”‚    â”‚ â”‚ â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”´â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   4 tasks @ 10 min each      3 tasks @ 2 min            â”‚
â”‚   Total: 10 minutes          1 task @ 1 hour            â”‚
â”‚                              Total: 1 hour (bottleneck!)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Causes of Skew:
â€¢ Uneven key distribution (popular keys get more data)
â€¢ NULL values grouped together
â€¢ Natural data patterns (Zipf distribution)

Symptoms:
â€¢ One or few tasks take much longer
â€¢ Executor OOM on specific partitions
â€¢ Shuffle read skew in Spark UI
â€¢ 90% tasks complete quickly, 10% run forever

SKEW MITIGATION TECHNIQUES:
---------------------------

Technique 1: Salting (Add Random Key)
Original:
  groupBy("user_id")  # 90% of data has user_id = "popular_user"

Salted:
  # Add random salt to split hot key
  df.withColumn("salt", (rand() * 10).cast("int")) \\
    .withColumn("salted_key", concat(col("user_id"), lit("_"), col("salt"))) \\
    .groupBy("salted_key") \\
    .agg(...) \\
    .groupBy("user_id") \\  # Remove salt, re-aggregate
    .agg(...)

Technique 2: Broadcast Join (for small dimension tables)
  # Instead of shuffling large fact table
  large_df.join(broadcast(small_df), "key")

Technique 3: Adaptive Query Execution (Spark 3.0+)
  spark.conf.set("spark.sql.adaptive.enabled", "true")
  spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
  # Spark automatically detects and handles skew

Technique 4: Separate Processing for Hot Keys
  # Process hot keys separately with broadcast
  # Process normal keys with standard join

BEST PRACTICES:
---------------

âœ… DO:
1. Set spark.sql.shuffle.partitions based on data size
2. Use coalesce() when reducing partitions
3. Partition on join keys before joins
4. Cache after expensive repartitioning
5. Monitor partition sizes in Spark UI
6. Use salting for skewed data
7. Enable Adaptive Query Execution (Spark 3+)

âŒ DON'T:
1. Use default 200 shuffle partitions for all data sizes
2. Use repartition() when coalesce() works
3. Create too many small partitions (< 10 MB)
4. Create too few large partitions (> 2 GB)
5. Ignore data skew
6. Repartition without checking current partition count

PERFORMANCE IMPACT:
-------------------

Bad Partitioning Costs:
â€¢ Too many partitions: Task scheduling overhead (milliseconds Ã— thousands)
â€¢ Too few partitions: Underutilized cluster, memory pressure
â€¢ Unbalanced partitions: Stragglers delay entire job
â€¢ Wrong key: Excessive shuffle, slow joins

Good Partitioning Benefits:
â€¢ Maximizes parallelism
â€¢ Minimizes shuffle
â€¢ Balances workload
â€¢ Reduces memory pressure
â€¢ Faster joins and aggregations

Example Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario                 â”‚ Bad Practice â”‚ Good Practiceâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 GB data                â”‚ 200 parts    â”‚ 5 parts      â”‚
â”‚                          â”‚ 15 min       â”‚ 3 min (5x)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 100 GB join              â”‚ No partition â”‚ Partition keyâ”‚
â”‚                          â”‚ 45 min       â”‚ 12 min (3.7x)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Skewed group by          â”‚ No handling  â”‚ Salting      â”‚
â”‚                          â”‚ 1 hour       â”‚ 15 min (4x)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MONITORING PARTITIONS:
----------------------

Check Partition Count:
>>> df.rdd.getNumPartitions()
8

View Partition Distribution:
>>> df.withColumn("partition", spark_partition_id()) \\
...   .groupBy("partition").count() \\
...   .orderBy("partition").show()

Spark UI Metrics:
â€¢ Stage Details: Task duration distribution (look for stragglers)
â€¢ Shuffle Read: Size per task (look for skew)
â€¢ Executor Memory: Usage per executor

TARGET AUDIENCE:
----------------
â€¢ Data engineers optimizing Spark jobs
â€¢ Anyone experiencing slow joins or aggregations
â€¢ Developers handling large-scale data processing
â€¢ Teams debugging performance bottlenecks

RELATED RESOURCES:
------------------
â€¢ cluster_computing/03_distributed_joins.py (partition-aware joins)
â€¢ cluster_computing/04_aggregations_at_scale.py
â€¢ cluster_computing/08_shuffle_optimization.py
â€¢ security/02_common_mistakes.py (#6 Wrong Shuffle Partitions)

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 2.0.0 - Comprehensive Partitioning Guide
UPDATED: 2024
================================================================================
"""

import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, spark_partition_id


def create_spark():
    return (
        SparkSession.builder.appName("DataPartitioning")
        .master("local[4]")
        .config("spark.sql.shuffle.partitions", "4")
        .getOrCreate()
    )


def demonstrate_default_partitioning(spark):
    """Show how Spark partitions data by default."""
    print("=" * 70)
    print("1. DEFAULT PARTITIONING")
    print("=" * 70)

    # Create large dataset
    df = spark.range(1, 1000001)  # 1 million rows

    print(f"âœ… Created DataFrame with {df.count():,} rows")
    print(f"ğŸ“¦ Default partitions: {df.rdd.getNumPartitions()}")

    # Show partition distribution
    partition_df = (
        df.withColumn("partition_id", spark_partition_id())
        .groupBy("partition_id")
        .agg(count("*").alias("row_count"))
    )

    print("\nğŸ“Š Rows per partition:")
    partition_df.orderBy("partition_id").show()

    # Rule of thumb
    num_cores = spark.sparkContext.defaultParallelism
    print(f"\nğŸ’¡ Default parallelism: {num_cores} cores")
    print(f"ğŸ’¡ Recommended partitions: {num_cores * 2} to {num_cores * 4}")


def demonstrate_repartition_vs_coalesce(spark):
    """Compare repartition() vs coalesce()."""
    print("\n" + "=" * 70)
    print("2. REPARTITION VS COALESCE")
    print("=" * 70)

    # Start with 8 partitions
    df = spark.range(1, 100001).repartition(8)
    print(f"ğŸ“¦ Initial partitions: {df.rdd.getNumPartitions()}")

    # REPARTITION: Increase partitions (full shuffle)
    print("\nğŸ”„ Using repartition(16) - FULL SHUFFLE:")
    start = time.time()
    df_repartitioned = df.repartition(16)
    df_repartitioned.write.mode("overwrite").format("noop").save()
    duration = time.time() - start
    print(f"   New partitions: {df_repartitioned.rdd.getNumPartitions()}")
    print(f"   Time: {duration:.3f}s")
    print(f"   âš ï¸  Full shuffle: Data moved across all nodes")

    # COALESCE: Decrease partitions (optimized, no shuffle)
    print("\nğŸ”„ Using coalesce(4) - NO SHUFFLE:")
    start = time.time()
    df_coalesced = df.coalesce(4)
    df_coalesced.write.mode("overwrite").format("noop").save()
    duration = time.time() - start
    print(f"   New partitions: {df_coalesced.rdd.getNumPartitions()}")
    print(f"   Time: {duration:.3f}s")
    print(f"   âœ… Optimized: Merges adjacent partitions, no shuffle")

    # When to use each
    print("\nğŸ“ When to use:")
    print("   repartition(N): Increase parallelism, balance skewed data")
    print("   coalesce(N): Reduce output files, final stage optimization")


def demonstrate_partitioning_strategies(spark):
    """Show different partitioning strategies."""
    print("\n" + "=" * 70)
    print("3. PARTITIONING STRATEGIES")
    print("=" * 70)

    # Create dataset with categories
    data = [(i, f"category_{i % 10}", i * 100) for i in range(1, 10001)]
    df = spark.createDataFrame(data, ["id", "category", "amount"])

    # Strategy 1: Random partitioning
    print("\nğŸ“Š Strategy 1: Random Partitioning")
    df_random = df.repartition(4)
    print(f"   Partitions: {df_random.rdd.getNumPartitions()}")
    print("   âœ… Good for: General balanced distribution")
    print("   âŒ Bad for: Operations on specific keys")

    # Strategy 2: Hash partitioning by key
    print("\nï¿½ï¿½ Strategy 2: Hash Partitioning by Key")
    df_hash = df.repartition(4, "category")
    print(f"   Partitions: {df_hash.rdd.getNumPartitions()}")
    print("   âœ… Good for: Group-by, joins on same key")
    print("   âœ… Same category always goes to same partition")

    # Show distribution
    partition_dist = (
        df_hash.withColumn("partition_id", spark_partition_id())
        .groupBy("partition_id", "category")
        .agg(count("*").alias("count"))
    )

    print("\n   Distribution by partition and category:")
    partition_dist.orderBy("partition_id", "category").show(20)

    # Strategy 3: Range partitioning
    print("\nğŸ“Š Strategy 3: Range Partitioning")
    df_range = df.repartitionByRange(4, "amount")
    print(f"   Partitions: {df_range.rdd.getNumPartitions()}")
    print("   âœ… Good for: Sorting, range queries")
    print("   âœ… Sorted data distribution")


def demonstrate_partition_best_practices(spark):
    """Best practices for production."""
    print("\n" + "=" * 70)
    print("4. PARTITION BEST PRACTICES")
    print("=" * 70)

    # Create large dataset
    df = spark.range(1, 1000001)

    # Practice 1: Right-size partitions
    print("\nğŸ’¡ Practice 1: Right-Size Partitions")
    print("   Rule: 128MB - 256MB per partition")
    print("   Formula: num_partitions = data_size_MB / 256")
    print("   Example: 10GB data â†’ 10,000MB / 256 â‰ˆ 40 partitions")

    # Practice 2: Partition before expensive operations
    print("\nğŸ’¡ Practice 2: Partition Before Expensive Operations")

    # Bad: Join without partitioning
    df1 = spark.range(1, 100001).withColumnRenamed("id", "key")
    df2 = spark.range(1, 50001).withColumnRenamed("id", "key")

    print("\n   âŒ Bad: Direct join (unbalanced):")
    start = time.time()
    result_bad = df1.join(df2, "key")
    result_bad.count()
    bad_time = time.time() - start
    print(f"      Time: {bad_time:.3f}s")

    # Good: Partition before join
    print("\n   âœ… Good: Partition before join:")
    start = time.time()
    df1_partitioned = df1.repartition(4, "key")
    df2_partitioned = df2.repartition(4, "key")
    result_good = df1_partitioned.join(df2_partitioned, "key")
    result_good.count()
    good_time = time.time() - start
    print(f"      Time: {good_time:.3f}s")
    print(f"      Speedup: {bad_time / good_time:.2f}x")

    # Practice 3: Cache after repartitioning
    print("\nğŸ’¡ Practice 3: Cache After Repartitioning")
    print("   If using DataFrame multiple times:")
    df_partitioned = df.repartition(8)
    df_partitioned.cache()
    df_partitioned.count()  # Materialize cache
    print("   âœ… Cached partitioned data")
    print("   âœ… Future operations use cached partitions")


def demonstrate_skew_handling(spark):
    """Handle data skew with salting."""
    print("\n" + "=" * 70)
    print("5. HANDLING DATA SKEW")
    print("=" * 70)

    # Create skewed dataset (90% of data in one category)
    skewed_data = []
    for i in range(1, 10001):
        if i < 9000:
            category = "popular"  # 90% of data
        else:
            category = f"cat_{i % 10}"
        skewed_data.append((i, category, i * 100))

    df_skewed = spark.createDataFrame(skewed_data, ["id", "category", "amount"])

    print("ğŸ“Š Skewed Dataset:")
    df_skewed.groupBy("category").count().orderBy(col("count").desc()).show(5)

    # Problem: One partition gets 90% of data
    print("\nâŒ Problem: Unbalanced partitions")
    df_skewed_partitioned = df_skewed.repartition(4, "category")
    partition_sizes = (
        df_skewed_partitioned.withColumn("partition_id", spark_partition_id())
        .groupBy("partition_id")
        .count()
    )
    print("   Partition sizes:")
    partition_sizes.orderBy("partition_id").show()

    # Solution: Salting
    print("\nâœ… Solution: Salting Technique")
    from pyspark.sql.functions import concat, lit, rand

    # SALTING EXPLAINED:
    # ==================
    # Problem: "popular" category has 90% of data â†’ One partition is overloaded
    # Solution: Split the hot key into multiple keys using random "salt"
    #
    # How it works:
    # 1. rand() generates random number between 0.0 and 1.0 for each row
    # 2. Multiply by N (here 4) to get range [0.0, 4.0)
    # 3. Cast to int to get discrete values: 0, 1, 2, 3
    # 4. Append salt to key: "popular" becomes "popular_0", "popular_1", "popular_2", "popular_3"
    # 5. Now "popular" is split across 4 partitions instead of 1!
    #
    # Example transformation:
    #   Original:  category="popular" (90% of data in one partition)
    #   Salted:    "popular_0", "popular_1", "popular_2", "popular_3"
    #   Result:    Each partition gets ~22.5% of data (90% / 4)
    #
    # Why rand() works:
    # - Random distribution ensures even split across salt values
    # - Each row independently gets random salt (0-3)
    # - Over large dataset, ~25% of rows get each salt value
    # - Hash partitioning on salted key distributes evenly
    #
    # Trade-off:
    # - Pros: Balanced partitions, better parallelism, faster execution
    # - Cons: Need extra aggregation step to remove salt later (if needed)
    #
    # When to use:
    # - Skewed groupBy (one key has majority of data)
    # - Skewed joins (hot keys cause stragglers)
    # - Uneven partition sizes visible in Spark UI
    df_salted = df_skewed.withColumn(
        "salted_category", concat(col("category"), lit("_"), (rand() * 4).cast("int"))
    )

    df_salted_partitioned = df_salted.repartition(4, "salted_category")
    salted_partition_sizes = (
        df_salted_partitioned.withColumn("partition_id", spark_partition_id())
        .groupBy("partition_id")
        .count()
    )

    print("   Salted partition sizes:")
    salted_partition_sizes.orderBy("partition_id").show()
    print("   âœ… More balanced distribution!")


def main():
    spark = create_spark()

    print("ğŸ”§ DATA PARTITIONING STRATEGIES")
    print("=" * 70)

    demonstrate_default_partitioning(spark)
    demonstrate_repartition_vs_coalesce(spark)
    demonstrate_partitioning_strategies(spark)
    demonstrate_partition_best_practices(spark)
    demonstrate_skew_handling(spark)

    print("\n" + "=" * 70)
    print("âœ… PARTITIONING DEMO COMPLETE!")
    print("=" * 70)
    print("\nğŸ“ Key Takeaways:")
    print("   1. repartition(): Full shuffle, increases/decreases partitions")
    print("   2. coalesce(): No shuffle, only decreases partitions")
    print("   3. Partition on join key for better performance")
    print("   4. Target 128-256MB per partition")
    print("   5. Use salting to handle data skew")
    print("   6. Cache after expensive repartitioning")

    spark.stop()


if __name__ == "__main__":
    main()
