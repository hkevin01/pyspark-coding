#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
FAULT TOLERANCE - Resilience, Recovery, and Checkpointing in Production
================================================================================

MODULE OVERVIEW:
----------------
Fault tolerance is Spark's superpower - the ability to automatically recover
from failures without losing work. In distributed systems, failures are not
edge cases but normal operations: executors crash, network partitions occur,
spot instances get reclaimed, and disks fill up. Spark handles all of this
through lineage-based recovery and checkpointing.

This module provides a comprehensive guide to:
1. How Spark achieves fault tolerance through RDD lineage
2. Persistence levels and their trade-offs
3. Checkpointing to truncate long lineage
4. Recovery strategies and optimization
5. Best practices for production resilience
6. Cost-performance trade-offs

PURPOSE:
--------
Learn to:
â€¢ Understand how lineage enables automatic recovery
â€¢ Choose appropriate persistence strategies (cache vs persist)
â€¢ Use checkpointing to optimize recovery time
â€¢ Handle executor failures gracefully
â€¢ Minimize recomputation costs
â€¢ Build resilient production pipelines

FAULT TOLERANCE FUNDAMENTALS:
------------------------------

How Spark Achieves Fault Tolerance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RDD LINEAGE GRAPH                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Data                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚   HDFS     â”‚                                                 â”‚
â”‚  â”‚ (Durable)  â”‚                                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚        â”‚ read()                                                 â”‚
â”‚        â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚    RDD1     â”‚ â† Lineage: read("hdfs://data")                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚        â”‚ map(x => x * 2)                                       â”‚
â”‚        â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚    RDD2     â”‚ â† Lineage: RDD1.map(x => x * 2)               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚        â”‚ filter(x => x > 100)                                  â”‚
â”‚        â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚    RDD3     â”‚ â† Lineage: RDD2.filter(x => x > 100)          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                 â”‚
â”‚  If RDD3 partition lost:                                       â”‚
â”‚  1. Check lineage: RDD2.filter(x => x > 100)                   â”‚
â”‚  2. Check lineage: RDD1.map(x => x * 2)                        â”‚
â”‚  3. Recompute from source: read("hdfs://data")                 â”‚
â”‚  4. Replay transformations: map â†’ filter                       â”‚
â”‚  5. Recovery complete! âœ…                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Concepts:

1. Lineage (DAG):
   â€¢ Spark tracks the sequence of transformations (lineage)
   â€¢ Forms a Directed Acyclic Graph (DAG)
   â€¢ Enables recomputation of lost partitions
   
2. Lazy Evaluation:
   â€¢ Transformations are not executed immediately
   â€¢ Spark builds execution plan
   â€¢ Optimizes before execution
   
3. Recomputation:
   â€¢ Lost data recomputed from source
   â€¢ Only affected partitions recomputed (not entire dataset)
   â€¢ Automatic and transparent

LINEAGE VISUALIZATION:
----------------------

Example: WordCount Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  textFile â”€â”€â–º map â”€â”€â–º flatMap â”€â”€â–º map â”€â”€â–º reduceByKey          â”‚
â”‚     â”‚          â”‚        â”‚          â”‚           â”‚                â”‚
â”‚   (HDFS)    (split)  (words)   (word,1)    (word,count)        â”‚
â”‚                                                                 â”‚
â”‚  Narrow Dependencies: â”€â”€â–º  (No shuffle, partition preserved)   â”‚
â”‚  Wide Dependencies:  â•â•â•â–º  (Shuffle, all-to-all communication) â”‚
â”‚                                                                 â”‚
â”‚  If partition lost at reduceByKey:                             â”‚
â”‚  â€¢ Recompute from previous shuffle boundary                    â”‚
â”‚  â€¢ Don't need to re-read from HDFS (shuffle data cached)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Transformation Types:

Narrow Transformations (Fast Recovery):
â€¢ map, filter, union, mapPartitions
â€¢ Each parent partition contributes to at most one child partition
â€¢ Lost partition recomputed from single parent partition
â€¢ Example: If partition 3 lost, only recompute from parent partition 3

Wide Transformations (Expensive Recovery):
â€¢ groupBy, join, reduce, distinct, repartition
â€¢ Each parent partition contributes to multiple child partitions
â€¢ Lost partition may require recomputing multiple parent partitions
â€¢ Example: If partition 3 lost, may need data from ALL parent partitions

PERSISTENCE STRATEGIES:
-----------------------

Storage Levels Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Storage Level         â”‚ Memory     â”‚ Disk   â”‚ Serializ â”‚ Recovery â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NONE (default)        â”‚ âŒ         â”‚ âŒ     â”‚ âŒ       â”‚ Slowest  â”‚
â”‚ Recompute every time  â”‚            â”‚        â”‚          â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MEMORY_ONLY           â”‚ âœ…         â”‚ âŒ     â”‚ âŒ       â”‚ Fastest  â”‚
â”‚ Deserialized objects  â”‚ Highest    â”‚        â”‚          â”‚ Risky    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MEMORY_AND_DISK       â”‚ âœ…         â”‚ âœ…     â”‚ âŒ       â”‚ Fast     â”‚
â”‚ Spill to disk         â”‚ High       â”‚ Backup â”‚          â”‚ Safe     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MEMORY_ONLY_SER       â”‚ âœ…         â”‚ âŒ     â”‚ âœ…       â”‚ Medium   â”‚
â”‚ Serialized (compact)  â”‚ Medium     â”‚        â”‚ CPU cost â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MEMORY_AND_DISK_SER   â”‚ âœ…         â”‚ âœ…     â”‚ âœ…       â”‚ Medium   â”‚
â”‚ Serialized + spill    â”‚ Medium     â”‚ Backup â”‚ CPU cost â”‚ Safe     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DISK_ONLY             â”‚ âŒ         â”‚ âœ…     â”‚ âœ…       â”‚ Slow     â”‚
â”‚ Only on disk          â”‚            â”‚ All    â”‚          â”‚ Durable  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OFF_HEAP              â”‚ Off-heap   â”‚ âŒ     â”‚ âœ…       â”‚ Fast     â”‚
â”‚ Tachyon/Alluxio       â”‚ (external) â”‚        â”‚          â”‚ Advanced â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When to Use Each:

MEMORY_ONLY:
âœ… Use when:
â€¢ Dataset fits in cluster memory
â€¢ Performance is critical
â€¢ Interactive queries (notebooks)
âŒ Avoid when:
â€¢ Dataset larger than memory (eviction thrashing)
â€¢ Production jobs (risk of data loss)

MEMORY_AND_DISK (Recommended for Production):
âœ… Use when:
â€¢ Dataset may exceed memory
â€¢ Need reliability
â€¢ Production jobs
âœ… Benefits:
â€¢ Automatic spill to disk
â€¢ No data loss
â€¢ Good performance

MEMORY_ONLY_SER:
âœ… Use when:
â€¢ Memory constrained
â€¢ Can tolerate serialization overhead
â€¢ Java/Scala (Kryo serialization efficient)
âŒ Avoid when:
â€¢ CPU is bottleneck

DISK_ONLY:
âœ… Use when:
â€¢ Very large datasets
â€¢ Memory extremely limited
â€¢ Cost optimization (smaller cluster)

Persistence API:
# cache() is alias for persist(MEMORY_ONLY)
df.cache()

# persist() with explicit level
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)

# Unpersist to free resources
df.unpersist()

CHECKPOINTING:
--------------

Why Checkpoint?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LONG LINEAGE PROBLEM                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Without Checkpointing:                                         â”‚
â”‚                                                                 â”‚
â”‚  RDD1 â†’ RDD2 â†’ RDD3 â†’ ... â†’ RDD50 â†’ RDD51                      â”‚
â”‚   â”‚                                      â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ 50 transformations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                 â”‚
â”‚  If RDD51 partition lost:                                      â”‚
â”‚  â€¢ Must recompute all 50 transformations                       â”‚
â”‚  â€¢ If any intermediate step fails, restart from beginning      â”‚
â”‚  â€¢ Long recovery time (minutes to hours)                       â”‚
â”‚  â€¢ Risk: Cascading failures                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  With Checkpointing (after RDD25):                             â”‚
â”‚                                                                 â”‚
â”‚  RDD1 â†’ RDD2 â†’ ... â†’ RDD25 â†’ [CHECKPOINT] â†’ RDD26 â†’ ... â†’ RDD51â”‚
â”‚                        â”‚                              â”‚         â”‚
â”‚                   (saved to HDFS)                     â”‚         â”‚
â”‚                                                                 â”‚
â”‚  If RDD51 partition lost:                                      â”‚
â”‚  â€¢ Start recovery from checkpoint (RDD25)                      â”‚
â”‚  â€¢ Only recompute RDD26-RDD51 (25 steps)                       â”‚
â”‚  â€¢ Recovery time cut in half! âœ…                               â”‚
â”‚  â€¢ Lineage truncated (reduced memory footprint)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Checkpointing vs Caching:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature                â”‚ Cache/Persist    â”‚ Checkpoint           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage                â”‚ Executor memory  â”‚ HDFS/S3 (reliable)   â”‚
â”‚                        â”‚ /disk            â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Durability             â”‚ Lost on executor â”‚ Durable              â”‚
â”‚                        â”‚ failure          â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Lineage                â”‚ Preserved        â”‚ Truncated âœ…         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Recovery               â”‚ From lineage     â”‚ From checkpoint file â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cost                   â”‚ Free (memory)    â”‚ Write to HDFS/S3     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Use Case               â”‚ Reuse in same jobâ”‚ Long jobs, iterative â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Checkpointing API:

# Set checkpoint directory (HDFS or S3)
spark.sparkContext.setCheckpointDir("hdfs:///checkpoint")

# Checkpoint DataFrame (truncates lineage)
df_checkpointed = df.checkpoint()
# OR for lazy checkpoint (doesn't trigger computation)
df_checkpointed = df.checkpoint(eager=False)

# Checkpoint triggers a job (writes to HDFS)
# After checkpoint, lineage is truncated

When to Checkpoint:
âœ… After 10-20 transformations
âœ… Before iterative algorithms (ML training loops)
âœ… After expensive operations (joins, aggregations)
âœ… In long-running streaming jobs
âœ… When lineage becomes very complex

âŒ Don't checkpoint:
â€¢ Every transformation (overhead)
â€¢ Short pipelines (< 5 transformations)
â€¢ When caching is sufficient

RECOVERY STRATEGIES:
--------------------

Strategy 1: Automatic Task Retry
spark.conf.set("spark.task.maxFailures", "4")  # Retry up to 4 times
â€¢ Spark automatically retries failed tasks
â€¢ Default: 4 attempts
â€¢ Exponential backoff

Strategy 2: Stage Retry
spark.conf.set("spark.stage.maxConsecutiveAttempts", "4")
â€¢ If all tasks in stage fail, retry entire stage
â€¢ Useful for transient failures (network, spot instance)

Strategy 3: Speculation
spark.conf.set("spark.speculation", "true")
spark.conf.set("spark.speculation.multiplier", "1.5")
â€¢ Launch duplicate tasks for stragglers
â€¢ If one task is 1.5x slower than median, launch backup
â€¢ First to complete wins

Strategy 4: Dynamic Executor Allocation
spark.conf.set("spark.dynamicAllocation.enabled", "true")
spark.conf.set("spark.dynamicAllocation.minExecutors", "2")
spark.conf.set("spark.dynamicAllocation.maxExecutors", "100")
â€¢ Auto-scale executors based on workload
â€¢ Replace failed executors automatically

FAILURE SCENARIOS:
------------------

Scenario 1: Executor Failure
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Executor Crashes (OOM, spot instance reclaimed)               â”‚
â”‚                                                                 â”‚
â”‚  Before:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Exec 1 â”‚ Exec 2 â”‚ Exec 3 â”‚ Exec 4 â”‚                         â”‚
â”‚  â”‚ Part 0 â”‚ Part 1 â”‚ Part 2 â”‚ Part 3 â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                        â†“                                        â”‚
â”‚                     ğŸ’¥ CRASH                                    â”‚
â”‚                                                                 â”‚
â”‚  After (Auto-recovery):                                        â”‚
â”‚  1. Driver detects executor loss                               â”‚
â”‚  2. Reschedules tasks from Exec 3 to other executors          â”‚
â”‚  3. Recompute lost partitions from lineage                     â”‚
â”‚  4. Job continues! âœ…                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scenario 2: Driver Failure (More Serious)
â€¢ Client mode: Job fails (driver on user machine)
â€¢ Cluster mode: Can recover with checkpoint (driver on cluster)
â€¢ Solution: Use cluster mode + checkpointing for production

Scenario 3: Shuffle Data Loss
â€¢ Shuffle data stored on executor disk
â€¢ If executor fails during shuffle, data lost
â€¢ Recovery: Recompute upstream tasks (expensive!)
â€¢ Mitigation: Enable shuffle service
  spark.conf.set("spark.shuffle.service.enabled", "true")

BEST PRACTICES:
---------------

âœ… Production Checklist:
â˜ Use cluster deploy mode (not client)
â˜ Enable checkpointing for long jobs
â˜ Use MEMORY_AND_DISK persistence (not MEMORY_ONLY)
â˜ Enable shuffle service (spark.shuffle.service.enabled)
â˜ Enable speculation for heterogeneous clusters
â˜ Set appropriate task retry limits
â˜ Monitor executor failures in Spark UI
â˜ Use reliable storage for checkpoints (HDFS/S3, not local disk)
â˜ Clean up checkpoint directories periodically
â˜ Test recovery by killing executors in staging

Checkpoint Strategy:
1. Checkpoint after expensive operations:
   df_joined = fact.join(dim, "key")
   df_joined = df_joined.checkpoint()  # Save expensive join result

2. Checkpoint in iterative algorithms:
   for i in range(10):
       df = df.withColumn(f"iter_{i}", ...)
       if i % 3 == 0:
           df = df.checkpoint()  # Truncate lineage every 3 iterations

3. Checkpoint in streaming:
   query = df.writeStream \\
       .option("checkpointLocation", "s3://bucket/checkpoint") \\
       .start()

Cache vs Checkpoint Decision Tree:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START                                                          â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€ Will reuse data multiple times?                          â”‚
â”‚    â”‚  â””â”€ YES â†’ Use cache() or persist()                        â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€ Lineage > 10 transformations?                            â”‚
â”‚    â”‚  â””â”€ YES â†’ Use checkpoint() to truncate                    â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€ Iterative algorithm (ML)?                                â”‚
â”‚    â”‚  â””â”€ YES â†’ Use both (cache + checkpoint every N iters)     â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€ Streaming job?                                           â”‚
â”‚    â”‚  â””â”€ YES â†’ MUST use checkpoint for fault tolerance         â”‚
â”‚    â”‚                                                            â”‚
â”‚    â””â”€ Otherwise: No caching/checkpointing needed               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COST-PERFORMANCE TRADE-OFFS:
-----------------------------

Checkpointing Costs:
â€¢ Storage: Checkpoint files stored in HDFS/S3 (pay for storage)
â€¢ Write latency: Job blocked while writing checkpoint
â€¢ Cleanup: Must manually delete old checkpoints

Example Cost Calculation:
Dataset: 100 GB
Checkpoint every hour: 100 GB Ã— 24 = 2.4 TB/day
S3 storage cost: $0.023/GB/month
Monthly cost: 2.4 TB Ã— 30 Ã— $0.023 = ~$1,650/month

Optimization:
â€¢ Checkpoint less frequently (trade-off: longer recovery)
â€¢ Compress checkpoints (Parquet with snappy)
â€¢ Clean up old checkpoints automatically

Recovery Time Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario                 â”‚ No Checkpointâ”‚ With Checkpoint      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 50-step pipeline         â”‚ Recompute 50 â”‚ Recompute 25 steps   â”‚
â”‚ (checkpoint at step 25)  â”‚ steps        â”‚                      â”‚
â”‚                          â”‚ 1 hour       â”‚ 30 min (2x faster)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ML training (10 iters)   â”‚ Restart from â”‚ Restart from last    â”‚
â”‚ (checkpoint every 3)     â”‚ iteration 1  â”‚ checkpoint (iter 6-9)â”‚
â”‚ Failure at iteration 9   â”‚ 9 hours lost â”‚ 3 hours lost (3x)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MONITORING:
-----------

Spark UI Metrics:
1. Executors Tab:
   â€¢ "Status" column: Active/Dead/Lost
   â€¢ "Failed Tasks" column: Identify problematic executors
   â€¢ "Blacklisted" column: Spark blacklists faulty nodes

2. Stages Tab:
   â€¢ "Shuffle Write" â†’ "Spill": Memory pressure
   â€¢ "Task Time": Look for long-running tasks (skew/issues)
   â€¢ "Failed Tasks": Transient errors

3. Storage Tab:
   â€¢ "Cached Partitions": Check cache effectiveness
   â€¢ "Fraction Cached": Should be 100% for cached data

4. Environment Tab:
   â€¢ Check checkpoint directory setting
   â€¢ Verify fault tolerance configs

Key Metrics to Monitor:
â€¢ Task failure rate: Should be < 1%
â€¢ Executor lost events: Investigate if frequent
â€¢ GC time: High GC = memory pressure
â€¢ Checkpoint write time: Should be predictable

TARGET AUDIENCE:
----------------
â€¢ Production engineers ensuring job reliability
â€¢ Data engineers debugging job failures
â€¢ DevOps teams managing Spark clusters
â€¢ Anyone running long-running or mission-critical jobs

RELATED RESOURCES:
------------------
â€¢ cluster_computing/01_cluster_setup.py (resource configuration)
â€¢ cluster_computing/07_resource_management.py
â€¢ Spark Configuration: https://spark.apache.org/docs/latest/configuration.html

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 2.0.0 - Comprehensive Fault Tolerance Guide
UPDATED: 2024
================================================================================
"""

import os
import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand, when


def create_spark(checkpoint_dir=None):
    """Create Spark session with optional checkpoint directory."""
    builder = (
        SparkSession.builder.appName("FaultTolerance")
        .master("local[4]")
        .config("spark.sql.shuffle.partitions", "8")
    )

    if checkpoint_dir:
        builder = builder.config(
            "spark.sql.streaming.checkpointLocation", checkpoint_dir
        )

    spark = builder.getOrCreate()

    if checkpoint_dir:
        spark.sparkContext.setCheckpointDir(checkpoint_dir)
        print(f"âœ… Checkpoint directory: {checkpoint_dir}")

    return spark


def demonstrate_lineage_basics(spark):
    """Understand RDD lineage and DAG."""
    print("=" * 70)
    print("1. LINEAGE BASICS")
    print("=" * 70)

    # Create transformation chain
    df1 = spark.range(1, 1000001).toDF("id")
    df2 = df1.withColumn("value", col("id") * 2)
    df3 = df2.filter(col("value") > 100)
    df4 = df3.withColumn("squared", col("value") * col("value"))
    df5 = df4.groupBy((col("id") % 10).alias("bucket")).count()

    print("\nğŸ“Š Transformation chain:")
    print("   1. spark.range(1000000) - Create DataFrame")
    print("   2. withColumn('value') - Transform")
    print("   3. filter(value > 100) - Filter")
    print("   4. withColumn('squared') - Transform")
    print("   5. groupBy('bucket').count() - Aggregate")

    # Explain plan
    print("\nğŸ” Execution plan:")
    df5.explain(extended=False)

    print("\nğŸ’¡ Lineage Concept:")
    print("   - Spark tracks transformation chain (lineage)")
    print("   - If data is lost, recompute from source")
    print("   - Long lineage = expensive recovery")
    print("   - Checkpointing truncates lineage")


def demonstrate_persistence_levels(spark):
    """Compare different persistence strategies."""
    print("\n" + "=" * 70)
    print("2. PERSISTENCE LEVELS")
    print("=" * 70)

    from pyspark import StorageLevel

    # Create expensive computation
    data = (
        spark.range(1, 5000001)
        .toDF("id")
        .withColumn("value1", rand() * 1000)
        .withColumn("value2", rand() * 1000)
    )

    print("ğŸ“Š Testing persistence levels:")
    print(f"   Dataset: {data.count():,} rows")

    # No caching
    print("\nâŒ No caching (recompute every time):")
    start = time.time()
    data.filter(col("value1") > 500).count()
    time1 = time.time() - start
    data.filter(col("value2") > 500).count()
    time2 = time.time() - start - time1
    print(f"   First query: {time1:.3f}s")
    print(f"   Second query: {time2:.3f}s")
    print(f"   Total: {time1 + time2:.3f}s")

    # Memory only
    print("\nâœ… MEMORY_ONLY:")
    data_mem = data.persist(StorageLevel.MEMORY_ONLY)
    data_mem.count()  # Materialize
    start = time.time()
    data_mem.filter(col("value1") > 500).count()
    time3 = time.time() - start
    data_mem.filter(col("value2") > 500).count()
    time4 = time.time() - start - time3
    print(f"   First query: {time3:.3f}s")
    print(f"   Second query: {time4:.3f}s")
    print(f"   Total: {time3 + time4:.3f}s")
    print(f"   Speedup: {(time1 + time2) / (time3 + time4):.2f}x")
    print(f"   âš ï¸  Risk: If memory full, partitions evicted")
    data_mem.unpersist()

    # Memory and disk
    print("\nâœ… MEMORY_AND_DISK:")
    data_disk = data.persist(StorageLevel.MEMORY_AND_DISK)
    data_disk.count()  # Materialize
    start = time.time()
    data_disk.filter(col("value1") > 500).count()
    time5 = time.time() - start
    data_disk.filter(col("value2") > 500).count()
    time6 = time.time() - start - time5
    print(f"   First query: {time5:.3f}s")
    print(f"   Second query: {time6:.3f}s")
    print(f"   Total: {time5 + time6:.3f}s")
    print(f"   âœ… Safer: Spills to disk if memory full")
    data_disk.unpersist()

    print("\nğŸ“Š Persistence Levels:")
    print(
        """
    MEMORY_ONLY          - Fastest, risky if memory full
    MEMORY_AND_DISK      - Safe fallback to disk
    MEMORY_ONLY_SER      - Serialized (saves memory, slower)
    MEMORY_AND_DISK_SER  - Serialized with disk fallback
    DISK_ONLY            - Only disk (slowest)
    OFF_HEAP             - Use off-heap memory (advanced)
    """
    )


def demonstrate_checkpointing(spark):
    """Demonstrate checkpointing to truncate lineage."""
    print("\n" + "=" * 70)
    print("3. CHECKPOINTING (Lineage Truncation)")
    print("=" * 70)

    # Setup checkpoint directory
    checkpoint_dir = "/tmp/spark-checkpoint"
    os.makedirs(checkpoint_dir, exist_ok=True)
    spark.sparkContext.setCheckpointDir(checkpoint_dir)

    print(f"ğŸ“ Checkpoint directory: {checkpoint_dir}")

    # Create long lineage
    print("\nğŸ“Š Building long lineage (20 transformations):")
    df = spark.range(1, 1000001).toDF("id")

    for i in range(20):
        df = df.withColumn(f"col_{i}", col("id") * (i + 1))
        if i % 5 == 0:
            print(f"   Transformation {i + 1}/20...")

    print("\nâŒ Without checkpointing:")
    start = time.time()
    result1 = df.count()
    time1 = time.time() - start
    print(f"   Count: {result1:,}")
    print(f"   Time: {time1:.3f}s")
    print(f"   âš ï¸  Long lineage: If failure, recompute all 20 steps")

    # With checkpointing
    print("\nâœ… With checkpointing after 10 transformations:")
    df = spark.range(1, 1000001).toDF("id")

    for i in range(10):
        df = df.withColumn(f"col_{i}", col("id") * (i + 1))

    # Checkpoint here (truncates lineage)
    print("   ğŸ’¾ Checkpointing at step 10...")
    df = df.checkpoint()  # Triggers computation and saves
    print("   âœ… Lineage truncated! Starting from checkpoint.")

    for i in range(10, 20):
        df = df.withColumn(f"col_{i}", col("id") * (i + 1))

    start = time.time()
    result2 = df.count()
    time2 = time.time() - start
    print(f"   Count: {result2:,}")
    print(f"   Time: {time2:.3f}s")
    print(f"   âœ… Recovery: Only recompute steps 11-20 (not 1-10)")

    print("\nğŸ’¡ When to Checkpoint:")
    print("   1. After expensive shuffles/joins")
    print("   2. Before iterative algorithms (ML)")
    print("   3. After 10+ transformations")
    print("   4. When lineage becomes complex")


def demonstrate_fault_recovery(spark):
    """Simulate fault recovery scenarios."""
    print("\n" + "=" * 70)
    print("4. FAULT RECOVERY SIMULATION")
    print("=" * 70)

    print(
        """
ğŸ“Š Scenario: Processing 1TB dataset with 100 workers

WITHOUT CHECKPOINTING:
----------------------
1. âŒ Worker 50 fails at 80% progress
2. âš ï¸  Recompute ALL partitions from source (1TB)
3. âŒ Restart from beginning
4. â±ï¸  Total time: 2x original time

WITH CHECKPOINTING (every 25%):
--------------------------------
1. âœ… Worker 50 fails at 80% progress
2. âœ… Last checkpoint at 75%
3. âœ… Recompute only 75% â†’ 80% (250GB)
4. â±ï¸  Total time: 1.05x original time (5% overhead)

Recovery Time Comparison:
-------------------------
No Checkpoint:    100% recomputation
Checkpoint 50%:   50% recomputation (avg)
Checkpoint 25%:   25% recomputation (avg)
Checkpoint 10%:   10% recomputation (avg)

âš ï¸  Trade-off: More checkpoints = more disk I/O overhead
    """
    )

    # Simulate recovery
    print("\nğŸ”„ Simulating recovery:")
    data = spark.range(1, 100001).toDF("id").withColumn("value", rand() * 1000)

    # Stage 1: Load and transform
    print("   Stage 1: Load and transform (25%)")
    stage1 = data.withColumn("transformed", col("value") * 2)

    # Stage 2: Filter and aggregate
    print("   Stage 2: Filter and aggregate (50%)")
    stage2 = (
        stage1.filter(col("transformed") > 100)
        .groupBy((col("id") % 10).alias("bucket"))
        .count()
    )

    # Checkpoint
    print("   ğŸ’¾ Checkpoint at 50% (truncate lineage)")
    stage2_checkpoint = stage2.checkpoint()

    # Stage 3: More transformations
    print("   Stage 3: Additional transformations (75%)")
    stage3 = stage2_checkpoint.withColumn("doubled", col("count") * 2)

    # Stage 4: Final aggregation
    print("   Stage 4: Final aggregation (100%)")
    result = stage3.agg({"doubled": "sum"}).collect()[0][0]

    print(f"\n   âœ… Result: {result:,.0f}")
    print("   âœ… If failure after checkpoint, only recompute stages 3-4")


def demonstrate_storage_strategies(spark):
    """Compare storage strategies for fault tolerance."""
    print("\n" + "=" * 70)
    print("5. STORAGE STRATEGIES")
    print("=" * 70)

    print(
        """
ğŸ—„ï¸  Storage Options for Checkpoints:

1. Local Disk (Development):
   âœ… Fast for testing
   âŒ Lost if node fails
   Path: file:///tmp/spark-checkpoint

2. HDFS (Hadoop Clusters):
   âœ… Replicated across nodes (3x default)
   âœ… Survives node failures
   âœ… High throughput
   Path: hdfs://namenode:8020/checkpoint

3. Amazon S3 (Cloud):
   âœ… Durable (11 9's)
   âœ… No cluster dependency
   âš ï¸  Higher latency
   Path: s3a://bucket/checkpoint

4. Azure Blob Storage:
   âœ… Durable cloud storage
   âœ… Integrated with Azure
   Path: wasbs://container@account.blob.core.windows.net/checkpoint

5. Google Cloud Storage:
   âœ… Durable cloud storage
   âœ… Integrated with GCP
   Path: gs://bucket/checkpoint

ğŸ“Š Performance Comparison:

Storage          Write Speed   Read Speed   Durability
--------         -----------   ----------   ----------
Local Disk       Fastest       Fastest      Low (single node)
HDFS             Fast          Fast         High (3x replication)
S3               Medium        Medium       Very High (11 9's)
Azure Blob       Medium        Medium       Very High
GCS              Medium        Medium       Very High

ğŸ’¡ Recommendation:
   - Development: Local disk
   - Production (on-prem): HDFS
   - Production (cloud): S3/Azure/GCS
    """
    )


def demonstrate_best_practices(spark):
    """Best practices for fault tolerance."""
    print("\n" + "=" * 70)
    print("6. FAULT TOLERANCE BEST PRACTICES")
    print("=" * 70)

    print(
        """
ğŸ¯ Checkpointing Strategy:

1. âœ… Checkpoint after expensive operations
   - Large shuffles (join, groupBy)
   - Complex aggregations
   - 10+ sequential transformations

2. âœ… Choose right checkpoint frequency
   Checkpoint every:    Recovery cost:
   ------------------   ---------------
   Never                100% recompute
   End only             50% recompute (avg)
   2 checkpoints        33% recompute (avg)
   4 checkpoints        20% recompute (avg)

3. âœ… Use durable storage in production
   # Development
   sc.setCheckpointDir("file:///tmp/checkpoint")
   
   # Production (HDFS)
   sc.setCheckpointDir("hdfs://namenode:8020/checkpoint")
   
   # Production (S3)
   sc.setCheckpointDir("s3a://bucket/checkpoint")

4. âœ… Clean up old checkpoints
   # Streaming: automatic cleanup
   spark.conf.set("spark.cleaner.referenceTracking.cleanCheckpoints", "true")
   
   # Batch: manual cleanup with retention
   # Keep last 7 days of checkpoints

5. âœ… Combine caching and checkpointing
   df.cache()       # Fast repeated access
   df.checkpoint()  # Truncate lineage, durable storage

âš ï¸  Common Mistakes:

1. âŒ Over-checkpointing
   Too frequent = excessive disk I/O overhead
   
2. âŒ No checkpointing for iterative algorithms
   ML training (100+ iterations) needs checkpoints
   
3. âŒ Using local disk in production
   Node failure loses checkpoint data
   
4. âŒ Not cleaning up checkpoints
   Fills up disk over time
   
5. âŒ Checkpointing small datasets
   Overhead > benefit for < 1GB data

ğŸ“Š Decision Tree:

                    Start
                      |
           Is lineage complex (>10 steps)?
          /                                \\
        No                                 Yes
        |                                   |
   Skip checkpoint              Is data > 1GB?
                              /                \\
                            No                 Yes
                            |                   |
                       Use cache()        Use checkpoint()
                                               |
                                    Which storage?
                                    /      |      \\
                                 Dev    HDFS    Cloud
                                  |       |       |
                              Local   Hadoop   S3/Azure

ğŸ”§ Configuration:

# Enable checkpoint cleanup
spark.conf.set("spark.cleaner.referenceTracking.cleanCheckpoints", "true")

# Checkpoint interval for streaming
spark.conf.set("spark.streaming.checkpoint.interval", "10s")

# Reliable checkpointing (write twice for durability)
spark.conf.set("spark.checkpoint.compress", "true")
    """
    )


def main():
    # Create checkpoint directory
    checkpoint_dir = "/tmp/spark-checkpoint-demo"
    os.makedirs(checkpoint_dir, exist_ok=True)

    spark = create_spark(checkpoint_dir)

    print("ğŸ’¾ FAULT TOLERANCE & CHECKPOINTING")
    print("=" * 70)

    demonstrate_lineage_basics(spark)
    demonstrate_persistence_levels(spark)
    demonstrate_checkpointing(spark)
    demonstrate_fault_recovery(spark)
    demonstrate_storage_strategies(spark)
    demonstrate_best_practices(spark)

    print("\n" + "=" * 70)
    print("âœ… FAULT TOLERANCE DEMO COMPLETE!")
    print("=" * 70)
    print("\nğŸ“ Key Takeaways:")
    print("   1. Checkpointing truncates lineage for faster recovery")
    print("   2. Checkpoint after expensive shuffles (>10 transformations)")
    print("   3. Use HDFS/S3 for production (not local disk)")
    print("   4. Balance checkpoint frequency vs overhead")
    print("   5. Cache for speed, checkpoint for fault tolerance")
    print("   6. Enable automatic checkpoint cleanup")
    print(f"\nğŸ“ Checkpoint directory: {checkpoint_dir}")

    spark.stop()


if __name__ == "__main__":
    main()
