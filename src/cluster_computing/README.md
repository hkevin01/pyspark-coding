# Cluster Computing with PySpark ğŸ–¥ï¸âš¡

This folder contains examples of using **PySpark on distributed clusters** to process massive datasets across multiple computers.

## ğŸ“š What is Cluster Computing?

**Cluster computing** means distributing computation across multiple machines (nodes) to process data that's too large for a single computer. PySpark orchestrates this distribution automatically.

### Key Concepts

1. **Driver Node**: The main program that coordinates work
2. **Worker Nodes**: Machines that execute tasks in parallel
3. **Executors**: Processes on worker nodes that run computations
4. **Partitions**: Data splits distributed across executors
5. **Shuffling**: Data movement between nodes during operations

---

## ğŸ—ï¸ Cluster Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DRIVER NODE                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SparkContext â”‚ â”€â”€â”€â”€â–¶   â”‚   Cluster Manager       â”‚  â”‚
â”‚  â”‚  (Your Code) â”‚         â”‚  (YARN/Mesos/K8s/Local) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKER 1    â”‚ â”‚ WORKER 2    â”‚ â”‚ WORKER 3    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Executor â”‚ â”‚ â”‚ â”‚Executor â”‚ â”‚ â”‚ â”‚Executor â”‚ â”‚
â”‚ â”‚ Core 1  â”‚ â”‚ â”‚ â”‚ Core 1  â”‚ â”‚ â”‚ â”‚ Core 1  â”‚ â”‚
â”‚ â”‚ Core 2  â”‚ â”‚ â”‚ â”‚ Core 2  â”‚ â”‚ â”‚ â”‚ Core 2  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  Partition  â”‚ â”‚  Partition  â”‚ â”‚  Partition  â”‚
â”‚   1, 2, 3   â”‚ â”‚   4, 5, 6   â”‚ â”‚   7, 8, 9   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Examples in This Folder

### 1. **01_cluster_setup.py**
- Configure Spark for cluster mode
- Set executor memory, cores, and parallelism
- Different cluster managers (YARN, Kubernetes, Standalone)

### 2. **02_data_partitioning.py**
- Understand partitioning strategies
- Repartition vs coalesce
- Optimize for cluster performance

### 3. **03_distributed_joins.py**
- Join large datasets across nodes
- Broadcast joins for small tables
- Shuffle optimization

### 4. **04_aggregations_at_scale.py**
- Aggregate billions of rows
- Window functions across partitions
- Group-by optimization

### 5. **05_fault_tolerance.py**
- Checkpointing for lineage truncation
- Handle node failures
- Data recovery strategies

### 6. **06_gpu_accelerated_udfs.py** âœ¨
- Use GPUs on each cluster node
- PyTorch and TensorFlow GPU UDFs
- Distributed deep learning inference
- Batch processing optimization
- Multi-GPU strategies

### 7. **07_resource_management.py**
- Dynamic allocation
- Memory tuning (executor, driver, overhead)
- CPU core allocation

### 8. **08_shuffle_optimization.py**
- Minimize shuffling
- Partition keys for joins
- Broadcast variables

### 9. **09_cluster_monitoring.py**
- Spark UI metrics
- Track executor utilization
- Identify bottlenecks

---

## ğŸš€ Quick Start

### Local Cluster Simulation (Testing)

```python
from pyspark.sql import SparkSession

# Simulate 3-node cluster locally
spark = SparkSession.builder \
    .appName("LocalClusterTest") \
    .master("local[3]")  # 3 cores = 3 "workers"
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

# Your code here
df = spark.read.csv("large_file.csv")
result = df.groupBy("category").count()
result.show()
```

### Real Cluster (YARN/Kubernetes)

```bash
# Submit to YARN cluster
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --num-executors 10 \
    --executor-cores 4 \
    --executor-memory 8g \
    --driver-memory 4g \
    your_script.py

# Submit to Kubernetes
spark-submit \
    --master k8s://https://kubernetes-api:443 \
    --deploy-mode cluster \
    --conf spark.executor.instances=10 \
    --conf spark.kubernetes.container.image=spark:3.5.0 \
    your_script.py
```

---

## ğŸ”§ Cluster Configuration Cheat Sheet

### Memory Configuration

```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g")           # Driver node memory
    .config("spark.executor.memory", "8g")         # Each executor memory
    .config("spark.executor.memoryOverhead", "2g") # Off-heap memory
    .config("spark.memory.fraction", "0.8")        # Heap for execution/storage
    .getOrCreate()
```

### Parallelism Configuration

```python
spark = SparkSession.builder \
    .config("spark.default.parallelism", "200")        # RDD operations
    .config("spark.sql.shuffle.partitions", "200")     # DataFrame shuffles
    .config("spark.executor.cores", "4")               # Cores per executor
    .config("spark.executor.instances", "10")          # Number of executors
    .getOrCreate()
```

### Network Configuration

```python
spark = SparkSession.builder \
    .config("spark.network.timeout", "800s")           # Network timeout
    .config("spark.shuffle.compress", "true")          # Compress shuffle data
    .config("spark.shuffle.file.buffer", "1m")         # Shuffle buffer size
    .config("spark.reducer.maxSizeInFlight", "96m")    # Reduce fetch size
    .getOrCreate()
```

---

## ğŸ“Š Performance Guidelines

### Data Size â†’ Cluster Size

| Data Size | Executors | Cores/Executor | Memory/Executor | Total Cores |
|-----------|-----------|----------------|-----------------|-------------|
| 10 GB     | 2-3       | 2-4            | 4-8 GB          | 4-12        |
| 100 GB    | 5-10      | 4-8            | 8-16 GB         | 20-80       |
| 1 TB      | 20-50     | 4-8            | 16-32 GB        | 80-400      |
| 10 TB     | 100-200   | 4-8            | 32-64 GB        | 400-1600    |

### Partition Guidelines

```python
# Rule of thumb: 2-4 partitions per CPU core
num_partitions = num_executors * cores_per_executor * 2

# For 10 executors with 4 cores each:
# 10 * 4 * 2 = 80 partitions
df = df.repartition(80)
```

---

## ğŸ¯ Best Practices

### âœ… DO

1. **Partition your data properly**
   ```python
   # Good: Balanced partitions
   df = df.repartition(num_partitions, "key_column")
   ```

2. **Use broadcast joins for small tables**
   ```python
   from pyspark.sql.functions import broadcast
   result = large_df.join(broadcast(small_df), "id")
   ```

3. **Cache frequently-used DataFrames**
   ```python
   df.cache()
   df.count()  # Materialize cache
   ```

4. **Enable dynamic allocation**
   ```python
   spark.conf.set("spark.dynamicAllocation.enabled", "true")
   spark.conf.set("spark.dynamicAllocation.minExecutors", "2")
   spark.conf.set("spark.dynamicAllocation.maxExecutors", "20")
   ```

5. **Monitor and tune**
   - Check Spark UI (http://driver:4040)
   - Look for data skew
   - Identify slow stages

### âŒ DON'T

1. **Don't use too many small partitions** (overhead)
2. **Don't ignore data skew** (some partitions much larger)
3. **Don't cache everything** (limited memory)
4. **Don't forget to unpersist** when done
5. **Don't use collect() on large data** (OOM on driver)

---

## ğŸ” Troubleshooting

### Problem: Out of Memory

**Symptoms**: `java.lang.OutOfMemoryError`

**Solutions**:
```python
# Increase executor memory
.config("spark.executor.memory", "16g")
.config("spark.executor.memoryOverhead", "4g")

# Increase partitions (smaller per-partition data)
df = df.repartition(400)

# Avoid wide transformations
df.cache()  # Cache before shuffle
```

### Problem: Slow Shuffles

**Symptoms**: Long shuffle read/write times in Spark UI

**Solutions**:
```python
# Use broadcast for small tables
broadcast(small_df)

# Increase shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", "400")

# Repartition on join key before join
df1 = df1.repartition("join_key")
df2 = df2.repartition("join_key")
result = df1.join(df2, "join_key")
```

### Problem: Data Skew

**Symptoms**: One task takes much longer than others

**Solutions**:
```python
# Salting technique: Add random suffix to skewed key
from pyspark.sql.functions import concat, lit, rand

df_salted = df.withColumn("salted_key", 
    concat(col("skewed_key"), lit("_"), (rand() * 10).cast("int")))
```

---

## ğŸ“ˆ Monitoring Commands

### Check Cluster Resources

```bash
# YARN cluster
yarn application -list
yarn application -status application_id

# Kubernetes cluster
kubectl get pods -n spark
kubectl logs spark-driver-pod
```

### Spark UI Metrics to Watch

- **Executor Tab**: Memory usage, active tasks
- **Stages Tab**: Shuffle read/write, task time
- **Storage Tab**: Cached RDDs/DataFrames
- **SQL Tab**: Query plans, physical plans

---

## ğŸ“ Interview Questions

### Q1: "How does PySpark distribute data across a cluster?"

**Answer**: PySpark partitions data into chunks and distributes them across executor processes on worker nodes. Each partition is processed independently in parallel. The number of partitions is controlled by `spark.default.parallelism` (RDDs) or `spark.sql.shuffle.partitions` (DataFrames).

### Q2: "What's the difference between repartition and coalesce?"

**Answer**: 
- `repartition(N)`: Can increase/decrease partitions, triggers full shuffle
- `coalesce(N)`: Only decreases partitions, avoids shuffle by merging adjacent partitions
- Use `repartition` to increase parallelism, `coalesce` to reduce output files

### Q3: "How do you optimize joins on large tables?"

**Answer**:
1. **Broadcast small tables**: `broadcast(small_df)` sends to all executors
2. **Partition on join key**: Pre-partition both tables on the join column
3. **Filter early**: Reduce data before join
4. **Bucketing**: Pre-bucket tables for repeated joins

### Q4: "Explain Spark's shuffle operation"

**Answer**: Shuffle redistributes data across partitions, typically for joins, groupBy, or repartition. It involves:
1. **Map side**: Each executor writes shuffle data to disk
2. **Network transfer**: Data sent to appropriate executors
3. **Reduce side**: Executors read and process received data

Shuffles are expensive (disk I/O, network, serialization). Minimize by using broadcast joins and reducing partition count.

---

## ğŸ“š Resources

- [Apache Spark Cluster Mode Overview](https://spark.apache.org/docs/latest/cluster-overview.html)
- [Spark Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)
- [Tuning Spark Applications](https://spark.apache.org/docs/latest/tuning.html)

---

**Next Steps**: Run the examples to see cluster computing in action!
