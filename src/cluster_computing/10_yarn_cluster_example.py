"""
================================================================================
CLUSTER COMPUTING #10 - YARN Cluster Deployment and Management
================================================================================

MODULE OVERVIEW:
----------------
YARN (Yet Another Resource Negotiator) is Hadoop's cluster resource manager
and the most common deployment platform for enterprise Spark applications.
YARN provides multi-tenancy, resource isolation, and battle-tested reliability.

This module demonstrates production YARN deployments with real ETL workloads,
dynamic allocation, resource management, and comprehensive monitoring.

PURPOSE:
--------
Master YARN cluster deployments:
â€¢ YARN architecture and components
â€¢ Production-ready configuration
â€¢ Dynamic resource allocation
â€¢ Queue management and priority
â€¢ Real ETL pipeline deployment
â€¢ Monitoring and debugging techniques
â€¢ Best practices and optimization

YARN ARCHITECTURE:
------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YARN CLUSTER ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Client (spark-submit)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Submit Spark application                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ RESOURCE MANAGER (Master - Port 8088)                   â”‚   â”‚
â”‚  â”‚ â€¢ Accepts applications                                  â”‚   â”‚
â”‚  â”‚ â€¢ Allocates resources                                   â”‚   â”‚
â”‚  â”‚ â€¢ Monitors applications                                 â”‚   â”‚
â”‚  â”‚ â€¢ Manages queues                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ APPLICATION MASTER (Per-app - on worker node)           â”‚   â”‚
â”‚  â”‚ â€¢ Spark driver                                          â”‚   â”‚
â”‚  â”‚ â€¢ Negotiates resources with RM                          â”‚   â”‚
â”‚  â”‚ â€¢ Monitors executors                                    â”‚   â”‚
â”‚  â”‚ â€¢ Restarts failed tasks                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ NODE MANAGER â”‚ NODE MANAGER â”‚ NODE MANAGER â”‚               â”‚
â”‚  â”‚ (Worker 1)   â”‚ (Worker 2)   â”‚ (Worker 3)   â”‚               â”‚
â”‚  â”‚              â”‚              â”‚              â”‚               â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚               â”‚
â”‚  â”‚ â”‚Container â”‚ â”‚ â”‚Container â”‚ â”‚ â”‚Container â”‚ â”‚               â”‚
â”‚  â”‚ â”‚Executor 1â”‚ â”‚ â”‚Executor 2â”‚ â”‚ â”‚Executor 3â”‚ â”‚               â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚               â”‚
â”‚  â”‚              â”‚              â”‚              â”‚               â”‚
â”‚  â”‚ Port: 8042   â”‚ Port: 8042   â”‚ Port: 8042   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â”‚  Key URLs:                                                      â”‚
â”‚  â€¢ Resource Manager: http://rm-host:8088                       â”‚
â”‚  â€¢ Node Manager: http://nm-host:8042                           â”‚
â”‚  â€¢ Application UI: http://driver-host:4040                     â”‚
â”‚  â€¢ History Server: http://history-host:18080                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YARN VS OTHER CLUSTER MANAGERS:
--------------------------------

Feature           â”‚ YARN      â”‚ Kubernetes â”‚ Standalone â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Maturity          â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜…â˜†     â”‚ â˜…â˜…â˜…â˜†â˜†     â”‚
Multi-tenancy     â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜…â˜†     â”‚ â˜…â˜…â˜†â˜†â˜†     â”‚
Resource isolationâ”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜…â˜…     â”‚ â˜…â˜…â˜…â˜†â˜†     â”‚
Dynamic allocationâ”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜…â˜†     â”‚ â˜…â˜…â˜…â˜†â˜†     â”‚
Ease of setup     â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜†â˜†     â”‚ â˜…â˜…â˜…â˜…â˜…     â”‚
Cloud native      â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜…â˜…     â”‚ â˜…â˜…â˜…â˜†â˜†     â”‚
Hadoop ecosystem  â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜†â˜†â˜†     â”‚ â˜…â˜…â˜†â˜†â˜†     â”‚

Best for: Enterprise Hadoop clusters, on-premises deployments

CONFIGURATION BREAKDOWN:
------------------------

1. **Resource Manager Configuration** (yarn-site.xml):
```xml
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>rm-master.company.com</value>
</property>

<property>
  <name>yarn.resourcemanager.address</name>
  <value>rm-master.company.com:8032</value>
</property>

<property>
  <name>yarn.resourcemanager.scheduler.class</name>
  <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<property>
  <name>yarn.nodemanager.resource.memory-mb</name>
  <value>65536</value>  <!-- 64 GB per node -->
</property>

<property>
  <name>yarn.nodemanager.resource.cpu-vcores</name>
  <value>16</value>  <!-- 16 cores per node -->
</property>

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>spark_shuffle</value>  <!-- Required for dynamic allocation -->
</property>
```

2. **Queue Configuration** (capacity-scheduler.xml):
```xml
<!-- Production queue: 60% capacity -->
<property>
  <name>yarn.scheduler.capacity.root.production.capacity</name>
  <value>60</value>
</property>

<!-- Development queue: 30% capacity -->
<property>
  <name>yarn.scheduler.capacity.root.development.capacity</name>
  <value>30</value>
</property>

<!-- Ad-hoc queue: 10% capacity -->
<property>
  <name>yarn.scheduler.capacity.root.adhoc.capacity</name>
  <value>10</value>
</property>
```

SPARK-SUBMIT COMMAND:
---------------------

Full production spark-submit command:

```bash
spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --name \"Production ETL Pipeline\" \\
  --queue production \\
  --num-executors 20 \\
  --executor-cores 5 \\
  --executor-memory 16g \\
  --driver-memory 8g \\
  --driver-cores 2 \\
  --conf spark.executor.memoryOverhead=3g \\
  --conf spark.driver.memoryOverhead=2g \\
  --conf spark.dynamicAllocation.enabled=true \\
  --conf spark.dynamicAllocation.minExecutors=5 \\
  --conf spark.dynamicAllocation.maxExecutors=50 \\
  --conf spark.dynamicAllocation.initialExecutors=10 \\
  --conf spark.shuffle.service.enabled=true \\
  --conf spark.sql.adaptive.enabled=true \\
  --conf spark.yarn.maxAppAttempts=2 \\
  --conf spark.yarn.am.memory=2g \\
  --conf spark.yarn.am.cores=1 \\
  --conf spark.yarn.submit.waitAppCompletion=true \\
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\
  --conf spark.network.timeout=600s \\
  --files hdfs:///configs/app.conf \\
  --py-files dependencies.zip \\
  --archives python_env.tar.gz#env \\
  production_etl.py
```

Parameter Explanations:
-----------------------

**--deploy-mode cluster**:
â€¢ Runs driver on worker node (not client)
â€¢ More stable for long-running jobs
â€¢ Driver failure doesn't kill client
â€¢ Use 'client' mode for interactive shells

**--queue production**:
â€¢ YARN queue for resource allocation
â€¢ Controls priority and capacity
â€¢ Multiple queues enable multi-tenancy

**Dynamic Allocation**:
â€¢ minExecutors=5: Always keep 5 executors
â€¢ maxExecutors=50: Scale up to 50 executors
â€¢ initialExecutors=10: Start with 10 executors
â€¢ executorIdleTimeout=60s: Remove after 60s idle

**Memory Configuration**:
â€¢ executor.memory: JVM heap for executor
â€¢ executor.memoryOverhead: Off-heap memory (network, shuffle)
â€¢ Rule: overhead = 10% of executor.memory (min 384MB)
â€¢ Total container = memory + memoryOverhead

DYNAMIC ALLOCATION DEEP DIVE:
------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DYNAMIC ALLOCATION TIMELINE (YARN)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Time: 0s (Job Start)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ E1  â”‚ E2  â”‚ E3  â”‚ E4  â”‚ E5  â”‚ E6  â”‚ E7  â”‚ E8  â”‚ E9  â”‚ E10 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Initial executors: 10                                          â”‚
â”‚                                                                 â”‚
â”‚  Time: 30s (High Load - Pending Tasks)                         â”‚
â”‚  Request more executors from Resource Manager...               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ E1  â”‚ ... â”‚ E10 â”‚ E11 â”‚ E12 â”‚ E13 â”‚ E14 â”‚ E15 â”‚ E16 â”‚ E17 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Scale up to 17 executors                                      â”‚
â”‚                                                                 â”‚
â”‚  Time: 1m (Peak Load)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”â”‚
â”‚  â”‚ E1 â”‚ E2 â”‚...........................â”‚E48 â”‚E49 â”‚E50 â”‚      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜â”‚
â”‚  At maximum: 50 executors                                      â”‚
â”‚                                                                 â”‚
â”‚  Time: 5m (Load Decreasing)                                    â”‚
â”‚  Executors idle > 60s â†’ Release back to YARN                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ E1  â”‚ E2  â”‚ E3  â”‚ E4  â”‚ E5  â”‚ E6  â”‚ E7  â”‚ E8  â”‚ E9  â”‚ E10 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Scale down to 10 executors                                    â”‚
â”‚                                                                 â”‚
â”‚  Time: 10m (Job End - Idle)                                    â”‚
â”‚  Keep minimum executors                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚ E1  â”‚ E2  â”‚ E3  â”‚ E4  â”‚ E5  â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚  At minimum: 5 executors                                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Cost optimization (pay for what you use)
âœ… Better cluster utilization
âœ… Automatic scaling for variable workloads
âœ… No manual tuning required

Requirements:
âš ï¸  YARN shuffle service must be configured
âš ï¸  spark.shuffle.service.enabled=true
âš ï¸  yarn.nodemanager.aux-services=spark_shuffle

MONITORING AND DEBUGGING:
--------------------------

1. **Resource Manager Web UI** (http://rm-host:8088):
   - View all running applications
   - Check queue utilization
   - Monitor cluster capacity
   - View application logs

2. **Application Tracking URL**:
   - Get from: yarn application -status <app_id>
   - Access Spark UI while running
   - View stages, tasks, executors

3. **Command Line Monitoring**:
```bash
# List all applications
yarn application -list

# Get application status
yarn application -status application_1234567890_0001

# View logs
yarn logs -applicationId application_1234567890_0001

# Kill application
yarn application -kill application_1234567890_0001

# Check queue status
yarn queue -status production
```

4. **Common Issues and Solutions**:

âŒ **Problem**: Application stuck in ACCEPTED state
**Cause**: Not enough resources in queue
**Solution**:
```bash
# Check queue capacity
yarn queue -status production

# Reduce executor count or wait for resources
--num-executors 10  # instead of 50
```

âŒ **Problem**: Executors killed by NodeManager
**Cause**: Exceeded memory limits
**Solution**:
```bash
# Increase memory overhead
--conf spark.executor.memoryOverhead=4g  # was 2g

# Or reduce executor memory
--executor-memory 12g  # was 16g
```

âŒ **Problem**: Shuffle service not found
**Cause**: External shuffle service not configured
**Solution**:
```xml
<!-- In yarn-site.xml -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>spark_shuffle</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
  <value>org.apache.spark.network.yarn.YarnShuffleService</value>
</property>
```

PRODUCTION BEST PRACTICES:
---------------------------

âœ… **1. Use Cluster Deploy Mode**:
```bash
--deploy-mode cluster  # Driver on worker node
```
Benefits: Driver failure doesn't kill client, more stable

âœ… **2. Enable Dynamic Allocation**:
```bash
--conf spark.dynamicAllocation.enabled=true
--conf spark.dynamicAllocation.minExecutors=5
--conf spark.dynamicAllocation.maxExecutors=50
```
Benefits: Cost optimization, better resource utilization

âœ… **3. Set Appropriate Queue**:
```bash
--queue production  # Use dedicated queue
```
Benefits: Resource isolation, priority control

âœ… **4. Configure Memory Overhead**:
```bash
--conf spark.executor.memoryOverhead=3g
```
Rule: 10-15% of executor.memory for overhead

âœ… **5. Enable AQE (Spark 3.0+)**:
```bash
--conf spark.sql.adaptive.enabled=true
--conf spark.sql.adaptive.coalescePartitions.enabled=true
--conf spark.sql.adaptive.skewJoin.enabled=true
```
Benefits: Automatic optimization, handle skew

âœ… **6. Set Retry Attempts**:
```bash
--conf spark.yarn.maxAppAttempts=2
```
Benefits: Automatic retry on failure

âœ… **7. Use Kryo Serialization**:
```bash
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
```
Benefits: Faster serialization, smaller shuffle files

PERFORMANCE BENCHMARKS:
-----------------------

Test: Process 1 TB of data with aggregations

Configuration Impact:
```
Baseline (10 executors, static):        45 minutes
+ Dynamic allocation (5-30):            32 minutes (30% faster)
+ AQE enabled:                          24 minutes (47% faster)
+ Optimal executor sizing:              18 minutes (60% faster)
+ Kryo serialization:                   15 minutes (67% faster)
```

EXPLAIN() OUTPUT NOTES:
-----------------------
explain() doesn't show YARN-specific information.
Use Spark UI and YARN RM UI for:
â€¢ Executor allocation
â€¢ Queue utilization
â€¢ Container status
â€¢ Resource usage

Check:
â€¢ Spark UI (port 4040): Application metrics
â€¢ YARN RM (port 8088): Cluster-level view
â€¢ History Server (port 18080): Historical jobs

See Also:
---------
â€¢ 11_kubernetes_cluster_example.py - K8s deployment
â€¢ 12_standalone_cluster_example.py - Standalone cluster
â€¢ 07_resource_management.py - Resource tuning
â€¢ 09_cluster_monitoring.py - Monitoring guide
"""

import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, col, count
from pyspark.sql.functions import sum as _sum
from pyspark.sql.functions import when
from pyspark.sql.types import (
    DoubleType,
    IntegerType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)


def create_yarn_spark_session():
    """
    Create SparkSession configured for YARN cluster.

    YARN Components:
    - Resource Manager: Global resource scheduler
    - Node Manager: Per-node resource manager
    - Application Master: Per-app coordinator
    """
    print("=" * 80)
    print("CREATING YARN SPARK SESSION")
    print("=" * 80)

    spark = (
        SparkSession.builder.appName("YARNClusterETL")
        .master("yarn")
        .config("spark.submit.deployMode", "cluster")
        .config("spark.executor.instances", "10")
        .config("spark.executor.cores", "5")
        .config("spark.executor.memory", "10g")
        .config("spark.executor.memoryOverhead", "2g")
        .config("spark.driver.memory", "4g")
        .config("spark.driver.cores", "2")
        .config("spark.driver.memoryOverhead", "1g")
        .config("spark.yarn.queue", "production")
        .config("spark.yarn.submit.waitAppCompletion", "true")
        .config("spark.yarn.maxAppAttempts", "2")
        .config("spark.dynamicAllocation.enabled", "true")
        .config("spark.dynamicAllocation.initialExecutors", "3")
        .config("spark.dynamicAllocation.minExecutors", "2")
        .config("spark.dynamicAllocation.maxExecutors", "20")
        .config("spark.dynamicAllocation.executorIdleTimeout", "60s")
        .config("spark.dynamicAllocation.cachedExecutorIdleTimeout", "300s")
        .config("spark.shuffle.service.enabled", "true")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.shuffle.partitions", "200")
        .config("spark.default.parallelism", "100")
        .config("spark.sql.autoBroadcastJoinThreshold", "50MB")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.kryoserializer.buffer.max", "512m")
        .config("spark.network.timeout", "600s")
        .config("spark.executor.heartbeatInterval", "20s")
        .config("spark.yarn.am.memory", "1g")
        .config("spark.yarn.am.cores", "1")
        .config("spark.yarn.executor.memoryOverhead", "2048")
        .config("spark.yarn.driver.memoryOverhead", "1024")
        .getOrCreate()
    )

    print(f"âœ… Spark {spark.version} on YARN cluster")
    print(f"âœ… Application ID: {spark.sparkContext.applicationId}")
    print(f"âœ… YARN Queue: production")
    print(f"âœ… Dynamic Allocation: 2-20 executors")
    print(f"âœ… Resources per executor: 5 cores, 10GB memory")

    return spark


def print_yarn_configuration(spark):
    """
    Display current YARN configuration.
    """
    print("\n" + "=" * 80)
    print("YARN CLUSTER CONFIGURATION")
    print("=" * 80)

    sc = spark.sparkContext
    conf = spark.sparkContext.getConf()

    print("\nğŸ“Š Cluster Information:")
    print(f"   Master: {sc.master}")
    print(f"   Deploy Mode: {conf.get('spark.submit.deployMode')}")
    print(f"   YARN Queue: {conf.get('spark.yarn.queue')}")
    print(f"   Application ID: {sc.applicationId}")

    print("\nğŸ”§ Resource Configuration:")
    print(f"   Executor Instances: {conf.get('spark.executor.instances')}")
    print(f"   Executor Cores: {conf.get('spark.executor.cores')}")
    print(f"   Executor Memory: {conf.get('spark.executor.memory')}")
    print(f"   Executor Memory Overhead: {conf.get('spark.executor.memoryOverhead')}")
    print(f"   Driver Memory: {conf.get('spark.driver.memory')}")
    print(f"   Driver Cores: {conf.get('spark.driver.cores')}")

    print("\nâš¡ Dynamic Allocation:")
    print(f"   Enabled: {conf.get('spark.dynamicAllocation.enabled')}")
    print(f"   Min Executors: {conf.get('spark.dynamicAllocation.minExecutors')}")
    print(f"   Max Executors: {conf.get('spark.dynamicAllocation.maxExecutors')}")
    print(
        f"   Initial Executors: {conf.get('spark.dynamicAllocation.initialExecutors')}"
    )

    print("\nğŸ¯ Performance Settings:")
    print(f"   Shuffle Partitions: {conf.get('spark.sql.shuffle.partitions')}")
    print(f"   Default Parallelism: {conf.get('spark.default.parallelism')}")
    print(f"   AQE Enabled: {conf.get('spark.sql.adaptive.enabled')}")
    print(f"   Broadcast Threshold: {conf.get('spark.sql.autoBroadcastJoinThreshold')}")


def generate_sample_data(spark, num_records=10_000_000):
    """
    Generate sample transactional data for ETL processing.
    """
    print("\n" + "=" * 80)
    print(f"GENERATING {num_records:,} SAMPLE RECORDS")
    print("=" * 80)

    # Generate large dataset to demonstrate cluster computing
    from pyspark.sql.functions import expr, rand, randn

    df = (
        spark.range(0, num_records)
        .withColumn("customer_id", (rand() * 100000).cast("int"))
        .withColumn("product_id", (rand() * 1000).cast("int"))
        .withColumn("quantity", (rand() * 10 + 1).cast("int"))
        .withColumn("price", (rand() * 100 + 10).cast("double"))
        .withColumn(
            "region",
            expr(
                "CASE "
                + "WHEN rand() < 0.3 THEN 'US' "
                + "WHEN rand() < 0.6 THEN 'EU' "
                + "WHEN rand() < 0.85 THEN 'ASIA' "
                + "ELSE 'OTHER' END"
            ),
        )
        .withColumn(
            "timestamp", expr("current_timestamp() - INTERVAL (rand() * 365) DAYS")
        )
    )

    print(f"âœ… Generated {df.count():,} records")
    print(f"âœ… Partitions: {df.rdd.getNumPartitions()}")

    return df


def etl_pipeline_on_yarn(spark, df):
    """
    Run complete ETL pipeline on YARN cluster.

    Stages:
    1. Data validation
    2. Transformations
    3. Aggregations
    4. Joins
    5. Output
    """
    print("\n" + "=" * 80)
    print("RUNNING ETL PIPELINE ON YARN")
    print("=" * 80)

    start_time = time.time()

    # Stage 1: Data Validation
    print("\nğŸ“‹ Stage 1: Data Validation")
    validated_df = df.filter(
        (col("quantity") > 0) & (col("price") > 0) & (col("customer_id").isNotNull())
    )

    invalid_count = df.count() - validated_df.count()
    print(f"   Filtered out {invalid_count:,} invalid records")

    # Stage 2: Calculate Revenue
    print("\nğŸ’° Stage 2: Calculate Revenue")
    revenue_df = validated_df.withColumn("revenue", col("quantity") * col("price"))

    # Stage 3: Regional Aggregations
    print("\nğŸŒ Stage 3: Regional Aggregations")
    regional_summary = (
        revenue_df.groupBy("region")
        .agg(
            count("*").alias("total_transactions"),
            _sum("revenue").alias("total_revenue"),
            avg("revenue").alias("avg_revenue"),
            _sum("quantity").alias("total_quantity"),
        )
        .orderBy(col("total_revenue").desc())
    )

    print("\nğŸ“Š Regional Performance:")
    regional_summary.show()

    # Stage 4: Product Performance
    print("\nğŸ“¦ Stage 4: Product Performance")
    product_summary = (
        revenue_df.groupBy("product_id")
        .agg(
            count("*").alias("sales_count"),
            _sum("revenue").alias("total_revenue"),
            avg("price").alias("avg_price"),
        )
        .filter(col("sales_count") > 100)
        .orderBy(col("total_revenue").desc())
        .limit(10)
    )

    print("\nğŸ† Top 10 Products:")
    product_summary.show()

    # Stage 5: Customer Segmentation
    print("\nï¿½ï¿½ Stage 5: Customer Segmentation")
    customer_summary = (
        revenue_df.groupBy("customer_id")
        .agg(
            count("*").alias("purchase_count"),
            _sum("revenue").alias("total_spent"),
            avg("revenue").alias("avg_transaction"),
        )
        .withColumn(
            "customer_segment",
            when(col("total_spent") > 10000, "VIP")
            .when(col("total_spent") > 5000, "Premium")
            .when(col("total_spent") > 1000, "Standard")
            .otherwise("Basic"),
        )
    )

    segment_distribution = (
        customer_summary.groupBy("customer_segment")
        .agg(
            count("*").alias("customer_count"),
            _sum("total_spent").alias("segment_revenue"),
        )
        .orderBy(col("segment_revenue").desc())
    )

    print("\nğŸ¯ Customer Segmentation:")
    segment_distribution.show()

    # Cache for reuse
    customer_summary.cache()

    # Stage 6: Execution Metrics
    elapsed = time.time() - start_time
    print("\n" + "=" * 80)
    print("ETL PIPELINE COMPLETE")
    print("=" * 80)
    print(f"â±ï¸  Total execution time: {elapsed:.2f} seconds")
    print(f"ğŸ“Š Records processed: {validated_df.count():,}")
    print(f"ğŸ’¾ Data cached: {customer_summary.count():,} customer records")

    # Show execution plan for last aggregation
    print("\nï¿½ï¿½ Physical Execution Plan:")
    print(segment_distribution.explain(mode="formatted"))

    return regional_summary, product_summary, customer_summary


def demonstrate_yarn_features(spark):
    """
    Demonstrate YARN-specific features.
    """
    print("\n" + "=" * 80)
    print("YARN-SPECIFIC FEATURES")
    print("=" * 80)

    print(
        """
ğŸ¯ YARN Queue Management:
   - Separate queues for different priorities (prod, dev, adhoc)
   - Resource allocation per queue
   - Fair or capacity scheduler
   
   Example: spark.yarn.queue=production
   
ğŸ“Š Dynamic Resource Allocation:
   - Automatically scales executors based on workload
   - Releases idle executors after timeout
   - Requests more when tasks are pending
   
   Min: 2 â†’ Current: ? â†’ Max: 20
   
ğŸ”’ Security Features:
   - Kerberos authentication
   - YARN ACLs for application access
   - Secure shuffle service
   
ğŸ’¾ YARN NodeManager Shuffle Service:
   - External shuffle service for dynamic allocation
   - Persists shuffle data even if executors are removed
   - Enable: spark.shuffle.service.enabled=true
   
ğŸ“ˆ Resource Monitoring:
   - YARN ResourceManager UI: http://rm-host:8088
   - Application logs aggregation
   - Container metrics and statistics
   
âš ï¸  Failure Recovery:
   - Application Master restart on failure
   - Max attempts: spark.yarn.maxAppAttempts
   - Automatic executor replacement
    """
    )


def yarn_submit_examples():
    """
    Print spark-submit examples for YARN.
    """
    print("\n" + "=" * 80)
    print("YARN SPARK-SUBMIT EXAMPLES")
    print("=" * 80)

    print(
        """
ğŸ“ 1. Client Mode (driver on local machine):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --master yarn \\
  --deploy-mode client \\
  --num-executors 10 \\
  --executor-cores 5 \\
  --executor-memory 10g \\
  --driver-memory 4g \\
  --conf spark.yarn.queue=production \\
  --conf spark.dynamicAllocation.enabled=true \\
  --conf spark.dynamicAllocation.maxExecutors=20 \\
  10_yarn_cluster_example.py

Use case: Interactive development, debugging


ğŸ“ 2. Cluster Mode (driver on YARN):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --num-executors 15 \\
  --executor-cores 4 \\
  --executor-memory 12g \\
  --driver-memory 6g \\
  --conf spark.yarn.queue=production \\
  --conf spark.yarn.submit.waitAppCompletion=false \\
  --conf spark.sql.shuffle.partitions=200 \\
  10_yarn_cluster_example.py

Use case: Production jobs, scheduled pipelines


ğŸ“ 3. High Memory Job (large datasets):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --num-executors 20 \\
  --executor-cores 5 \\
  --executor-memory 32g \\
  --executor-memoryOverhead 4g \\
  --driver-memory 8g \\
  --conf spark.yarn.queue=high-memory \\
  --conf spark.memory.fraction=0.8 \\
  --conf spark.memory.storageFraction=0.3 \\
  10_yarn_cluster_example.py

Use case: Large-scale aggregations, joins


ğŸ“ 4. Dynamic Allocation (variable workload):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --conf spark.dynamicAllocation.enabled=true \\
  --conf spark.dynamicAllocation.initialExecutors=3 \\
  --conf spark.dynamicAllocation.minExecutors=2 \\
  --conf spark.dynamicAllocation.maxExecutors=50 \\
  --conf spark.dynamicAllocation.executorIdleTimeout=60s \\
  --conf spark.shuffle.service.enabled=true \\
  --executor-cores 4 \\
  --executor-memory 8g \\
  10_yarn_cluster_example.py

Use case: Variable load, cost optimization


ğŸ“ 5. With Dependencies (Python packages):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --py-files dependencies.zip \\
  --archives environment.tar.gz#env \\
  --conf spark.yarn.dist.files=config.json \\
  --conf spark.pyspark.python=./env/bin/python \\
  10_yarn_cluster_example.py

Use case: Custom Python packages, configurations
    """
    )


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ¯" * 40)
    print("YARN CLUSTER COMPUTING - COMPLETE EXAMPLE")
    print("ğŸ¯" * 40)

    # Note: This will fail without a real YARN cluster
    # For demo purposes, we'll use local mode
    print("\nâš ï¸  Note: Running in LOCAL mode for demonstration")
    print("   In production, this would connect to YARN cluster")

    # Create Spark session (local for demo)
    spark = (
        SparkSession.builder.appName("YARNDemo")
        .master("local[*]")
        .config("spark.sql.shuffle.partitions", "8")
        .getOrCreate()
    )

    # Show what YARN configuration would look like
    print_yarn_configuration(spark)

    # Generate sample data
    df = generate_sample_data(spark, num_records=1_000_000)

    # Run ETL pipeline
    regional, products, customers = etl_pipeline_on_yarn(spark, df)

    # Show YARN features
    demonstrate_yarn_features(spark)

    # Show submit examples
    yarn_submit_examples()

    print("\n" + "=" * 80)
    print("âœ… YARN EXAMPLE COMPLETE")
    print("=" * 80)
    print("\nğŸ“š Key Takeaways:")
    print("   1. YARN is the resource manager in Hadoop ecosystem")
    print("   2. Dynamic allocation automatically scales executors")
    print("   3. Queue system manages multi-tenant resources")
    print("   4. Cluster mode runs driver on YARN (production)")
    print("   5. Client mode runs driver locally (development)")
    print("   6. Shuffle service enables dynamic allocation")
    print("   7. Monitor via YARN RM UI: http://rm-host:8088")

    spark.stop()


if __name__ == "__main__":
    main()
