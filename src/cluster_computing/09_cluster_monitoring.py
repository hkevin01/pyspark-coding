"""
================================================================================
CLUSTER COMPUTING #9 - Cluster Monitoring and Performance Debugging
================================================================================

MODULE OVERVIEW:
----------------
Effective monitoring is essential for production Spark applications. The Spark UI
provides comprehensive metrics for debugging performance issues, identifying
bottlenecks, and optimizing resource utilization.

This module teaches you to master the Spark UI, interpret metrics, and debug
common performance problems in distributed clusters.

PURPOSE:
--------
Master cluster monitoring and debugging:
â€¢ Navigate Spark UI tabs and metrics
â€¢ Identify performance bottlenecks
â€¢ Debug executor failures and stragglers
â€¢ Monitor shuffle and memory usage
â€¢ Analyze query execution plans
â€¢ Set up production monitoring
â€¢ Troubleshoot common issues

SPARK UI ARCHITECTURE:
-----------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK UI OVERVIEW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Access URLs:                                                   â”‚
â”‚  â€¢ Local: http://localhost:4040                                â”‚
â”‚  â€¢ Driver: http://<driver-ip>:4040                             â”‚
â”‚  â€¢ History Server: http://<history-server>:18080               â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ TABS                                                    â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  1. Jobs Tab                                            â”‚   â”‚
â”‚  â”‚     â€¢ High-level job overview                           â”‚   â”‚
â”‚  â”‚     â€¢ Job duration and status                           â”‚   â”‚
â”‚  â”‚     â€¢ Timeline visualization                            â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  2. Stages Tab                                          â”‚   â”‚
â”‚  â”‚     â€¢ Detailed stage metrics                            â”‚   â”‚
â”‚  â”‚     â€¢ Task distribution                                 â”‚   â”‚
â”‚  â”‚     â€¢ Shuffle read/write                                â”‚   â”‚
â”‚  â”‚     â€¢ Input/output sizes                                â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  3. Storage Tab                                         â”‚   â”‚
â”‚  â”‚     â€¢ Cached RDDs/DataFrames                            â”‚   â”‚
â”‚  â”‚     â€¢ Memory usage                                      â”‚   â”‚
â”‚  â”‚     â€¢ Persistence levels                                â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  4. Environment Tab                                     â”‚   â”‚
â”‚  â”‚     â€¢ Spark configuration                               â”‚   â”‚
â”‚  â”‚     â€¢ System properties                                 â”‚   â”‚
â”‚  â”‚     â€¢ Classpath entries                                 â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  5. Executors Tab                                       â”‚   â”‚
â”‚  â”‚     â€¢ Executor resource usage                           â”‚   â”‚
â”‚  â”‚     â€¢ GC time                                           â”‚   â”‚
â”‚  â”‚     â€¢ Shuffle metrics                                   â”‚   â”‚
â”‚  â”‚     â€¢ Task failures                                     â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  6. SQL Tab                                             â”‚   â”‚
â”‚  â”‚     â€¢ Query execution plans                             â”‚   â”‚
â”‚  â”‚     â€¢ Physical vs logical plans                         â”‚   â”‚
â”‚  â”‚     â€¢ Metrics per operation                             â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JOBS TAB - HIGH-LEVEL OVERVIEW:
--------------------------------

What to Look For:
-----------------
âœ… **Job Status**:
   - Succeeded: âœ… Job completed
   - Running: â³ In progress
   - Failed: âŒ Check errors

âœ… **Duration**:
   - Compare similar jobs
   - Identify slow jobs
   - Track performance trends

âœ… **Stages**:
   - Number of stages (fewer is better)
   - Stage dependencies
   - Parallel vs sequential execution

Example Timeline Visualization:
```
Job 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (45s)
  Stage 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (15s)
  Stage 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (30s)  â† Slow stage!
  
Job 1: â–ˆâ–ˆâ–ˆâ–ˆ (8s)
  Stage 2: â–ˆâ–ˆâ–ˆâ–ˆ (8s)
```

Red Flags:
âŒ Jobs taking much longer than expected
âŒ High number of stages (> 100)
âŒ Stages not running in parallel

STAGES TAB - DETAILED METRICS:
-------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE METRICS DASHBOARD                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Stage: Stage 5 (join)                                         â”‚
â”‚  Status: âœ… Succeeded                                           â”‚
â”‚  Duration: 2.3 min                                             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ TASK METRICS                                            â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Total Tasks: 200                                        â”‚   â”‚
â”‚  â”‚ Succeeded: 200  Failed: 0  Running: 0                   â”‚   â”‚
â”‚  â”‚ Task Duration: min=2s, median=5s, max=45s âš ï¸           â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ Task Timeline:                                          â”‚   â”‚
â”‚  â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]              â”‚   â”‚
â”‚  â”‚ â””â”€â”€ Straggler task (45s) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â”‚     âš ï¸  10x slower than median!                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ SHUFFLE METRICS                                         â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Shuffle Read: 2.5 GB                                    â”‚   â”‚
â”‚  â”‚ Shuffle Write: 1.8 GB                                   â”‚   â”‚
â”‚  â”‚ Shuffle Spill (Memory): 500 MB âš ï¸                       â”‚   â”‚
â”‚  â”‚ Shuffle Spill (Disk): 2.1 GB   âŒ BAD!                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ MEMORY METRICS                                          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Input Size: 3.2 GB                                      â”‚   â”‚
â”‚  â”‚ Output Size: 1.5 GB                                     â”‚   â”‚
â”‚  â”‚ Peak Execution Memory: 6.8 GB                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Metrics Explained:
----------------------

1. **Task Duration Distribution**:
   - Min: Fastest task
   - Median: Typical task time
   - Max: Slowest task (straggler)
   
   âš ï¸  If max >> median: Data skew or resource contention

2. **Shuffle Metrics**:
   - Shuffle Read: Data read from other executors
   - Shuffle Write: Data written for next stage
   - Spill (Memory): Temp storage in memory (OK)
   - Spill (Disk): Spilled to disk (SLOW! âŒ)

3. **Input/Output Size**:
   - Input: Data read by stage
   - Output: Data produced by stage
   - Large input â†’ Consider filtering earlier

EXECUTORS TAB - RESOURCE MONITORING:
-------------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXECUTORS DASHBOARD                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Executor ID | Address        | Status | Memory   | Disk       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  driver      | 192.168.1.10   | Active | 2.0 GB   | 0 B        â”‚
â”‚  1           | 192.168.1.11   | Active | 8.0 GB   | 120 GB     â”‚
â”‚  2           | 192.168.1.12   | Active | 7.8 GB   | 118 GB     â”‚
â”‚  3           | 192.168.1.13   | Active | 0.2 GB âš ï¸| 15 GB      â”‚
â”‚  4           | 192.168.1.14   | Dead âŒ|          |            â”‚
â”‚                                                                 â”‚
â”‚  Per-Executor Metrics:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Executor 1                                               â”‚  â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚
â”‚  â”‚ Tasks: 245 (Success: 245, Failed: 0)                    â”‚  â”‚
â”‚  â”‚ Duration: 2.3 hours                                      â”‚  â”‚
â”‚  â”‚ GC Time: 12 min (8.6% of total) âœ… Good                 â”‚  â”‚
â”‚  â”‚ Input: 45 GB                                             â”‚  â”‚
â”‚  â”‚ Shuffle Read: 12 GB                                      â”‚  â”‚
â”‚  â”‚ Shuffle Write: 8 GB                                      â”‚  â”‚
â”‚  â”‚ Memory Used: 6.5 GB / 8.0 GB (81%)                       â”‚  â”‚
â”‚  â”‚ Disk Used: 120 GB                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Executor 3  âš ï¸  UNDERUTILIZED                            â”‚  â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚
â”‚  â”‚ Tasks: 15 (Success: 15, Failed: 0)                      â”‚  â”‚
â”‚  â”‚ Duration: 10 min                                         â”‚  â”‚
â”‚  â”‚ GC Time: 5 min (50% of total) âŒ BAD!                   â”‚  â”‚
â”‚  â”‚ Input: 2 GB                                              â”‚  â”‚
â”‚  â”‚ Memory Used: 0.2 GB / 8.0 GB (2.5%)                      â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Issue: Data skew â†’ executor has very few tasks          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Red Flags:
----------
âŒ **High GC Time** (> 10% of task time):
   - Symptom: Executor spending too much time in garbage collection
   - Cause: Insufficient memory or memory leaks
   - Fix: Increase executor memory, reduce cached data

âŒ **Unbalanced Task Distribution**:
   - Symptom: Some executors idle, others overloaded
   - Cause: Data skew or wrong partition count
   - Fix: Repartition data, use salting for skewed keys

âŒ **Dead Executors**:
   - Symptom: Executor marked as "Dead"
   - Cause: OOM, network issues, or task timeout
   - Fix: Check logs, increase memory, check network

STORAGE TAB - CACHE MONITORING:
--------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CACHED DATAFRAMES                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  RDD/DF Name       | Storage Level | Size     | Partitions     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  customers         | Memory Only   | 2.5 GB   | 200            â”‚
â”‚  orders_cached     | Memory & Disk | 8.2 GB   | 400            â”‚
â”‚  large_dataset âš ï¸  | Memory Only   | 15 GB    | 100 (50% cached)â”‚
â”‚                                                                 â”‚
â”‚  Details: large_dataset                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Storage Level: MEMORY_ONLY                               â”‚  â”‚
â”‚  â”‚ Cached Partitions: 50 / 100 (50%)  âš ï¸                   â”‚  â”‚
â”‚  â”‚ Size in Memory: 7.5 GB / 15 GB                           â”‚  â”‚
â”‚  â”‚ Size on Disk: 0 B                                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚ Issue: Not enough memory â†’ only 50% cached              â”‚  â”‚
â”‚  â”‚ Solution:                                                â”‚  â”‚
â”‚  â”‚   1. Increase executor memory                            â”‚  â”‚
â”‚  â”‚   2. Use MEMORY_AND_DISK storage                         â”‚  â”‚
â”‚  â”‚   3. Unpersist unused data                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage Levels:
```python
# MEMORY_ONLY: Fast but limited by memory
df.persist(StorageLevel.MEMORY_ONLY)

# MEMORY_AND_DISK: Fallback to disk if OOM
df.persist(StorageLevel.MEMORY_AND_DISK)

# MEMORY_ONLY_SER: Serialized (more compact, slower access)
df.persist(StorageLevel.MEMORY_ONLY_SER)

# OFF_HEAP: Use off-heap memory (no GC overhead)
df.persist(StorageLevel.OFF_HEAP)
```

SQL TAB - QUERY ANALYSIS:
--------------------------

Most important tab for DataFrame/SQL optimization!

What to Look For:
-----------------

1. **Query Execution Time**:
   - Duration per query
   - Compare similar queries
   - Identify slow queries

2. **Physical Plan**:
   - Operations performed
   - Look for "Exchange" (= shuffle)
   - Check join strategies (BroadcastHashJoin vs SortMergeJoin)

3. **Metrics per Operation**:
   - Number of rows
   - Data size
   - Time spent in each operation

Example:
```
Query: SELECT category, SUM(sales) FROM orders GROUP BY category

Execution Plan:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HashAggregate (final)                                      â”‚
â”‚ â€¢ Output: [category, sum(sales)]                           â”‚
â”‚ â€¢ Time: 500 ms                                             â”‚
â”‚ â€¢ Rows: 10                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Exchange (Shuffle)  âš ï¸                                     â”‚
â”‚ â€¢ Shuffle Read: 2.5 GB                                     â”‚
â”‚ â€¢ Time: 8 seconds  â† BOTTLENECK!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HashAggregate (partial)                                    â”‚
â”‚ â€¢ Time: 1 second                                           â”‚
â”‚ â€¢ Rows: 10,000,000                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scan Parquet                                               â”‚
â”‚ â€¢ Time: 2 seconds                                          â”‚
â”‚ â€¢ Rows: 10,000,000                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Analysis: Shuffle is the bottleneck (8 seconds out of 11.5 total)

PERFORMANCE DEBUGGING WORKFLOW:
--------------------------------

Step 1: Identify Slow Job
```
Jobs Tab â†’ Find job with long duration
```

Step 2: Find Slow Stage
```
Stages Tab â†’ Look at stage durations
â†’ Identify stage taking most time
```

Step 3: Analyze Stage Metrics
```
Click on slow stage â†’ Check:
â€¢ Task duration distribution (stragglers?)
â€¢ Shuffle metrics (spill to disk?)
â€¢ Input/output sizes (too much data?)
```

Step 4: Check Executors
```
Executors Tab â†’ Look for:
â€¢ High GC time (> 10%)
â€¢ Unbalanced task distribution
â€¢ Dead executors
```

Step 5: Examine Query Plan
```
SQL Tab â†’ Check:
â€¢ Exchange operations (shuffles)
â€¢ Join strategies
â€¢ Filter pushdown
```

Step 6: Fix and Re-run
```
Apply optimization â†’ Monitor improvement
```

COMMON PERFORMANCE ISSUES:
---------------------------

1. **Data Skew** (Unbalanced Partitions):

Symptoms:
- One task takes 10-100x longer than others
- One executor has most of the data
- High memory usage on one executor

Detection:
```
Stages Tab â†’ Task metrics:
Min: 2s, Median: 3s, Max: 300s  â† Skew!
```

Solutions:
```python
# Add salt to skewed key
from pyspark.sql.functions import rand, floor
df = df.withColumn("salt", (floor(rand() * 10)).cast("int"))
df = df.repartition("key", "salt")

# Or use AQE (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

2. **Excessive Shuffles**:

Symptoms:
- Long shuffle read/write times
- High network usage
- Disk spill

Detection:
```
SQL Tab â†’ Look for many "Exchange" operations
Stages Tab â†’ High shuffle read/write sizes
```

Solutions:
```python
# Use broadcast joins
result = large_df.join(broadcast(small_df), "key")

# Filter early
df = df.filter(col("date") > "2024-01-01").join(...)

# Enable AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

3. **Memory Issues**:

Symptoms:
- Executor OOM errors
- High GC time (> 10%)
- Disk spill (memory â†’ disk)

Detection:
```
Executors Tab â†’ GC Time > 10%
Stages Tab â†’ Spill (Disk) > 0
```

Solutions:
```python
# Increase executor memory
--executor-memory 16g  # was 8g

# Reduce cached data
df.unpersist()

# Use off-heap memory
--conf spark.memory.offHeap.enabled=true
--conf spark.memory.offHeap.size=4g
```

4. **Too Many Small Files**:

Symptoms:
- Many tasks (> 10,000)
- High task scheduling overhead
- Slow reads

Detection:
```
Stages Tab â†’ Total Tasks: 50,000  â† Too many!
Input: 500 MB across 50,000 files
```

Solutions:
```python
# Coalesce before writing
df.coalesce(100).write.parquet("output")

# Or repartition
df.repartition(200).write.parquet("output")
```

MONITORING IN PRODUCTION:
--------------------------

1. **Enable Event Logs**:
```python
spark = SparkSession.builder \\
    .config("spark.eventLog.enabled", "true") \\
    .config("spark.eventLog.dir", "hdfs:///spark-logs") \\
    .getOrCreate()
```

2. **Set Up History Server**:
```bash
# Start Spark History Server
$SPARK_HOME/sbin/start-history-server.sh

# Access: http://localhost:18080
```

3. **Configure Metrics**:
```python
spark.conf.set("spark.metrics.namespace", "myapp")
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

4. **Monitoring Tools**:
- Ganglia: Cluster-wide metrics
- Prometheus + Grafana: Custom dashboards
- DataDog: APM integration
- AWS CloudWatch: For EMR clusters

KEY METRICS TO TRACK:
----------------------

âœ… **Job-Level**:
   - Job duration (target: < 5 min)
   - Success rate (target: > 99%)
   - Jobs per hour (throughput)

âœ… **Stage-Level**:
   - Shuffle read/write sizes
   - Spill to disk (target: 0)
   - Task duration distribution

âœ… **Executor-Level**:
   - GC time % (target: < 10%)
   - Memory utilization (target: 70-85%)
   - Task failures (target: 0)

âœ… **Query-Level**:
   - Query execution time
   - Number of shuffles
   - Data scanned vs returned

SPARK UI ACCESS:
----------------

Local Mode:
```bash
# Default: http://localhost:4040
# If port taken: http://localhost:4041, 4042, etc.
```

Cluster Mode (YARN):
```bash
# While running:
yarn application -status <app_id>
# â†’ Get Tracking URL

# After completion:
# History Server: http://<history-server>:18080
```

Kubernetes:
```bash
# Port-forward to driver pod
kubectl port-forward <driver-pod> 4040:4040

# Access: http://localhost:4040
```

EXPLAIN() FOR MONITORING:
--------------------------

Always use explain() to preview query plan:

```python
# Simple explain
df.groupBy("category").count().explain()

# Extended explain (all plans)
df.groupBy("category").count().explain(mode="extended")

# Cost-based explain (statistics)
df.groupBy("category").count().explain(mode="cost")

# Formatted explain (Spark 3.0+)
df.groupBy("category").count().explain(mode="formatted")
```

Look for:
â€¢ Number of stages
â€¢ Exchange operations (shuffles)
â€¢ Join strategies
â€¢ Estimated data sizes

PRODUCTION CHECKLIST:
---------------------

âœ… Monitoring Setup:
   - Event logs enabled
   - History server running
   - Alerts configured
   - Metrics collection

âœ… Regular Checks:
   - Review slow queries daily
   - Monitor executor failures
   - Track memory usage trends
   - Analyze shuffle patterns

âœ… Optimization:
   - Enable AQE (Spark 3.0+)
   - Configure appropriate partitions
   - Use broadcast joins
   - Cache wisely

âœ… Alerting:
   - Job failures
   - Executor OOM
   - High GC time
   - Abnormal duration

See Also:
---------
â€¢ 08_shuffle_optimization.py - Reduce shuffles
â€¢ 07_resource_management.py - Memory tuning
â€¢ 05_fault_tolerance.py - Handle failures
â€¢ ../spark_execution_architecture/ - Execution internals
"""

import time

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand


def create_spark():
    return (
        SparkSession.builder.appName("ClusterMonitoring")
        .master("local[4]")
        .config("spark.ui.port", "4040")
        .config("spark.eventLog.enabled", "true")
        .getOrCreate()
    )


def demonstrate_spark_ui_overview():
    """Overview of Spark UI tabs."""
    print("=" * 70)
    print("1. SPARK UI OVERVIEW")
    print("=" * 70)

    print(
        """
ğŸ–¥ï¸  Spark UI: Your monitoring dashboard

Access:
-------
Local: http://localhost:4040
Driver: http://<driver-ip>:4040
History Server: http://<history-server>:18080

ğŸ—‚ï¸  Main Tabs:

1. **Jobs Tab**
   - Overview of all Spark jobs
   - Success/failure status
   - Duration of each job
   - Stages per job
   
   Use for: High-level job monitoring

2. **Stages Tab** â­ MOST IMPORTANT
   - Detailed stage metrics
   - Task distribution
   - Shuffle read/write
   - Spill to disk
   - Data skew detection
   
   Use for: Performance debugging

3. **Storage Tab**
   - Cached RDDs/DataFrames
   - Memory usage per partition
   - Storage levels (MEMORY_ONLY, DISK, etc.)
   
   Use for: Memory management

4. **Environment Tab**
   - Spark configuration
   - System properties
   - Classpath entries
   
   Use for: Configuration verification

5. **Executors Tab**
   - Executor memory usage
   - Task distribution
   - Shuffle read/write per executor
   - GC time
   
   Use for: Resource utilization

6. **SQL Tab**
   - DataFrame/SQL query plans
   - Physical vs logical plans
   - DAG visualization
   
   Use for: Query optimization

7. **Streaming Tab** (if using Structured Streaming)
   - Batch processing time
   - Input rate
   - Processing rate
   
   Use for: Streaming performance
    """
    )


def demonstrate_stages_tab_metrics(spark):
    """Key metrics in Stages tab."""
    print("\n" + "=" * 70)
    print("2. STAGES TAB METRICS")
    print("=" * 70)

    print(
        """
ğŸ“Š Key Metrics to Monitor:

1. **Duration**
   Total time for stage
   Look for: Stages taking much longer than others

2. **Input Size / Records**
   Data read into stage
   Look for: Very large input causing slowdowns

3. **Shuffle Read / Write**
   Data moved between nodes
   Look for: Large shuffle sizes (> 1 GB per task)

4. **Spill (Memory / Disk)**
   Data that didn't fit in memory
   Look for: Any spill indicates memory pressure
   
   âš ï¸  Spill to Memory: Moderate issue
   âŒ Spill to Disk: Serious performance problem

5. **Task Time Distribution**
   Min, 25%, Median, 75%, Max task times
   Look for: Wide spread = data skew
   
   Example:
   Min: 1s, Median: 2s, Max: 60s â† SKEW!

6. **GC Time**
   Time spent in garbage collection
   Look for: > 10% of task time = memory issues


ğŸ” Reading the Stages Table:

Stage ID | Description      | Duration | Input   | Shuffle R | Shuffle W | Spill
---------|------------------|----------|---------|-----------|-----------|-------
0        | map              | 2s       | 1 GB    | 0 B       | 500 MB    | 0 B
1        | reduceByKey      | 10s      | 0 B     | 500 MB    | 100 MB    | 2 GB âš ï¸
2        | collect          | 1s       | 100 MB  | 0 B       | 0 B       | 0 B

Analysis:
---------
âœ… Stage 0: Clean (no spill)
âŒ Stage 1: 2 GB spill to disk! Increase memory or partitions
âœ… Stage 2: Clean
    """
    )

    # Generate some data to monitor
    print("\nğŸ“Š Generating sample workload to monitor...")
    data = spark.range(1, 5000001).toDF("id").withColumn("value", rand() * 1000)

    result = data.groupBy((col("id") % 100).alias("bucket")).count()
    result.collect()

    print("\nâœ… Check Spark UI at http://localhost:4040")
    print("   â†’ Go to 'Stages' tab")
    print("   â†’ Click on latest stage")
    print("   â†’ Review metrics: Duration, Shuffle, Spill")


def demonstrate_task_metrics():
    """Understanding task-level metrics."""
    print("\n" + "=" * 70)
    print("3. TASK-LEVEL METRICS")
    print("=" * 70)

    print(
        """
ğŸ”¬ Task Details (Click on stage â†’ Task table):

Columns to Watch:
-----------------
1. **Task ID**
   Unique identifier for each task

2. **Index**
   Partition number being processed

3. **Status**
   SUCCESS, RUNNING, FAILED
   Look for: Failed tasks

4. **Duration**
   Total time for task
   Look for: Tasks taking 10x longer than median

5. **GC Time**
   Garbage collection time
   Look for: > 10% of duration

6. **Shuffle Read / Write**
   Data read/written per task
   Look for: Imbalanced sizes across tasks

7. **Spill (Memory / Disk)**
   Per-task spill
   Look for: Any spill

8. **Executor ID / Host**
   Which executor ran the task
   Look for: All tasks on same executor = poor distribution


ğŸ¯ Identifying Data Skew:

Task Durations:
---------------
Task 0:  2s  |â– â– 
Task 1:  2s  |â– â– 
Task 2:  2s  |â– â– 
Task 3:  45s |â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â–   â† SKEWED PARTITION!
Task 4:  2s  |â– â– 
Task 5:  2s  |â– â– 

Diagnosis: Partition 3 has 20x more data
Solution: Use salting or repartition


ğŸ“Š Shuffle Metrics:

Good Distribution:
-----------------
Task | Shuffle Read
-----|-------------
0    | 100 MB
1    | 105 MB
2    | 98 MB
3    | 102 MB
âœ… Balanced!

Bad Distribution (Skew):
------------------------
Task | Shuffle Read
-----|-------------
0    | 50 MB
1    | 60 MB
2    | 2 GB  â† 90% of data in one partition!
3    | 55 MB
âŒ Skewed! Use salting technique
    """
    )


def demonstrate_executor_metrics(spark):
    """Monitor executor utilization."""
    print("\n" + "=" * 70)
    print("4. EXECUTOR METRICS")
    print("=" * 70)

    print(
        """
ğŸ–¥ï¸  Executors Tab Metrics:

Key Columns:
------------
1. **Executor ID**
   Unique identifier (0 = driver)

2. **Address**
   Host:port where executor runs

3. **Status**
   Active, Dead, Lost
   Look for: Dead executors

4. **RDD Blocks**
   Number of cached RDD partitions

5. **Storage Memory**
   Memory used for cached data
   Look for: Nearing storage memory limit

6. **Disk Used**
   Disk space for spilled data
   Look for: Large disk usage

7. **Cores**
   CPU cores allocated

8. **Active Tasks / Total Tasks**
   Current and historical task counts
   Look for: Imbalanced task distribution

9. **Failed Tasks**
   Number of failed tasks
   Look for: > 0 (indicates issues)

10. **GC Time / Duration**
    GC time as % of execution time
    Look for: > 10%

11. **Shuffle Read / Write**
    Total shuffle data per executor
    Look for: Imbalanced shuffles


ğŸ“Š Example Executor Dashboard:

Executor | Cores | Memory | Tasks | Failed | GC Time | Shuffle R | Shuffle W
---------|-------|--------|-------|--------|---------|-----------|----------
0        | 4     | 4 GB   | 120   | 0      | 5%      | 2 GB      | 1 GB
1        | 4     | 4 GB   | 115   | 0      | 6%      | 1.9 GB    | 950 MB
2        | 4     | 4 GB   | 10    | 5 âŒ   | 45% âŒ  | 200 MB    | 100 MB
3        | 4     | 4 GB   | 118   | 0      | 5%      | 2.1 GB    | 1.1 GB

Analysis:
---------
âŒ Executor 2 problems:
   - Only 10 tasks (underutilized)
   - 5 failed tasks
   - 45% GC time (memory pressure)
   
Action: Check executor logs, may need restart or more memory
    """
    )


def demonstrate_sql_query_plan(spark):
    """Understand SQL query plans."""
    print("\n" + "=" * 70)
    print("5. SQL QUERY PLANS")
    print("=" * 70)

    print(
        """
ğŸ—ºï¸  Query Plan Visualization:

SQL Tab shows:
--------------
1. **Logical Plan**
   High-level operations (filter, join, aggregation)

2. **Optimized Logical Plan**
   After Catalyst optimizer

3. **Physical Plan**
   Actual execution (Exchange = shuffle)

4. **DAG Visualization**
   Visual graph of stages


ğŸ” Reading Physical Plan:

Key Operations:
---------------
- **FileScan**: Read from source
- **Filter**: Filter rows (no shuffle)
- **Project**: Select columns (no shuffle)
- **Exchange**: SHUFFLE POINT! âš ï¸
- **HashAggregate**: Group by aggregation
- **SortMergeJoin**: Join with sort
- **BroadcastHashJoin**: Broadcast join âœ…


Example Plan:
-------------
== Physical Plan ==
*(3) HashAggregate(keys=[category], functions=[count(1)])
+- Exchange hashpartitioning(category, 200) â† SHUFFLE!
   +- *(2) HashAggregate(keys=[category], functions=[partial_count(1)])
      +- *(2) Project [category]
         +- *(2) Filter (value > 100)
            +- *(1) FileScan parquet [value, category]

Reading the plan:
-----------------
1. FileScan: Read parquet
2. Filter: Filter value > 100 (no shuffle)
3. Partial aggregation (map side)
4. Exchange: SHUFFLE on category key
5. Final aggregation (reduce side)


ğŸ¯ Optimization Opportunities:

Before:
-------
Exchange hashpartitioning (200 partitions) â† Expensive!

After tuning:
-------------
Exchange hashpartitioning (50 partitions) â† Better!

Or even better:
---------------
BroadcastHashJoin â† No shuffle at all!
    """
    )

    # Demonstrate explain
    print("\nğŸ’» Example: Using .explain()")
    data = (
        spark.range(10000)
        .toDF("id")
        .withColumn("category", (col("id") % 10).cast("int"))
    )

    result = data.filter(col("id") > 5000).groupBy("category").count()

    print("\nPhysical Plan:")
    result.explain()


def demonstrate_monitoring_best_practices():
    """Best practices for cluster monitoring."""
    print("\n" + "=" * 70)
    print("6. MONITORING BEST PRACTICES")
    print("=" * 70)

    print(
        """
ğŸ¯ Monitoring Checklist:

Daily Checks:
-------------
1. âœ… Job success rate
   Target: > 99%

2. âœ… Average job duration
   Look for: Increasing trends

3. âœ… Executor utilization
   Target: > 80% task distribution

4. âœ… Memory usage
   Target: < 90% to avoid spills

5. âœ… GC time percentage
   Target: < 10% of execution time


Per-Job Analysis:
-----------------
1. âœ… Check Stages tab for slowest stage
2. âœ… Identify shuffle sizes (should be < 1 GB/task)
3. âœ… Look for spill to disk (should be 0)
4. âœ… Check task time distribution for skew
5. âœ… Verify all executors are being used


ğŸš¨ Red Flags:

1. âŒ Spill to disk > 0
   â†’ Increase executor memory or partitions

2. âŒ Task time max >> median
   â†’ Data skew, use salting

3. âŒ GC time > 10%
   â†’ Memory pressure, increase executor memory

4. âŒ Shuffle read > 10 GB per partition
   â†’ Increase spark.sql.shuffle.partitions

5. âŒ Failed tasks > 0
   â†’ Check executor logs

6. âŒ One executor with most tasks
   â†’ Poor data distribution

7. âŒ Long shuffle read time
   â†’ Network or disk bottleneck


ğŸ“Š Performance Metrics to Track:

Metric                    Good      Warning   Critical
------                    ----      -------   --------
GC Time %                 < 5%      5-10%     > 10%
Spill to Memory          0         < 1 GB    > 1 GB
Spill to Disk            0         > 0       > 5 GB
Task time variance       < 2x      2-5x      > 5x
Shuffle per partition    < 1 GB    1-5 GB    > 5 GB
Executor utilization     > 80%     60-80%    < 60%


ğŸ”§ Debugging Workflow:

1. Job is slow â†’ Check Stages tab
2. Find slowest stage â†’ Click stage ID
3. Check metrics:
   - Large shuffle? â†’ Use broadcast or pre-partition
   - Spill to disk? â†’ Increase memory or partitions
   - Task skew? â†’ Use salting technique
   - High GC time? â†’ Increase executor memory
4. Apply fix and re-run
5. Compare metrics before/after


ğŸ’¡ Pro Tips:

1. Enable event log for history
   spark.conf.set("spark.eventLog.enabled", "true")
   spark.conf.set("spark.eventLog.dir", "hdfs://logs")

2. Use Spark History Server for past jobs
   spark-history-server.sh start

3. Export metrics to monitoring systems
   - Prometheus + Grafana
   - Datadog
   - CloudWatch (AWS)

4. Set up alerts
   - Job failure alerts
   - Long-running job alerts
   - Resource usage alerts

5. Regular log review
   - Executor logs for errors
   - Driver logs for application issues


ğŸ“ˆ Grafana Dashboard Metrics:

Panel 1: Job Success Rate
- Line chart of % successful jobs

Panel 2: Average Job Duration
- Track trends over time

Panel 3: Executor Memory Usage
- Gauge showing % used

Panel 4: Active Executors
- Count of active vs total

Panel 5: Shuffle Read/Write
- Bar chart per job

Panel 6: GC Time %
- Line chart with 10% threshold


ğŸ” Log Analysis Commands:

# Find OOM errors
grep "OutOfMemoryError" executor-*.log

# Find failed tasks
grep "Task.*FAILED" executor-*.log

# Check GC logs
grep "GC" executor-*.log | tail -100

# Find slow tasks
grep "Task.*took" executor-*.log | sort -k4 -n | tail -10

# Check shuffle errors
grep "shuffle" executor-*.log | grep -i error
    """
    )


def demonstrate_common_issues():
    """Common issues and solutions."""
    print("\n" + "=" * 70)
    print("7. COMMON ISSUES & SOLUTIONS")
    print("=" * 70)

    print(
        """
ğŸ› Issue 1: Job Stuck / Very Slow

Symptoms:
---------
- Job runs for hours
- Progress bar stuck at same %
- Few active tasks

Diagnosis:
----------
1. Check Stages tab â†’ Look for long-running stage
2. Check task distribution â†’ All tasks on 1-2 executors?
3. Check shuffle sizes â†’ > 10 GB per partition?

Solutions:
----------
âœ… Increase spark.sql.shuffle.partitions
âœ… Check for data skew (use salting)
âœ… Add more executors


ğŸ› Issue 2: OutOfMemoryError

Symptoms:
---------
- Job fails with OOM
- Executor lost
- Container killed by YARN

Diagnosis:
----------
1. Check Stages tab â†’ Spill to disk?
2. Check Executors tab â†’ Memory usage near limit?
3. Check task size â†’ Processing huge partition?

Solutions:
----------
âœ… Increase executor memory: --executor-memory 16g
âœ… Increase memory overhead: --executor-memoryOverhead 4g
âœ… Increase partitions to reduce per-task data
âœ… Don't cache too much data
âœ… Use broadcast for small tables


ğŸ› Issue 3: Data Skew

Symptoms:
---------
- Most tasks finish quickly
- 1-2 tasks take 10x longer
- Median task time: 2s, Max: 60s

Diagnosis:
----------
1. Check Stages tab â†’ Task time distribution
2. Click stage â†’ Look at task table
3. Identify which partition is large

Solutions:
----------
âœ… Use salting technique for skewed keys
âœ… Broadcast join if one side is small
âœ… Increase partitions


ğŸ› Issue 4: Shuffle Performance

Symptoms:
---------
- Long shuffle read/write times
- Large shuffle sizes (> 1 GB/task)

Diagnosis:
----------
1. Check Stages tab â†’ Shuffle Read/Write columns
2. Check if shuffle is necessary

Solutions:
----------
âœ… Filter before shuffle
âœ… Use broadcast join for small tables
âœ… Pre-partition on join key
âœ… Increase spark.sql.shuffle.partitions
âœ… Enable compression (default on)


ğŸ› Issue 5: Executor Lost

Symptoms:
---------
- Executor becomes unresponsive
- Tasks fail and retry
- "Executor lost" in logs

Diagnosis:
----------
1. Check executor logs
2. Look for OOM, killed by YARN, network timeout

Solutions:
----------
âœ… Increase executor memory
âœ… Increase network timeout: spark.network.timeout=800s
âœ… Check node health (disk, memory, network)
âœ… Increase heartbeat: spark.executor.heartbeatInterval=30s


ğŸ› Issue 6: High GC Time

Symptoms:
---------
- GC time > 10% of task time
- Slow task execution
- Frequent GC pauses

Diagnosis:
----------
1. Check Executors tab â†’ GC Time column
2. Check memory usage near limit

Solutions:
----------
âœ… Increase executor memory
âœ… Reduce cached data
âœ… Use off-heap memory
âœ… Tune GC settings (G1GC recommended)
âœ… Increase partitions to reduce per-task memory
    """
    )


def main():
    spark = create_spark()

    print("ğŸ“Š CLUSTER MONITORING")
    print("=" * 70)
    print("\nMaster Spark UI and performance debugging!")
    print()

    demonstrate_spark_ui_overview()
    demonstrate_stages_tab_metrics(spark)
    demonstrate_task_metrics()
    demonstrate_executor_metrics(spark)
    demonstrate_sql_query_plan(spark)
    demonstrate_monitoring_best_practices()
    demonstrate_common_issues()

    print("\n" + "=" * 70)
    print("âœ… CLUSTER MONITORING DEMO COMPLETE!")
    print("=" * 70)
    print("\nğŸ“ Key Takeaways:")
    print("   1. Always check Stages tab first for bottlenecks")
    print("   2. Look for: Spill, Skew, Large shuffles")
    print("   3. Monitor: GC time < 10%, No spill to disk")
    print("   4. Use .explain() to understand query plans")
    print("   5. Enable event logs for history analysis")
    print("   6. Set up alerts for failures and slow jobs")
    print("   7. Regular log review prevents issues")
    print("\nğŸ–¥ï¸  Access Spark UI: http://localhost:4040")

    spark.stop()


if __name__ == "__main__":
    main()
