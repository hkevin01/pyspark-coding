"""
09_cluster_monitoring.py
========================

Master cluster monitoring: Spark UI, metrics, and performance debugging.

Learn how to monitor executor utilization, identify bottlenecks, and
debug performance issues in distributed clusters.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand
import time


def create_spark():
    return SparkSession.builder \
        .appName("ClusterMonitoring") \
        .master("local[4]") \
        .config("spark.ui.port", "4040") \
        .config("spark.eventLog.enabled", "true") \
        .getOrCreate()


def demonstrate_spark_ui_overview():
    """Overview of Spark UI tabs."""
    print("=" * 70)
    print("1. SPARK UI OVERVIEW")
    print("=" * 70)
    
    print("""
üñ•Ô∏è  Spark UI: Your monitoring dashboard

Access:
-------
Local: http://localhost:4040
Driver: http://<driver-ip>:4040
History Server: http://<history-server>:18080

üóÇÔ∏è  Main Tabs:

1. **Jobs Tab**
   - Overview of all Spark jobs
   - Success/failure status
   - Duration of each job
   - Stages per job
   
   Use for: High-level job monitoring

2. **Stages Tab** ‚≠ê MOST IMPORTANT
   - Detailed stage metrics
   - Task distribution
   - Shuffle read/write
   - Spill to disk
   - Data skew detection
   
   Use for: Performance debugging

3. **Storage Tab**
   - Cached RDDs/DataFrames
   - Memory usage per partition
   - Storage levels (MEMORY_ONLY, DISK, etc.)
   
   Use for: Memory management

4. **Environment Tab**
   - Spark configuration
   - System properties
   - Classpath entries
   
   Use for: Configuration verification

5. **Executors Tab**
   - Executor memory usage
   - Task distribution
   - Shuffle read/write per executor
   - GC time
   
   Use for: Resource utilization

6. **SQL Tab**
   - DataFrame/SQL query plans
   - Physical vs logical plans
   - DAG visualization
   
   Use for: Query optimization

7. **Streaming Tab** (if using Structured Streaming)
   - Batch processing time
   - Input rate
   - Processing rate
   
   Use for: Streaming performance
    """)


def demonstrate_stages_tab_metrics(spark):
    """Key metrics in Stages tab."""
    print("\n" + "=" * 70)
    print("2. STAGES TAB METRICS")
    print("=" * 70)
    
    print("""
üìä Key Metrics to Monitor:

1. **Duration**
   Total time for stage
   Look for: Stages taking much longer than others

2. **Input Size / Records**
   Data read into stage
   Look for: Very large input causing slowdowns

3. **Shuffle Read / Write**
   Data moved between nodes
   Look for: Large shuffle sizes (> 1 GB per task)

4. **Spill (Memory / Disk)**
   Data that didn't fit in memory
   Look for: Any spill indicates memory pressure
   
   ‚ö†Ô∏è  Spill to Memory: Moderate issue
   ‚ùå Spill to Disk: Serious performance problem

5. **Task Time Distribution**
   Min, 25%, Median, 75%, Max task times
   Look for: Wide spread = data skew
   
   Example:
   Min: 1s, Median: 2s, Max: 60s ‚Üê SKEW!

6. **GC Time**
   Time spent in garbage collection
   Look for: > 10% of task time = memory issues


üîç Reading the Stages Table:

Stage ID | Description      | Duration | Input   | Shuffle R | Shuffle W | Spill
---------|------------------|----------|---------|-----------|-----------|-------
0        | map              | 2s       | 1 GB    | 0 B       | 500 MB    | 0 B
1        | reduceByKey      | 10s      | 0 B     | 500 MB    | 100 MB    | 2 GB ‚ö†Ô∏è
2        | collect          | 1s       | 100 MB  | 0 B       | 0 B       | 0 B

Analysis:
---------
‚úÖ Stage 0: Clean (no spill)
‚ùå Stage 1: 2 GB spill to disk! Increase memory or partitions
‚úÖ Stage 2: Clean
    """)
    
    # Generate some data to monitor
    print("\nüìä Generating sample workload to monitor...")
    data = spark.range(1, 5000001).toDF("id") \
        .withColumn("value", rand() * 1000)
    
    result = data.groupBy((col("id") % 100).alias("bucket")).count()
    result.collect()
    
    print("\n‚úÖ Check Spark UI at http://localhost:4040")
    print("   ‚Üí Go to 'Stages' tab")
    print("   ‚Üí Click on latest stage")
    print("   ‚Üí Review metrics: Duration, Shuffle, Spill")


def demonstrate_task_metrics():
    """Understanding task-level metrics."""
    print("\n" + "=" * 70)
    print("3. TASK-LEVEL METRICS")
    print("=" * 70)
    
    print("""
üî¨ Task Details (Click on stage ‚Üí Task table):

Columns to Watch:
-----------------
1. **Task ID**
   Unique identifier for each task

2. **Index**
   Partition number being processed

3. **Status**
   SUCCESS, RUNNING, FAILED
   Look for: Failed tasks

4. **Duration**
   Total time for task
   Look for: Tasks taking 10x longer than median

5. **GC Time**
   Garbage collection time
   Look for: > 10% of duration

6. **Shuffle Read / Write**
   Data read/written per task
   Look for: Imbalanced sizes across tasks

7. **Spill (Memory / Disk)**
   Per-task spill
   Look for: Any spill

8. **Executor ID / Host**
   Which executor ran the task
   Look for: All tasks on same executor = poor distribution


üéØ Identifying Data Skew:

Task Durations:
---------------
Task 0:  2s  |‚ñ†‚ñ†
Task 1:  2s  |‚ñ†‚ñ†
Task 2:  2s  |‚ñ†‚ñ†
Task 3:  45s |‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  ‚Üê SKEWED PARTITION!
Task 4:  2s  |‚ñ†‚ñ†
Task 5:  2s  |‚ñ†‚ñ†

Diagnosis: Partition 3 has 20x more data
Solution: Use salting or repartition


üìä Shuffle Metrics:

Good Distribution:
-----------------
Task | Shuffle Read
-----|-------------
0    | 100 MB
1    | 105 MB
2    | 98 MB
3    | 102 MB
‚úÖ Balanced!

Bad Distribution (Skew):
------------------------
Task | Shuffle Read
-----|-------------
0    | 50 MB
1    | 60 MB
2    | 2 GB  ‚Üê 90% of data in one partition!
3    | 55 MB
‚ùå Skewed! Use salting technique
    """)


def demonstrate_executor_metrics(spark):
    """Monitor executor utilization."""
    print("\n" + "=" * 70)
    print("4. EXECUTOR METRICS")
    print("=" * 70)
    
    print("""
üñ•Ô∏è  Executors Tab Metrics:

Key Columns:
------------
1. **Executor ID**
   Unique identifier (0 = driver)

2. **Address**
   Host:port where executor runs

3. **Status**
   Active, Dead, Lost
   Look for: Dead executors

4. **RDD Blocks**
   Number of cached RDD partitions

5. **Storage Memory**
   Memory used for cached data
   Look for: Nearing storage memory limit

6. **Disk Used**
   Disk space for spilled data
   Look for: Large disk usage

7. **Cores**
   CPU cores allocated

8. **Active Tasks / Total Tasks**
   Current and historical task counts
   Look for: Imbalanced task distribution

9. **Failed Tasks**
   Number of failed tasks
   Look for: > 0 (indicates issues)

10. **GC Time / Duration**
    GC time as % of execution time
    Look for: > 10%

11. **Shuffle Read / Write**
    Total shuffle data per executor
    Look for: Imbalanced shuffles


üìä Example Executor Dashboard:

Executor | Cores | Memory | Tasks | Failed | GC Time | Shuffle R | Shuffle W
---------|-------|--------|-------|--------|---------|-----------|----------
0        | 4     | 4 GB   | 120   | 0      | 5%      | 2 GB      | 1 GB
1        | 4     | 4 GB   | 115   | 0      | 6%      | 1.9 GB    | 950 MB
2        | 4     | 4 GB   | 10    | 5 ‚ùå   | 45% ‚ùå  | 200 MB    | 100 MB
3        | 4     | 4 GB   | 118   | 0      | 5%      | 2.1 GB    | 1.1 GB

Analysis:
---------
‚ùå Executor 2 problems:
   - Only 10 tasks (underutilized)
   - 5 failed tasks
   - 45% GC time (memory pressure)
   
Action: Check executor logs, may need restart or more memory
    """)


def demonstrate_sql_query_plan(spark):
    """Understand SQL query plans."""
    print("\n" + "=" * 70)
    print("5. SQL QUERY PLANS")
    print("=" * 70)
    
    print("""
üó∫Ô∏è  Query Plan Visualization:

SQL Tab shows:
--------------
1. **Logical Plan**
   High-level operations (filter, join, aggregation)

2. **Optimized Logical Plan**
   After Catalyst optimizer

3. **Physical Plan**
   Actual execution (Exchange = shuffle)

4. **DAG Visualization**
   Visual graph of stages


üîç Reading Physical Plan:

Key Operations:
---------------
- **FileScan**: Read from source
- **Filter**: Filter rows (no shuffle)
- **Project**: Select columns (no shuffle)
- **Exchange**: SHUFFLE POINT! ‚ö†Ô∏è
- **HashAggregate**: Group by aggregation
- **SortMergeJoin**: Join with sort
- **BroadcastHashJoin**: Broadcast join ‚úÖ


Example Plan:
-------------
== Physical Plan ==
*(3) HashAggregate(keys=[category], functions=[count(1)])
+- Exchange hashpartitioning(category, 200) ‚Üê SHUFFLE!
   +- *(2) HashAggregate(keys=[category], functions=[partial_count(1)])
      +- *(2) Project [category]
         +- *(2) Filter (value > 100)
            +- *(1) FileScan parquet [value, category]

Reading the plan:
-----------------
1. FileScan: Read parquet
2. Filter: Filter value > 100 (no shuffle)
3. Partial aggregation (map side)
4. Exchange: SHUFFLE on category key
5. Final aggregation (reduce side)


üéØ Optimization Opportunities:

Before:
-------
Exchange hashpartitioning (200 partitions) ‚Üê Expensive!

After tuning:
-------------
Exchange hashpartitioning (50 partitions) ‚Üê Better!

Or even better:
---------------
BroadcastHashJoin ‚Üê No shuffle at all!
    """)
    
    # Demonstrate explain
    print("\nüíª Example: Using .explain()")
    data = spark.range(10000).toDF("id") \
        .withColumn("category", (col("id") % 10).cast("int"))
    
    result = data.filter(col("id") > 5000).groupBy("category").count()
    
    print("\nPhysical Plan:")
    result.explain()


def demonstrate_monitoring_best_practices():
    """Best practices for cluster monitoring."""
    print("\n" + "=" * 70)
    print("6. MONITORING BEST PRACTICES")
    print("=" * 70)
    
    print("""
üéØ Monitoring Checklist:

Daily Checks:
-------------
1. ‚úÖ Job success rate
   Target: > 99%

2. ‚úÖ Average job duration
   Look for: Increasing trends

3. ‚úÖ Executor utilization
   Target: > 80% task distribution

4. ‚úÖ Memory usage
   Target: < 90% to avoid spills

5. ‚úÖ GC time percentage
   Target: < 10% of execution time


Per-Job Analysis:
-----------------
1. ‚úÖ Check Stages tab for slowest stage
2. ‚úÖ Identify shuffle sizes (should be < 1 GB/task)
3. ‚úÖ Look for spill to disk (should be 0)
4. ‚úÖ Check task time distribution for skew
5. ‚úÖ Verify all executors are being used


üö® Red Flags:

1. ‚ùå Spill to disk > 0
   ‚Üí Increase executor memory or partitions

2. ‚ùå Task time max >> median
   ‚Üí Data skew, use salting

3. ‚ùå GC time > 10%
   ‚Üí Memory pressure, increase executor memory

4. ‚ùå Shuffle read > 10 GB per partition
   ‚Üí Increase spark.sql.shuffle.partitions

5. ‚ùå Failed tasks > 0
   ‚Üí Check executor logs

6. ‚ùå One executor with most tasks
   ‚Üí Poor data distribution

7. ‚ùå Long shuffle read time
   ‚Üí Network or disk bottleneck


üìä Performance Metrics to Track:

Metric                    Good      Warning   Critical
------                    ----      -------   --------
GC Time %                 < 5%      5-10%     > 10%
Spill to Memory          0         < 1 GB    > 1 GB
Spill to Disk            0         > 0       > 5 GB
Task time variance       < 2x      2-5x      > 5x
Shuffle per partition    < 1 GB    1-5 GB    > 5 GB
Executor utilization     > 80%     60-80%    < 60%


üîß Debugging Workflow:

1. Job is slow ‚Üí Check Stages tab
2. Find slowest stage ‚Üí Click stage ID
3. Check metrics:
   - Large shuffle? ‚Üí Use broadcast or pre-partition
   - Spill to disk? ‚Üí Increase memory or partitions
   - Task skew? ‚Üí Use salting technique
   - High GC time? ‚Üí Increase executor memory
4. Apply fix and re-run
5. Compare metrics before/after


üí° Pro Tips:

1. Enable event log for history
   spark.conf.set("spark.eventLog.enabled", "true")
   spark.conf.set("spark.eventLog.dir", "hdfs://logs")

2. Use Spark History Server for past jobs
   spark-history-server.sh start

3. Export metrics to monitoring systems
   - Prometheus + Grafana
   - Datadog
   - CloudWatch (AWS)

4. Set up alerts
   - Job failure alerts
   - Long-running job alerts
   - Resource usage alerts

5. Regular log review
   - Executor logs for errors
   - Driver logs for application issues


üìà Grafana Dashboard Metrics:

Panel 1: Job Success Rate
- Line chart of % successful jobs

Panel 2: Average Job Duration
- Track trends over time

Panel 3: Executor Memory Usage
- Gauge showing % used

Panel 4: Active Executors
- Count of active vs total

Panel 5: Shuffle Read/Write
- Bar chart per job

Panel 6: GC Time %
- Line chart with 10% threshold


üîç Log Analysis Commands:

# Find OOM errors
grep "OutOfMemoryError" executor-*.log

# Find failed tasks
grep "Task.*FAILED" executor-*.log

# Check GC logs
grep "GC" executor-*.log | tail -100

# Find slow tasks
grep "Task.*took" executor-*.log | sort -k4 -n | tail -10

# Check shuffle errors
grep "shuffle" executor-*.log | grep -i error
    """)


def demonstrate_common_issues():
    """Common issues and solutions."""
    print("\n" + "=" * 70)
    print("7. COMMON ISSUES & SOLUTIONS")
    print("=" * 70)
    
    print("""
üêõ Issue 1: Job Stuck / Very Slow

Symptoms:
---------
- Job runs for hours
- Progress bar stuck at same %
- Few active tasks

Diagnosis:
----------
1. Check Stages tab ‚Üí Look for long-running stage
2. Check task distribution ‚Üí All tasks on 1-2 executors?
3. Check shuffle sizes ‚Üí > 10 GB per partition?

Solutions:
----------
‚úÖ Increase spark.sql.shuffle.partitions
‚úÖ Check for data skew (use salting)
‚úÖ Add more executors


üêõ Issue 2: OutOfMemoryError

Symptoms:
---------
- Job fails with OOM
- Executor lost
- Container killed by YARN

Diagnosis:
----------
1. Check Stages tab ‚Üí Spill to disk?
2. Check Executors tab ‚Üí Memory usage near limit?
3. Check task size ‚Üí Processing huge partition?

Solutions:
----------
‚úÖ Increase executor memory: --executor-memory 16g
‚úÖ Increase memory overhead: --executor-memoryOverhead 4g
‚úÖ Increase partitions to reduce per-task data
‚úÖ Don't cache too much data
‚úÖ Use broadcast for small tables


üêõ Issue 3: Data Skew

Symptoms:
---------
- Most tasks finish quickly
- 1-2 tasks take 10x longer
- Median task time: 2s, Max: 60s

Diagnosis:
----------
1. Check Stages tab ‚Üí Task time distribution
2. Click stage ‚Üí Look at task table
3. Identify which partition is large

Solutions:
----------
‚úÖ Use salting technique for skewed keys
‚úÖ Broadcast join if one side is small
‚úÖ Increase partitions


üêõ Issue 4: Shuffle Performance

Symptoms:
---------
- Long shuffle read/write times
- Large shuffle sizes (> 1 GB/task)

Diagnosis:
----------
1. Check Stages tab ‚Üí Shuffle Read/Write columns
2. Check if shuffle is necessary

Solutions:
----------
‚úÖ Filter before shuffle
‚úÖ Use broadcast join for small tables
‚úÖ Pre-partition on join key
‚úÖ Increase spark.sql.shuffle.partitions
‚úÖ Enable compression (default on)


üêõ Issue 5: Executor Lost

Symptoms:
---------
- Executor becomes unresponsive
- Tasks fail and retry
- "Executor lost" in logs

Diagnosis:
----------
1. Check executor logs
2. Look for OOM, killed by YARN, network timeout

Solutions:
----------
‚úÖ Increase executor memory
‚úÖ Increase network timeout: spark.network.timeout=800s
‚úÖ Check node health (disk, memory, network)
‚úÖ Increase heartbeat: spark.executor.heartbeatInterval=30s


üêõ Issue 6: High GC Time

Symptoms:
---------
- GC time > 10% of task time
- Slow task execution
- Frequent GC pauses

Diagnosis:
----------
1. Check Executors tab ‚Üí GC Time column
2. Check memory usage near limit

Solutions:
----------
‚úÖ Increase executor memory
‚úÖ Reduce cached data
‚úÖ Use off-heap memory
‚úÖ Tune GC settings (G1GC recommended)
‚úÖ Increase partitions to reduce per-task memory
    """)


def main():
    spark = create_spark()
    
    print("üìä CLUSTER MONITORING")
    print("=" * 70)
    print("\nMaster Spark UI and performance debugging!")
    print()
    
    demonstrate_spark_ui_overview()
    demonstrate_stages_tab_metrics(spark)
    demonstrate_task_metrics()
    demonstrate_executor_metrics(spark)
    demonstrate_sql_query_plan(spark)
    demonstrate_monitoring_best_practices()
    demonstrate_common_issues()
    
    print("\n" + "=" * 70)
    print("‚úÖ CLUSTER MONITORING DEMO COMPLETE!")
    print("=" * 70)
    print("\nüìù Key Takeaways:")
    print("   1. Always check Stages tab first for bottlenecks")
    print("   2. Look for: Spill, Skew, Large shuffles")
    print("   3. Monitor: GC time < 10%, No spill to disk")
    print("   4. Use .explain() to understand query plans")
    print("   5. Enable event logs for history analysis")
    print("   6. Set up alerts for failures and slow jobs")
    print("   7. Regular log review prevents issues")
    print("\nüñ•Ô∏è  Access Spark UI: http://localhost:4040")
    
    spark.stop()


if __name__ == "__main__":
    main()
