# PySpark Beginner Track ğŸ“

Welcome to PySpark! This track covers fundamentals to build your foundation.

## Prerequisites
- Python basics
- Understanding of data structures (lists, dictionaries)
- No Spark experience needed!

## Learning Path (5 modules, ~2 hours)

### 01. Hello World & DataFrames â­â˜†â˜†â˜†â˜†
**Time:** 15 minutes | **Difficulty:** 1/5
- Create your first DataFrame
- Select, filter, aggregate
- Group by operations
```bash
python 01_hello_world_dataframes.py
```

### 02. Reading & Writing Files â­â­â˜†â˜†â˜†
**Time:** 20 minutes | **Difficulty:** 2/5
- Read CSV, JSON, Parquet
- Write to multiple formats
- File format comparison
```bash
python 02_reading_writing_files.py
```

### 03. Data Cleaning â­â­â˜†â˜†â˜†
**Time:** 25 minutes | **Difficulty:** 2/5
- Handle nulls and duplicates
- Clean string data
- Validate business rules
```bash
python 03_data_cleaning.py
```

### 04. Joins â­â­â­â˜†â˜†
**Time:** 30 minutes | **Difficulty:** 3/5
- Inner, left, right, outer joins
- Practical join patterns
- Customer lifetime value analysis
```bash
python 04_joins.py
```

### 05. Simple ETL Pipeline â­â­â­â˜†â˜†
**Time:** 30 minutes | **Difficulty:** 3/5
- Complete Extract-Transform-Load pipeline
- Data quality checks
- Write to data warehouse
```bash
python 05_simple_etl_pipeline.py
```

## What You'll Learn
âœ… DataFrame basics (create, select, filter)  
âœ… File I/O (CSV, JSON, Parquet)  
âœ… Data cleaning (nulls, duplicates, validation)  
âœ… Joining tables (inner, left, right, outer)  
âœ… Building ETL pipelines  

## Next Steps
After completing this track, move to:
```bash
cd ../intermediate
```

## Tips for Success
- Run each example in order
- Experiment with the code
- Read the WHAT/WHY/HOW comments
- Don't skip examples - they build on each other!

## Need Help?
- Check PySpark documentation: https://spark.apache.org/docs/latest/api/python/
- Review cheatsheet: `../../docs/pyspark_cheatsheet.md`
- Practice notebooks: `../../notebooks/practice/`
