#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
DATABRICKS + MLFLOW - Production ML Platform
================================================================================

MODULE OVERVIEW:
----------------
Databricks provides a unified analytics platform built on Apache Spark.
MLflow is an open-source platform for managing the complete ML lifecycle including:
‚Ä¢ Experiment tracking
‚Ä¢ Model registry
‚Ä¢ Model deployment
‚Ä¢ Reproducibility

This module demonstrates production ML workflows on Databricks with MLflow.

PURPOSE:
--------
Learn Databricks + MLflow patterns:
‚Ä¢ MLflow experiment tracking
‚Ä¢ Model registry and versioning
‚Ä¢ Databricks-specific optimizations
‚Ä¢ Delta Lake integration
‚Ä¢ Production deployment patterns

DATABRICKS ARCHITECTURE:
------------------------
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATABRICKS PLATFORM                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Workspace                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Notebooks  ‚îÇ  Jobs  ‚îÇ  Models  ‚îÇ  Data             ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  Delta Lake (Storage Layer)                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  ACID transactions                                 ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Time travel                                       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Schema enforcement                                ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  Spark Cluster (Compute)                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Auto-scaling                                      ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Spot instances                                    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  GPU support                                       ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                          ‚Üì                                       ‚îÇ
‚îÇ  MLflow (ML Lifecycle)                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Tracking  ‚îÇ  Registry  ‚îÇ  Deployment              ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

MLFLOW WORKFLOW:
----------------
1. Track experiments ‚Üí 2. Register best model ‚Üí 3. Deploy to production
"""

# NOTE: These examples are designed to run on Databricks
# For local development, install: pip install mlflow databricks-cli

try:
    import mlflow
    import mlflow.spark
    from mlflow.models.signature import infer_signature
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("‚ö†Ô∏è  MLflow not installed. Install with: pip install mlflow")

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import os


def create_spark():
    """Create Spark session (works locally and on Databricks)."""
    return SparkSession.builder \
        .appName("Databricks_MLflow") \
        .master("local[*]") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()


def example1_mlflow_tracking():
    """
    EXAMPLE 1: MLflow Experiment Tracking
    ======================================
    
    Track experiments with parameters, metrics, and artifacts.
    """
    print("=" * 70)
    print("EXAMPLE 1: MLflow Experiment Tracking")
    print("=" * 70)
    
    if not MLFLOW_AVAILABLE:
        print("\n‚ö†Ô∏è  MLflow not available")
        return
    
    print("""
    üìä MLFLOW TRACKING WORKFLOW:
    
    1. Start experiment
    2. Log parameters
    3. Train model
    4. Log metrics
    5. Save artifacts
    6. Compare runs
    
    üîß Code Example:
    
    import mlflow
    import mlflow.spark
    
    # Set experiment
    mlflow.set_experiment("/Users/yourname/my-experiment")
    
    # Start run
    with mlflow.start_run(run_name="random_forest_v1"):
        
        # Log parameters
        mlflow.log_param("num_trees", 100)
        mlflow.log_param("max_depth", 10)
        mlflow.log_param("min_instances_per_node", 5)
        
        # Train model
        rf = RandomForestClassifier(
            numTrees=100,
            maxDepth=10,
            minInstancesPerNode=5
        )
        model = rf.fit(train_df)
        
        # Make predictions
        predictions = model.transform(test_df)
        
        # Evaluate
        evaluator = BinaryClassificationEvaluator()
        auc = evaluator.evaluate(predictions)
        accuracy = predictions.filter(col("prediction") == col("label")).count() / predictions.count()
        
        # Log metrics
        mlflow.log_metric("auc", auc)
        mlflow.log_metric("accuracy", accuracy)
        
        # Log model
        mlflow.spark.log_model(
            model, 
            "model",
            signature=infer_signature(train_df, predictions)
        )
        
        # Log artifacts (plots, data, etc.)
        import matplotlib.pyplot as plt
        plt.figure()
        plt.plot([1, 2, 3], [4, 5, 6])
        plt.savefig("plot.png")
        mlflow.log_artifact("plot.png")
    
    üí° Benefits:
    ‚úÖ Track all experiments automatically
    ‚úÖ Compare runs side-by-side
    ‚úÖ Reproduce any experiment
    ‚úÖ Share results with team
    """)


def example2_model_registry():
    """
    EXAMPLE 2: MLflow Model Registry
    =================================
    
    Version and manage models through their lifecycle.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 2: MLflow Model Registry")
    print("=" * 70)
    
    print("""
    üì¶ MODEL REGISTRY WORKFLOW:
    
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           MODEL LIFECYCLE STAGES                     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                                                      ‚îÇ
    ‚îÇ  1. None (default)                                  ‚îÇ
    ‚îÇ     ‚Üì                                                ‚îÇ
    ‚îÇ  2. Staging (testing)                               ‚îÇ
    ‚îÇ     ‚Üì                                                ‚îÇ
    ‚îÇ  3. Production (live)                               ‚îÇ
    ‚îÇ     ‚Üì                                                ‚îÇ
    ‚îÇ  4. Archived (retired)                              ‚îÇ
    ‚îÇ                                                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîß Register Model:
    
    # After training, register model
    model_uri = f"runs:/{run.info.run_id}/model"
    
    mlflow.register_model(
        model_uri=model_uri,
        name="fraud_detection_model"
    )
    
    üîß Transition to Staging:
    
    from mlflow.tracking import MlflowClient
    client = MlflowClient()
    
    client.transition_model_version_stage(
        name="fraud_detection_model",
        version=1,
        stage="Staging"
    )
    
    üîß Load Model from Registry:
    
    # Load staging model
    model = mlflow.pyfunc.load_model(
        "models:/fraud_detection_model/Staging"
    )
    
    # Load production model
    model = mlflow.pyfunc.load_model(
        "models:/fraud_detection_model/Production"
    )
    
    # Load specific version
    model = mlflow.pyfunc.load_model(
        "models:/fraud_detection_model/3"
    )
    
    üîß Production Deployment:
    
    # Test in staging
    staging_model = mlflow.spark.load_model("models:/my_model/Staging")
    test_results = evaluate_model(staging_model, test_data)
    
    # If good, promote to production
    if test_results['auc'] > 0.85:
        client.transition_model_version_stage(
            name="my_model",
            version=3,
            stage="Production"
        )
        print("Model promoted to Production!")
    
    üí° Benefits:
    ‚úÖ Version control for models
    ‚úÖ Staged rollout (staging ‚Üí production)
    ‚úÖ Easy rollback if issues
    ‚úÖ Model lineage tracking
    ‚úÖ Centralized model store
    """)


def example3_delta_lake_integration():
    """
    EXAMPLE 3: Delta Lake Integration
    ==================================
    
    ACID transactions and time travel for data.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 3: Delta Lake Integration")
    print("=" * 70)
    
    print("""
    üóÑÔ∏è  DELTA LAKE FEATURES:
    
    ‚úÖ ACID transactions
    ‚úÖ Time travel (version history)
    ‚úÖ Schema enforcement & evolution
    ‚úÖ Upserts and deletes
    ‚úÖ Streaming + batch unification
    
    üîß Write Delta Table:
    
    # Write DataFrame to Delta
    df.write.format("delta").mode("overwrite").save("/path/to/delta")
    
    # Or create table
    df.write.format("delta").saveAsTable("my_table")
    
    üîß Read Delta Table:
    
    # Read latest version
    df = spark.read.format("delta").load("/path/to/delta")
    
    # Or read table
    df = spark.read.table("my_table")
    
    üîß Time Travel:
    
    # Read specific version
    df = spark.read.format("delta").option("versionAsOf", 5).load("/path/to/delta")
    
    # Read as of timestamp
    df = spark.read.format("delta") \\
        .option("timestampAsOf", "2024-01-01") \\
        .load("/path/to/delta")
    
    # Show history
    spark.sql("DESCRIBE HISTORY my_table").show()
    
    üîß Upserts (Merge):
    
    from delta.tables import DeltaTable
    
    deltaTable = DeltaTable.forPath(spark, "/path/to/delta")
    
    deltaTable.alias("target").merge(
        updates.alias("source"),
        "target.id = source.id"
    ).whenMatchedUpdateAll() \\
     .whenNotMatchedInsertAll() \\
     .execute()
    
    üîß Optimization:
    
    # Optimize (compaction)
    spark.sql("OPTIMIZE my_table")
    
    # Z-order (clustering)
    spark.sql("OPTIMIZE my_table ZORDER BY (user_id, date)")
    
    # Vacuum (delete old files)
    spark.sql("VACUUM my_table RETAIN 168 HOURS")  # 7 days
    
    üí° ML Workflow with Delta:
    
    # 1. Read training data from Delta
    train_df = spark.read.table("features_table")
    
    # 2. Train model
    model = train_pipeline(train_df)
    
    # 3. Score new data
    predictions = model.transform(new_data)
    
    # 4. Write predictions to Delta
    predictions.write.format("delta").mode("append") \\
        .saveAsTable("predictions_table")
    
    # 5. Track with MLflow
    with mlflow.start_run():
        mlflow.log_param("data_version", train_df.version)
        mlflow.log_model(model, "model")
    """)


def example4_databricks_automl():
    """
    EXAMPLE 4: Databricks AutoML
    =============================
    
    Automated machine learning.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 4: Databricks AutoML")
    print("=" * 70)
    
    print("""
    ü§ñ AUTOML WORKFLOW:
    
    1. Load data
    2. Run AutoML
    3. Review results
    4. Deploy best model
    
    üîß Python API:
    
    from databricks import automl
    
    # Classification
    summary = automl.classify(
        dataset=train_df,
        target_col="label",
        primary_metric="f1",
        timeout_minutes=30,
        max_trials=20
    )
    
    # Regression
    summary = automl.regress(
        dataset=train_df,
        target_col="price",
        primary_metric="rmse",
        timeout_minutes=30
    )
    
    # Get best run
    best_trial = summary.best_trial
    
    # Load best model
    model = mlflow.sklearn.load_model(f"runs:/{best_trial.mlflow_run_id}/model")
    
    üìä What AutoML Does:
    
    ‚úÖ Feature preprocessing
    ‚úÖ Algorithm selection
    ‚úÖ Hyperparameter tuning
    ‚úÖ Cross-validation
    ‚úÖ Model explanation
    ‚úÖ Generates notebook with code
    
    üéØ Algorithms Tried:
    
    Classification:
    ‚Ä¢ Logistic Regression
    ‚Ä¢ Decision Tree
    ‚Ä¢ Random Forest
    ‚Ä¢ XGBoost
    ‚Ä¢ LightGBM
    
    Regression:
    ‚Ä¢ Linear Regression
    ‚Ä¢ Decision Tree
    ‚Ä¢ Random Forest
    ‚Ä¢ XGBoost
    ‚Ä¢ LightGBM
    
    üí° Best Practices:
    
    ‚úÖ Clean data beforehand
    ‚úÖ Set timeout appropriately
    ‚úÖ Review generated notebook
    ‚úÖ Customize if needed
    ‚úÖ Use as baseline
    """)


def example5_production_deployment():
    """
    EXAMPLE 5: Production Deployment Patterns
    ==========================================
    
    Deploy models to production on Databricks.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 5: Production Deployment")
    print("=" * 70)
    
    print("""
    üöÄ DEPLOYMENT OPTIONS:
    
    1. Batch Inference (Scheduled Jobs)
    2. Real-time Inference (Model Serving)
    3. Streaming Inference (Structured Streaming)
    
    üìä OPTION 1: Batch Inference Job
    
    # inference_job.py
    import mlflow
    
    # Load production model
    model = mlflow.spark.load_model("models:/my_model/Production")
    
    # Load new data
    new_data = spark.read.table("new_customers")
    
    # Make predictions
    predictions = model.transform(new_data)
    
    # Write results
    predictions.write.format("delta").mode("append") \\
        .saveAsTable("predictions")
    
    # Schedule in Databricks:
    # Jobs ‚Üí Create Job ‚Üí Schedule: Daily at 2 AM
    
    üìä OPTION 2: Real-time Model Serving
    
    # Enable model serving in UI
    # Or via API:
    
    from databricks.sdk import WorkspaceClient
    
    w = WorkspaceClient()
    
    endpoint = w.serving_endpoints.create(
        name="fraud-detection",
        config={
            "served_models": [{
                "model_name": "fraud_detection_model",
                "model_version": "1",
                "workload_size": "Small",
                "scale_to_zero_enabled": True
            }]
        }
    )
    
    # Query endpoint:
    
    import requests
    
    response = requests.post(
        "https://<workspace>.cloud.databricks.com/serving-endpoints/fraud-detection/invocations",
        headers={"Authorization": f"Bearer {token}"},
        json={"dataframe_records": [{"feature1": 1, "feature2": 2}]}
    )
    
    üìä OPTION 3: Streaming Inference
    
    # Load model
    model = mlflow.spark.load_model("models:/my_model/Production")
    
    # Read stream
    stream_df = spark.readStream \\
        .format("delta") \\
        .table("incoming_events")
    
    # Apply model
    predictions_stream = model.transform(stream_df)
    
    # Write stream
    query = predictions_stream.writeStream \\
        .format("delta") \\
        .outputMode("append") \\
        .option("checkpointLocation", "/checkpoints/predictions") \\
        .table("real_time_predictions")
    
    query.awaitTermination()
    
    üîß MONITORING AND ALERTING:
    
    # Track prediction metrics
    with mlflow.start_run():
        mlflow.log_metric("predictions_count", predictions.count())
        mlflow.log_metric("avg_confidence", 
            predictions.select(avg("probability")).first()[0])
    
    # Set alerts in Databricks:
    # SQL Warehouse ‚Üí Create Alert
    # Example: Alert if prediction count drops below threshold
    
    üí° PRODUCTION CHECKLIST:
    
    ‚úÖ Model versioned and tested
    ‚úÖ Data pipeline validated
    ‚úÖ Monitoring configured
    ‚úÖ Alerts set up
    ‚úÖ Rollback plan ready
    ‚úÖ Documentation updated
    ‚úÖ Stakeholders notified
    """)


def show_databricks_best_practices():
    """Best practices for Databricks + MLflow."""
    print("\n" + "=" * 70)
    print("DATABRICKS + MLFLOW BEST PRACTICES")
    print("=" * 70)
    
    print("""
    üéØ BEST PRACTICES:
    
    1. EXPERIMENT MANAGEMENT
       ‚úÖ Use meaningful experiment names
       ‚úÖ Tag runs with metadata
       ‚úÖ Document parameters and metrics
       ‚úÖ Save artifacts (plots, data samples)
       ‚ùå Don't leave experiments unorganized
    
    2. MODEL VERSIONING
       ‚úÖ Use Model Registry
       ‚úÖ Stage: None ‚Üí Staging ‚Üí Production
       ‚úÖ Add model descriptions
       ‚úÖ Test before promoting
       ‚ùå Don't skip staging
    
    3. DATA MANAGEMENT
       ‚úÖ Use Delta Lake for all tables
       ‚úÖ Partition large tables
       ‚úÖ Optimize regularly
       ‚úÖ Vacuum old files
       ‚ùå Don't use CSV for production
    
    4. CLUSTER CONFIGURATION
       ‚úÖ Use autoscaling
       ‚úÖ Spot instances for dev/test
       ‚úÖ GPU clusters for deep learning
       ‚úÖ Right-size executors
       ‚ùå Don't over-provision
    
    5. COST OPTIMIZATION
       ‚úÖ Auto-terminate idle clusters
       ‚úÖ Use pools for faster startup
       ‚úÖ Spot instances (70% cheaper)
       ‚úÖ Right-size clusters
       ‚úÖ Cache frequently used data
    
    üìä COST SAVINGS TIPS:
    
    # 1. Auto-terminate
    spark.conf.set("spark.databricks.cluster.autoTermination.enabled", "true")
    spark.conf.set("spark.databricks.cluster.autoTermination.minutes", "30")
    
    # 2. Cache data
    df.cache()
    df.count()  # Materialize cache
    
    # 3. Optimize queries
    df.repartition(200)  # Optimal partition count
    df.coalesce(10)      # Reduce partitions for small data
    
    # 4. Use Delta caching
    spark.sql("CACHE SELECT * FROM my_table")
    
    üöÄ PERFORMANCE TIPS:
    
    1. Photon Engine:
       Enable for 2-3x speedup on SQL queries
    
    2. Adaptive Query Execution:
       spark.conf.set("spark.sql.adaptive.enabled", "true")
    
    3. Z-ordering:
       OPTIMIZE my_table ZORDER BY (user_id, date)
    
    4. Broadcast joins:
       df.join(broadcast(small_df), "key")
    
    5. Predicate pushdown:
       Use filters early in pipeline
    
    üìã CHECKLIST FOR PRODUCTION:
    
    ‚úÖ Code reviewed and tested
    ‚úÖ Model accuracy > threshold
    ‚úÖ Data quality checks passed
    ‚úÖ Performance benchmarked
    ‚úÖ Monitoring configured
    ‚úÖ Alerts set up
    ‚úÖ Documentation complete
    ‚úÖ Rollback plan ready
    ‚úÖ Stakeholders notified
    ‚úÖ Cost estimated
    """)


def main():
    """Run all Databricks + MLflow examples."""
    print("üß± DATABRICKS + MLFLOW PLATFORM")
    print("=" * 70)
    print("\nProduction ML platform with experiment tracking and deployment!")
    
    # Check MLflow availability
    if MLFLOW_AVAILABLE:
        print("\n‚úÖ MLflow available")
    else:
        print("\n‚ö†Ô∏è  MLflow not available")
        print("   Install: pip install mlflow databricks-cli")
    
    # Run examples
    example1_mlflow_tracking()
    example2_model_registry()
    example3_delta_lake_integration()
    example4_databricks_automl()
    example5_production_deployment()
    
    # Show best practices
    show_databricks_best_practices()
    
    print("\n" + "=" * 70)
    print("‚úÖ DATABRICKS + MLFLOW EXAMPLES COMPLETE")
    print("=" * 70)
    print("\nüìù Key Takeaways:")
    print("   1. MLflow tracks experiments and models")
    print("   2. Model Registry manages lifecycle")
    print("   3. Delta Lake provides ACID + time travel")
    print("   4. AutoML accelerates development")
    print("   5. Multiple deployment options")
    print("   6. Monitor and optimize costs")
    
    print("\nüìö See Also:")
    print("   ‚Ä¢ 01_ml_pipelines.py - MLlib pipelines")
    print("   ‚Ä¢ ../gpu_acceleration/ - GPU acceleration")
    
    print("\nüîó Resources:")
    print("   ‚Ä¢ MLflow: https://mlflow.org")
    print("   ‚Ä¢ Databricks: https://docs.databricks.com")
    print("   ‚Ä¢ Delta Lake: https://delta.io")


if __name__ == "__main__":
    main()
