# ğŸ”¥ PySpark Monitoring Dashboard

Modern, real-time web dashboard for monitoring Apache Spark clusters built with Next.js, React, and TypeScript.

## âœ¨ Features

### ğŸ“Š Real-Time Monitoring
- **Auto-refresh** with configurable intervals (2s, 5s, 10s, 30s)
- **Live metrics** from Spark REST API
- **Real-time charts** showing resource utilization
- **WebSocket-ready** architecture for instant updates

### ğŸ“ˆ Comprehensive Metrics
- **Cluster Overview**: Workers, CPU cores, memory, active applications
- **Worker Status**: Individual worker health and resource usage
- **Application Tracking**: Active and completed applications
- **Live Jobs**: Running jobs with detailed metrics
- **Performance Charts**: CPU, memory, and application trends

### ğŸ¨ Modern UI
- **Dark theme** with Spark brand colors
- **Responsive design** for mobile, tablet, and desktop
- **Gradient cards** with smooth animations
- **Status indicators** with color coding
- **Loading states** and error handling

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+ 
- npm or yarn
- Running Spark cluster (see ../docker/spark-cluster)

### Installation

```bash
# Install dependencies
npm install

# Copy environment variables
cp .env.example .env

# Edit .env with your Spark URLs
nano .env
```

### Configuration

Edit `.env` file:

```env
NEXT_PUBLIC_SPARK_MASTER_URL=http://localhost:9080
NEXT_PUBLIC_SPARK_APP_URL=http://localhost:4040
NEXT_PUBLIC_PROMETHEUS_URL=http://localhost:9090
```

### Development

```bash
# Start development server
npm run dev

# Open browser
open http://localhost:3000
```

### Production

```bash
# Build for production
npm run build

# Start production server
npm start
```

## ğŸ³ Docker Deployment

### Build Docker Image

```bash
docker build -t pyspark-monitoring-dashboard .
```

### Run with Docker

```bash
docker run -p 3000:3000 \
  -e NEXT_PUBLIC_SPARK_MASTER_URL=http://spark-master:9080 \
  -e NEXT_PUBLIC_SPARK_APP_URL=http://spark-master:4040 \
  pyspark-monitoring-dashboard
```

### Docker Compose (Recommended)

See `docker-compose.monitoring.yml` for full stack deployment including:
- Spark Cluster (Master + 3 Workers)
- Prometheus (metrics storage)
- Monitoring Dashboard (this app)

```bash
docker-compose -f docker-compose.monitoring.yml up -d
```

## ğŸ“‚ Project Structure

```
monitoring_frontend/
â”œâ”€â”€ components/           # React components
â”‚   â”œâ”€â”€ ClusterOverview.tsx    # Cluster stats cards
â”‚   â”œâ”€â”€ WorkersList.tsx        # Workers table
â”‚   â”œâ”€â”€ ApplicationsList.tsx   # Applications list
â”‚   â”œâ”€â”€ LiveJobs.tsx          # Active jobs
â”‚   â””â”€â”€ MetricsChart.tsx      # Performance charts
â”œâ”€â”€ lib/                  # Utilities
â”‚   â””â”€â”€ sparkApi.ts       # Spark REST API client
â”œâ”€â”€ pages/               # Next.js pages
â”‚   â”œâ”€â”€ index.tsx        # Main dashboard
â”‚   â”œâ”€â”€ _app.tsx         # App wrapper
â”‚   â””â”€â”€ _document.tsx    # HTML document
â”œâ”€â”€ styles/              # CSS styles
â”‚   â””â”€â”€ globals.css      # Global styles
â”œâ”€â”€ public/              # Static assets
â”œâ”€â”€ package.json         # Dependencies
â”œâ”€â”€ tsconfig.json        # TypeScript config
â”œâ”€â”€ tailwind.config.js   # Tailwind CSS config
â””â”€â”€ next.config.js       # Next.js config
```

## ğŸ”§ Configuration

### Auto-Refresh Settings

The dashboard auto-refreshes data at configurable intervals:

- **2 seconds**: Fastest, best for monitoring active jobs
- **5 seconds**: Default, balanced performance
- **10 seconds**: Slower refresh, reduces API calls
- **30 seconds**: Minimal refresh for overview monitoring

Toggle auto-refresh on/off with the switch in the header.

### API Endpoints

The dashboard uses these Spark REST API endpoints:

```
# Cluster Status
GET http://localhost:9080/json/

# Applications
GET http://localhost:4040/api/v1/applications

# Jobs (per application)
GET http://localhost:4040/api/v1/applications/{app-id}/jobs

# Stages (per application)
GET http://localhost:4040/api/v1/applications/{app-id}/stages

# Executors (per application)
GET http://localhost:4040/api/v1/applications/{app-id}/executors
```

## ğŸ“Š Metrics Displayed

### Cluster Overview
- Number of workers
- Total/used CPU cores
- Total/used memory
- Active applications count

### Worker Details
- Worker hostname and web UI
- Health status (ALIVE/DEAD)
- CPU cores (used/total)
- Memory (used/total)

### Application Tracking
- Application ID and name
- State (RUNNING/COMPLETED/FAILED)
- CPU cores allocated
- Memory per executor
- Duration/runtime

### Performance Charts
- CPU utilization over time
- Memory utilization over time
- Active applications count

## ğŸ¯ Use Cases

### Development
Monitor local Spark jobs during development

### Testing
Track performance metrics during load testing

### Production
Real-time monitoring of production Spark clusters

### Demo/Training
Visualize Spark concepts for learning

## ğŸ”Œ Integration with Prometheus

For historical metrics and alerting, integrate with Prometheus:

1. Configure Prometheus to scrape Spark metrics
2. Set `NEXT_PUBLIC_PROMETHEUS_URL` in `.env`
3. Access Prometheus at http://localhost:9090

See `../prometheus/prometheus.yml` for configuration.

## ğŸ› ï¸ Tech Stack

- **Framework**: Next.js 14
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Charts**: Recharts
- **Data Fetching**: SWR (stale-while-revalidate)
- **HTTP Client**: Axios
- **Icons**: Lucide React

## ğŸ“– API Reference

### `lib/sparkApi.ts`

#### Functions

```typescript
// Fetch cluster status
fetchClusterStatus(): Promise<ClusterStatus>

// Fetch applications
fetchApplications(appId?: string): Promise<Application[]>

// Fetch jobs for an application
fetchJobs(appId: string): Promise<SparkJob[]>

// Fetch stages for an application
fetchStages(appId: string): Promise<SparkStage[]>

// Fetch executors for an application
fetchExecutors(appId: string): Promise<Executor[]>
```

#### Utility Functions

```typescript
formatBytes(bytes: number): string
formatDuration(ms: number): string
formatTimestamp(timestamp: string | number): string
getStatusColor(status: string): string
getStatusBadgeColor(status: string): string
```

## ğŸ› Troubleshooting

### Dashboard shows "Failed to load cluster status"

**Solution**: Check that Spark Master is running on port 9080:
```bash
curl http://localhost:9080/json/
```

### No applications shown

**Solution**: Make sure Spark Application UI is accessible on port 4040:
```bash
curl http://localhost:4040/api/v1/applications
```

### CORS errors in browser console

**Solution**: Add CORS headers to Spark configuration or use the Next.js proxy in `next.config.js` (already configured).

### Charts not updating

**Solution**: 
1. Check auto-refresh is enabled (toggle in header)
2. Verify refresh interval is not 0
3. Check browser console for errors

## ğŸš€ Performance Tips

### Optimize for Large Clusters

1. **Increase refresh interval** to 10s or 30s
2. **Limit history** in charts (modify `MetricsChart.tsx`)
3. **Enable pagination** for workers/applications lists

### Reduce API Calls

1. **Disable auto-refresh** when not actively monitoring
2. **Use longer intervals** (30s instead of 2s)
3. **Cache data** with SWR (already implemented)

## ğŸ” Security

### Production Deployment

- **Use HTTPS** for all connections
- **Add authentication** (integrate with corporate SSO)
- **Restrict API access** to authorized networks
- **Enable CORS** only for trusted origins
- **Use environment variables** for sensitive config

### Example Nginx Proxy

```nginx
server {
    listen 443 ssl;
    server_name spark-monitor.company.com;
    
    location / {
        proxy_pass http://localhost:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    # Add authentication
    auth_basic "Spark Monitoring";
    auth_basic_user_file /etc/nginx/.htpasswd;
}
```

## ğŸ“ Future Enhancements

- [ ] WebSocket support for real-time updates
- [ ] Alerting system for failed jobs
- [ ] Historical metrics (24h, 7d, 30d)
- [ ] SQL query visualization
- [ ] DAG visualization
- [ ] Multi-cluster support
- [ ] Custom dashboards
- [ ] Export metrics to CSV
- [ ] Dark/Light theme toggle
- [ ] Mobile app (React Native)

## ğŸ¤ Contributing

This is part of a PySpark learning project. Feel free to:
- Report bugs
- Suggest features
- Submit pull requests
- Improve documentation

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”— Related

- **Spark Cluster**: `../docker/spark-cluster/` - Docker cluster setup
- **Prometheus**: `../prometheus/` - Metrics collection
- **Examples**: `../examples/` - PySpark example jobs

## ğŸ’¡ Tips

1. **Start with slow refresh** (30s) then speed up if needed
2. **Open browser dev tools** to see API requests
3. **Run demo jobs** from docker/spark-cluster/apps/
4. **Check Spark UI** at http://localhost:4040 for detailed metrics
5. **Use split screen** - dashboard on left, Spark UI on right

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section above
2. Review Spark REST API docs
3. Check browser console for errors
4. Verify Spark cluster is running

---

**Built with â¤ï¸ for PySpark Learning**
