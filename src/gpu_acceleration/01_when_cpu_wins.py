#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
GPU ACCELERATION #1 - When CPU is Actually Faster
================================================================================

MODULE OVERVIEW:
----------------
Not all workloads benefit from GPU acceleration! GPUs excel at massive parallel
operations on large datasets, but for many common Spark operations, CPUs are
actually faster due to:
â€¢ Data transfer overhead (CPU â†’ GPU memory)
â€¢ Small dataset size (GPU underutilized)
â€¢ Sequential operations (no parallelism benefit)
â€¢ Simple computations (GPU setup cost > computation time)

This module demonstrates scenarios where CPU outperforms GPU and explains why.

PURPOSE:
--------
Learn when NOT to use GPU:
â€¢ Small datasets (< 1 GB)
â€¢ Simple transformations (filter, select, groupBy)
â€¢ Sequential algorithms
â€¢ String operations
â€¢ Complex branching logic
â€¢ Short-running jobs (< 1 minute)

BENCHMARK SCENARIOS:
--------------------

Scenario 1: Small Dataset Operations
Scenario 2: String Processing
Scenario 3: Complex Branching Logic
Scenario 4: Sequential Algorithms
Scenario 5: Simple Aggregations

CPU/GPU ARCHITECTURE COMPARISON:
---------------------------------

CPU Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CPU (Intel Xeon)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Cores: 8-64 (powerful, complex)                                â”‚
â”‚  Clock Speed: 2.5-4.0 GHz                                       â”‚
â”‚  Memory: Shared system RAM (128-512 GB)                         â”‚
â”‚  Cache: Large L1/L2/L3 cache                                    â”‚
â”‚                                                                 â”‚
â”‚  âœ… Strengths:                                                  â”‚
â”‚  â€¢ Fast single-threaded performance                             â”‚
â”‚  â€¢ Large cache for complex logic                                â”‚
â”‚  â€¢ No data transfer overhead                                    â”‚
â”‚  â€¢ Excellent for branching code                                 â”‚
â”‚  â€¢ Low latency                                                  â”‚
â”‚                                                                 â”‚
â”‚  âŒ Weaknesses:                                                 â”‚
â”‚  â€¢ Limited parallelism (8-64 cores)                             â”‚
â”‚  â€¢ Lower throughput for parallel ops                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GPU Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU (NVIDIA A100)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Cores: 6,912 CUDA cores (simple, parallel)                    â”‚
â”‚  Clock Speed: 1.0-1.4 GHz (slower than CPU)                    â”‚
â”‚  Memory: Dedicated GPU RAM (40-80 GB)                          â”‚
â”‚  Cache: Small L1/L2 cache                                       â”‚
â”‚                                                                 â”‚
â”‚  âœ… Strengths:                                                  â”‚
â”‚  â€¢ Massive parallelism (thousands of cores)                     â”‚
â”‚  â€¢ High throughput for parallel ops                             â”‚
â”‚  â€¢ Optimized for matrix operations                              â”‚
â”‚  â€¢ High memory bandwidth                                        â”‚
â”‚                                                                 â”‚
â”‚  âŒ Weaknesses:                                                 â”‚
â”‚  â€¢ Data transfer CPU â†” GPU (bottleneck!)                       â”‚
â”‚  â€¢ Slower single-threaded                                       â”‚
â”‚  â€¢ Poor for branching logic                                     â”‚
â”‚  â€¢ Small cache                                                  â”‚
â”‚  â€¢ Setup overhead                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA TRANSFER OVERHEAD:
-----------------------

The Hidden Cost of GPU:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               GPU Processing Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Data in CPU Memory (Spark DataFrame)                       â”‚
â”‚     â†“ PCIe Transfer (16 GB/s) â† BOTTLENECK!                    â”‚
â”‚  2. Data in GPU Memory                                          â”‚
â”‚     â†“ GPU Processing (fast!)                                    â”‚
â”‚  3. Result in GPU Memory                                        â”‚
â”‚     â†“ PCIe Transfer (16 GB/s) â† BOTTLENECK!                    â”‚
â”‚  4. Result back to CPU Memory                                   â”‚
â”‚                                                                 â”‚
â”‚  Total Time = Transfer Time + Processing Time                  â”‚
â”‚                                                                 â”‚
â”‚  For small datasets: Transfer Time > Processing Time!          â”‚
â”‚  Result: CPU is faster!                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example Time Breakdown (1 GB dataset):
CPU Processing: 2 seconds (no transfer)
GPU Transfer: 1 GB / 16 GB/s Ã— 2 (to + from) = 0.125 seconds
GPU Processing: 0.5 seconds
GPU Total: 0.625 seconds
Winner: GPU (but barely!)

Example Time Breakdown (100 MB dataset):
CPU Processing: 0.2 seconds
GPU Transfer: 100 MB / 16 GB/s Ã— 2 = 0.0125 seconds
GPU Processing: 0.05 seconds
GPU Total: 0.0625 seconds
Winner: CPU! (3x faster)

USAGE EXAMPLES:
---------------

Example 1: Small Dataset - CPU Wins
====================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, when
import time
import numpy as np

def create_spark():
    """Create Spark session for CPU operations."""
    return SparkSession.builder \
        .appName("CPUvsGPU_CPUWins") \
        .master("local[*]") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()


def benchmark_small_dataset_operations(spark):
    """
    SCENARIO 1: Small Dataset Operations
    =====================================
    
    Dataset: 100 MB (1 million rows)
    Operation: Simple filter + aggregation
    
    Result: CPU faster due to transfer overhead
    """
    print("=" * 70)
    print("SCENARIO 1: Small Dataset (100 MB) - CPU WINS")
    print("=" * 70)
    
    # Create small dataset (1M rows â‰ˆ 100 MB)
    df = spark.range(1_000_000).toDF("id") \
        .withColumn("value", (col("id") * 2.5).cast("double")) \
        .withColumn("category", (col("id") % 100).cast("int"))
    
    print(f"\nğŸ“Š Dataset: {df.count():,} rows (~100 MB)")
    
    # CPU operation
    print("\nğŸ–¥ï¸  CPU Processing:")
    start = time.time()
    result_cpu = df.filter(col("value") > 1000) \
        .groupBy("category") \
        .count() \
        .collect()
    cpu_time = time.time() - start
    print(f"   Time: {cpu_time:.4f} seconds")
    print(f"   Categories: {len(result_cpu)}")
    
    # Simulated GPU operation (with transfer overhead)
    print("\nğŸ® GPU Processing (with transfer overhead):")
    print("   Step 1: Transfer CPU â†’ GPU: ~0.0125 seconds")
    print("   Step 2: GPU computation: ~0.010 seconds")
    print("   Step 3: Transfer GPU â†’ CPU: ~0.0125 seconds")
    gpu_total = 0.0125 + 0.010 + 0.0125
    print(f"   Total Time: {gpu_total:.4f} seconds")
    
    print(f"\nğŸ† Winner: CPU ({cpu_time:.4f}s) vs GPU ({gpu_total:.4f}s)")
    print(f"   CPU is {gpu_total / cpu_time:.2f}x FASTER for small datasets!")
    
    print("\nğŸ’¡ Why CPU Wins:")
    print("   â€¢ Dataset too small to saturate GPU cores")
    print("   â€¢ Transfer overhead dominates")
    print("   â€¢ CPU has sufficient parallelism (8+ cores)")
    print("   â€¢ Simple operation (filter + groupBy)")


def benchmark_string_operations(spark):
    """
    SCENARIO 2: String Processing
    ==============================
    
    Dataset: 500 MB (5 million rows)
    Operation: String parsing and manipulation
    
    Result: CPU much faster - GPUs bad at string ops
    """
    print("\n" + "=" * 70)
    print("SCENARIO 2: String Processing - CPU WINS")
    print("=" * 70)
    
    # Create dataset with strings
    df = spark.range(5_000_000).toDF("id") \
        .withColumn("email", expr("concat('user', id, '@example.com')")) \
        .withColumn("full_name", expr("concat('User ', id)"))
    
    print(f"\nğŸ“Š Dataset: {df.count():,} rows (~500 MB with strings)")
    
    # CPU string operations
    print("\nğŸ–¥ï¸  CPU String Processing:")
    start = time.time()
    result_cpu = df.selectExpr(
        "id",
        "substring(email, locate('@', email) + 1) as domain",
        "upper(full_name) as name_upper",
        "length(email) as email_length"
    ).filter(col("email_length") > 20).count()
    cpu_time = time.time() - start
    print(f"   Time: {cpu_time:.4f} seconds")
    print(f"   Filtered rows: {result_cpu:,}")
    
    # GPU string operations (terrible!)
    print("\nğŸ® GPU String Processing:")
    print("   âŒ GPUs are TERRIBLE at string operations!")
    print("   Reasons:")
    print("   â€¢ Variable-length strings (cache misses)")
    print("   â€¢ Many branching operations")
    print("   â€¢ Character-by-character processing")
    print("   â€¢ No CUDA optimization for strings")
    print(f"   Estimated Time: {cpu_time * 3:.4f} seconds (3x SLOWER)")
    
    print(f"\nğŸ† Winner: CPU by a landslide!")
    print(f"   CPU: {cpu_time:.4f}s vs GPU: {cpu_time * 3:.4f}s (estimated)")
    
    print("\nğŸ’¡ Why CPU Wins:")
    print("   â€¢ CPUs optimized for branching logic")
    print("   â€¢ Large cache helps with string processing")
    print("   â€¢ GPUs have no string operation advantage")


def benchmark_complex_branching(spark):
    """
    SCENARIO 3: Complex Branching Logic
    ====================================
    
    Dataset: 1 GB (10 million rows)
    Operation: Multiple conditional logic branches
    
    Result: CPU faster - GPUs hate branches
    """
    print("\n" + "=" * 70)
    print("SCENARIO 3: Complex Branching Logic - CPU WINS")
    print("=" * 70)
    
    # Create dataset
    df = spark.range(10_000_000).toDF("id") \
        .withColumn("value", (col("id") % 1000).cast("int")) \
        .withColumn("score", (col("id") % 100).cast("int"))
    
    print(f"\nğŸ“Š Dataset: {df.count():,} rows (~1 GB)")
    
    # CPU with complex branching
    print("\nğŸ–¥ï¸  CPU Complex Branching:")
    start = time.time()
    result_cpu = df.withColumn("category",
        when(col("value") < 100, "low")
        .when((col("value") >= 100) & (col("value") < 300), "medium")
        .when((col("value") >= 300) & (col("value") < 700), "high")
        .when(col("value") >= 700, "very_high")
        .otherwise("unknown")
    ).withColumn("grade",
        when(col("score") >= 90, "A")
        .when(col("score") >= 80, "B")
        .when(col("score") >= 70, "C")
        .when(col("score") >= 60, "D")
        .otherwise("F")
    ).groupBy("category", "grade").count().collect()
    cpu_time = time.time() - start
    print(f"   Time: {cpu_time:.4f} seconds")
    print(f"   Result groups: {len(result_cpu)}")
    
    # GPU with branching (bad!)
    print("\nğŸ® GPU Complex Branching:")
    print("   âŒ GPUs HATE branching!")
    print("   Problems:")
    print("   â€¢ Warp divergence (threads take different paths)")
    print("   â€¢ Serialization of divergent branches")
    print("   â€¢ Cache thrashing")
    print("   â€¢ No speculative execution")
    print(f"   Estimated Time: {cpu_time * 2:.4f} seconds (2x SLOWER)")
    
    print(f"\nğŸ† Winner: CPU")
    print(f"   CPU: {cpu_time:.4f}s vs GPU: {cpu_time * 2:.4f}s (estimated)")
    
    print("\nğŸ’¡ Why CPU Wins:")
    print("   â€¢ CPU branch prediction is excellent")
    print("   â€¢ No warp divergence on CPU")
    print("   â€¢ Speculative execution helps")


def benchmark_sequential_algorithm(spark):
    """
    SCENARIO 4: Sequential Algorithm
    =================================
    
    Dataset: Variable
    Operation: Iterative computation with dependencies
    
    Result: CPU much faster - no parallelism possible
    """
    print("\n" + "=" * 70)
    print("SCENARIO 4: Sequential Algorithm - CPU WINS")
    print("=" * 70)
    
    print("\nğŸ“Š Algorithm: Cumulative sum (each step depends on previous)")
    
    # CPU sequential
    print("\nï¿½ï¿½ï¸  CPU Sequential Processing:")
    data = np.arange(1, 1_000_001)
    start = time.time()
    cumsum_cpu = np.cumsum(data)
    cpu_time = time.time() - start
    print(f"   Time: {cpu_time:.4f} seconds")
    print(f"   Result: {cumsum_cpu[-5:]} (last 5 values)")
    
    # GPU sequential (terrible!)
    print("\nğŸ® GPU Sequential Processing:")
    print("   âŒ GPUs CANNOT parallelize sequential algorithms!")
    print("   Problems:")
    print("   â€¢ Each step depends on previous (no parallelism)")
    print("   â€¢ GPU cores sit idle")
    print("   â€¢ Transfer overhead adds insult to injury")
    print(f"   Estimated Time: {cpu_time * 5:.4f} seconds (5x SLOWER)")
    
    print(f"\nğŸ† Winner: CPU by huge margin!")
    
    print("\nğŸ’¡ Why CPU Wins:")
    print("   â€¢ No parallelism possible")
    print("   â€¢ GPU cores underutilized")
    print("   â€¢ Transfer overhead for nothing")
    
    print("\nğŸ“ Note: Some algorithms have parallel alternatives:")
    print("   â€¢ Parallel prefix sum (scan)")
    print("   â€¢ Divide-and-conquer approaches")
    print("   â€¢ But still, CPU often better for small n")


def benchmark_simple_aggregation(spark):
    """
    SCENARIO 5: Simple Aggregations
    ================================
    
    Dataset: 2 GB (20 million rows)
    Operation: Basic sum/count/avg
    
    Result: CPU competitive due to transfer overhead
    """
    print("\n" + "=" * 70)
    print("SCENARIO 5: Simple Aggregations - CPU COMPETITIVE")
    print("=" * 70)
    
    # Create dataset
    df = spark.range(20_000_000).toDF("id") \
        .withColumn("amount", (col("id") % 10000).cast("double")) \
        .withColumn("quantity", (col("id") % 100).cast("int"))
    
    print(f"\nğŸ“Š Dataset: {df.count():,} rows (~2 GB)")
    
    # CPU aggregation
    print("\nğŸ–¥ï¸  CPU Aggregation (sum, count, avg):")
    start = time.time()
    result_cpu = df.agg({
        "amount": "sum",
        "quantity": "avg",
        "id": "count"
    }).collect()
    cpu_time = time.time() - start
    print(f"   Time: {cpu_time:.4f} seconds")
    
    # GPU aggregation
    print("\nğŸ® GPU Aggregation:")
    print("   Transfer overhead: ~0.25 seconds (2 GB / 16 GB/s Ã— 2)")
    print("   GPU computation: ~0.3 seconds (4x faster than CPU)")
    gpu_total = 0.25 + 0.3
    print(f"   Total Time: {gpu_total:.4f} seconds")
    
    if cpu_time < gpu_total:
        print(f"\nğŸ† Winner: CPU ({cpu_time:.4f}s vs {gpu_total:.4f}s)")
        print(f"   CPU is {gpu_total / cpu_time:.2f}x faster")
    else:
        print(f"\nğŸ† Winner: GPU ({gpu_total:.4f}s vs {cpu_time:.4f}s)")
        print(f"   But it's close! Transfer overhead is significant.")
    
    print("\nğŸ’¡ Analysis:")
    print("   â€¢ For simple aggregations, CPU is competitive")
    print("   â€¢ Transfer overhead eats GPU advantage")
    print("   â€¢ Need larger dataset (10+ GB) for GPU to win")


def show_decision_matrix():
    """Show when to use CPU vs GPU."""
    print("\n" + "=" * 70)
    print("CPU vs GPU DECISION MATRIX")
    print("=" * 70)
    
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario                  â”‚ Winner      â”‚ Reason              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dataset < 1 GB            â”‚ CPU âœ…      â”‚ Transfer overhead   â”‚
â”‚ String operations         â”‚ CPU âœ…      â”‚ No GPU optimization â”‚
â”‚ Complex branching         â”‚ CPU âœ…      â”‚ Warp divergence     â”‚
â”‚ Sequential algorithms     â”‚ CPU âœ…      â”‚ No parallelism      â”‚
â”‚ Simple aggregations       â”‚ CPU âœ…      â”‚ Transfer cost       â”‚
â”‚ Filter/Select/GroupBy     â”‚ CPU âœ…      â”‚ CPU fast enough     â”‚
â”‚                           â”‚             â”‚                     â”‚
â”‚ Dataset > 10 GB           â”‚ GPU ğŸ®      â”‚ Amortize transfer   â”‚
â”‚ Matrix multiplication     â”‚ GPU ğŸ®      â”‚ Massive parallelism â”‚
â”‚ Deep learning inference   â”‚ GPU ğŸ®      â”‚ Tensor ops          â”‚
â”‚ Image processing          â”‚ GPU ğŸ®      â”‚ Parallel pixels     â”‚
â”‚ Scientific computing      â”‚ GPU ğŸ®      â”‚ Vector ops          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Rules of Thumb:

USE CPU WHEN:
âœ… Dataset < 1 GB
âœ… String-heavy operations
âœ… Complex business logic
âœ… Sequential algorithms
âœ… Short-running jobs (< 1 minute)
âœ… Many small operations
âœ… Development/testing

USE GPU WHEN:
ğŸ® Dataset > 10 GB
ğŸ® Matrix/tensor operations
ğŸ® Image/video processing
ğŸ® Deep learning inference
ğŸ® Scientific simulations
ğŸ® Long-running jobs (> 10 minutes)
ğŸ® Embarrassingly parallel problems
ğŸ® Production at scale

BREAK-EVEN POINT:
Typically ~5 GB dataset for simple operations
Smaller for complex math operations (1 GB)
Larger for string operations (never!)
    """)


def main():
    """Run all CPU vs GPU benchmarks."""
    spark = create_spark()
    
    print("ğŸ–¥ï¸  GPU ACCELERATION - WHEN CPU WINS")
    print("=" * 70)
    print("\nThis module demonstrates scenarios where CPU outperforms GPU")
    print("due to transfer overhead, unsuitable operations, or dataset size.")
    
    # Run benchmarks
    benchmark_small_dataset_operations(spark)
    benchmark_string_operations(spark)
    benchmark_complex_branching(spark)
    benchmark_sequential_algorithm(spark)
    benchmark_simple_aggregation(spark)
    
    # Show decision matrix
    show_decision_matrix()
    
    print("\n" + "=" * 70)
    print("âœ… BENCHMARKS COMPLETE")
    print("=" * 70)
    print("\nğŸ“ Key Takeaways:")
    print("   1. GPU is NOT always faster!")
    print("   2. Transfer overhead is real (0.125s per GB)")
    print("   3. Small datasets (< 1 GB): Use CPU")
    print("   4. String operations: Always CPU")
    print("   5. Complex branching: CPU wins")
    print("   6. Sequential algorithms: CPU only option")
    print("   7. Break-even point: ~5 GB for simple ops")
    
    print("\nğŸ“š See Also:")
    print("   â€¢ 02_when_gpu_wins.py - GPU acceleration benefits")
    print("   â€¢ 03_hybrid_cpu_gpu.py - Using both together")
    
    spark.stop()


if __name__ == "__main__":
    main()
