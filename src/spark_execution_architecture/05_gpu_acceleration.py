#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
================================================================================
GPU ACCELERATION IN PYSPARK - Complete Guide
================================================================================

MODULE OVERVIEW:
----------------
This module demonstrates how to leverage GPU acceleration in PySpark for
massive performance improvements on data-intensive and compute-intensive
workloads. Learn how to use RAPIDS, PyTorch, and GPU-accelerated UDFs to
process data 10-100x faster than CPU-only approaches.

GPUs excel at:
â€¢ Matrix operations (ML training, linear algebra)
â€¢ Parallel data transformations (apply same operation to millions of rows)
â€¢ Deep learning inference
â€¢ Image/video processing
â€¢ Time series analysis

PURPOSE:
--------
Spark traditionally runs on CPUs across a cluster. However, modern GPUs can
process data MUCH faster for certain workloads. This module shows:

1. Why use GPUs with Spark?
2. GPU architecture basics (CUDA cores, memory, bandwidth)
3. RAPIDS integration (GPU-accelerated DataFrames)
4. PyTorch integration (DL inference on Spark)
5. GPU-accelerated UDFs (custom CUDA kernels)
6. Configuration and cluster setup
7. Performance comparison (CPU vs GPU)
8. Best practices and limitations

TARGET AUDIENCE:
----------------
â€¢ Data scientists doing ML/DL on large datasets
â€¢ Engineers working with image/video/time series data
â€¢ Teams experiencing slow DataFrame operations
â€¢ Anyone with GPU-enabled clusters (cloud or on-prem)

================================================================================
WHY GPUs FOR BIG DATA?
================================================================================

CPU vs GPU Architecture:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CPU (Designed for Flexibility)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Core 1  â”‚  â”‚ Core 2  â”‚  â”‚ Core 3  â”‚  â”‚ Core 4  â”‚  4-64 coresâ”‚
â”‚  â”‚ Complex â”‚  â”‚ Complex â”‚  â”‚ Complex â”‚  â”‚ Complex â”‚  per CPU   â”‚
â”‚  â”‚ Logic   â”‚  â”‚ Logic   â”‚  â”‚ Logic   â”‚  â”‚ Logic   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚     Large Cache (MB per core)        â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                  â”‚
â”‚  Best for: Complex branching logic, variable workloads         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU (Designed for Throughput)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Core][Core][Core][Core][Core][Core][Core][Core]              â”‚
â”‚  [Core][Core][Core][Core][Core][Core][Core][Core]              â”‚
â”‚  [Core][Core][Core][Core][Core][Core][Core][Core]              â”‚
â”‚  [Core][Core][Core][Core][Core][Core][Core][Core]              â”‚
â”‚  ...  (2000-10,000+ simple cores per GPU!)                      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  High-bandwidth memory (100+ GB/s)   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                  â”‚
â”‚  Best for: Same operation on millions of data elements         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PERFORMANCE EXAMPLE (Matrix Multiplication):
CPU (64 cores):   Process 1M rows/second
GPU (5120 cores): Process 50-100M rows/second  âš¡ 50-100x faster!

SPEEDUP BY OPERATION TYPE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Operation             â”‚ CPU vs GPU Speedup   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Matrix multiply       â”‚ 50-100x              â”‚
â”‚ Element-wise ops      â”‚ 10-50x               â”‚
â”‚ Joins                 â”‚ 2-10x                â”‚
â”‚ String operations     â”‚ 1-3x                 â”‚
â”‚ Complex if/else logic â”‚ 1x (no benefit)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
APPROACH 1: RAPIDS cuDF (GPU-Accelerated DataFrames)
================================================================================

RAPIDS is NVIDIA's GPU-accelerated data science library. It provides
cuDF - a pandas-like DataFrame API that runs entirely on GPU.

ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PySpark DataFrame API (your code)                           â”‚
â”‚         â†“                                                    â”‚
â”‚  Spark â†’ Pandas DataFrame â†’ Transfer to GPU                 â”‚
â”‚                                   â†“                          â”‚
â”‚                            cuDF DataFrame (on GPU)           â”‚
â”‚                                   â†“                          â”‚
â”‚                      GPU processes data (50x faster)         â”‚
â”‚                                   â†“                          â”‚
â”‚                      Transfer result back to Spark           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INSTALLATION:
pip install cudf-cu11  # CUDA 11.x
pip install cuml       # GPU ML algorithms
pip install cugraph    # GPU graph analytics

EXAMPLE USE CASE:
â€¢ Feature engineering on 100M rows
â€¢ Aggregations across billions of records
â€¢ Complex joins on large tables
â€¢ Time series resampling

================================================================================
APPROACH 2: PyTorch Integration (Deep Learning Inference)
================================================================================

Run PyTorch models on Spark partitions using GPUs for massively parallel
inference.

ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Cluster (1 driver + 10 executors)                    â”‚
â”‚                                                              â”‚
â”‚  Driver: Loads PyTorch model once                           â”‚
â”‚         â†“ Broadcast model to executors                      â”‚
â”‚                                                              â”‚
â”‚  Executor 1 (GPU):  â”‚  Executor 2 (GPU):  â”‚  Executor 3...  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚
â”‚  â”‚ PyTorch Modelâ”‚  â”‚  â”‚ PyTorch Modelâ”‚  â”‚  Each executor   â”‚
â”‚  â”‚ on GPU       â”‚  â”‚  â”‚ on GPU       â”‚  â”‚  processes its   â”‚
â”‚  â”‚              â”‚  â”‚  â”‚              â”‚  â”‚  partitions      â”‚
â”‚  â”‚ Process      â”‚  â”‚  â”‚ Process      â”‚  â”‚  in parallel     â”‚
â”‚  â”‚ Partition 0  â”‚  â”‚  â”‚ Partition 1  â”‚  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚
â”‚                                                              â”‚
â”‚  RESULT: 10 GPUs process 10 partitions simultaneously        â”‚
â”‚          = 10x throughput vs single GPU!                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USE CASES:
â€¢ Image classification (process 10M images)
â€¢ Text embeddings (BERT, GPT on millions of documents)
â€¢ Fraud detection (ML inference on transactions)
â€¢ Recommendation scoring

================================================================================
APPROACH 3: GPU-Accelerated UDFs (Custom CUDA Kernels)
================================================================================

Write custom GPU functions using Numba CUDA or CuPy and apply them to
Spark DataFrames.

HOW IT WORKS:
1. Write function using Numba @cuda.jit decorator
2. Function compiled to CUDA kernel
3. Spark sends data batches to GPU
4. GPU executes function on entire batch (parallel)
5. Results returned to Spark

EXAMPLE:
@cuda.jit
def gpu_square(input_array, output_array):
    idx = cuda.grid(1)
    if idx < input_array.size:
        output_array[idx] = input_array[idx] ** 2

This runs on 1000s of CUDA cores simultaneously!

================================================================================
CONFIGURATION FOR GPU CLUSTERS:
================================================================================

SPARK CONFIGURATION:

--conf spark.executor.resource.gpu.amount=1
  â””â”€> Each executor gets 1 GPU

--conf spark.task.resource.gpu.amount=0.25
  â””â”€> Each task uses 25% of GPU (4 tasks per GPU concurrently)

--conf spark.rapids.sql.enabled=true
  â””â”€> Enable RAPIDS SQL plugin for GPU execution

--conf spark.rapids.memory.gpu.allocFraction=0.9
  â””â”€> GPU can use up to 90% of its memory

EXAMPLE SPARK-SUBMIT:
spark-submit \\
  --master yarn \\
  --deploy-mode cluster \\
  --num-executors 10 \\
  --executor-cores 8 \\
  --executor-memory 32g \\
  --conf spark.executor.resource.gpu.amount=1 \\
  --conf spark.task.resource.gpu.amount=0.25 \\
  --conf spark.rapids.sql.enabled=true \\
  gpu_app.py

CLUSTER SETUP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU-Enabled Cluster                                       â”‚
â”‚                                                            â”‚
â”‚  Node 1 (Driver):    CPU only (no GPU needed)             â”‚
â”‚                                                            â”‚
â”‚  Nodes 2-11 (Executors): Each has 1-4 GPUs                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚ Executor 1   â”‚  NVIDIA A100 GPU (40GB)                 â”‚
â”‚  â”‚ 8 CPU cores  â”‚  â€¢ 6912 CUDA cores                      â”‚
â”‚  â”‚ 32GB RAM     â”‚  â€¢ 1555 GB/s memory bandwidth           â”‚
â”‚  â”‚ 1 GPU        â”‚  â€¢ 19.5 TFLOPS FP64 performance         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                            â”‚
â”‚  Ã— 10 executors = 10 GPUs = Massive parallel compute!     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CLOUD PROVIDERS:
â€¢ AWS: p3.2xlarge (1Ã— V100), p4d.24xlarge (8Ã— A100)
â€¢ Google Cloud: n1-standard-8 + 1-4Ã— T4/A100/V100
â€¢ Azure: NC6s_v3 (1Ã— V100), NC24ads_A100_v4 (1Ã— A100)
â€¢ Databricks: Use GPU-enabled clusters (select GPU instance types)

================================================================================
PERFORMANCE COMPARISON:
================================================================================

EXAMPLE WORKLOAD: Feature engineering on 100M rows with 50 transformations

CPU CLUSTER (No GPUs):
â€¢ 10 executors Ã— 8 cores = 80 concurrent tasks
â€¢ Processing time: 45 minutes
â€¢ Cost: $20 (10 nodes Ã— 45 min)

GPU CLUSTER (10 GPUs):
â€¢ 10 executors Ã— 1 GPU each = 10 GPUs
â€¢ Processing time: 3 minutes âš¡ 15x faster!
â€¢ Cost: $8 (10 nodes Ã— 3 min, despite higher $/hour for GPUs)

RESULT: 15x faster + 60% cost reduction! ğŸ’°

WHEN TO USE GPU:
âœ… Matrix operations (linear algebra, ML training)
âœ… Element-wise operations (math on every row)
âœ… Deep learning inference
âœ… Image/video/signal processing
âœ… Large-scale aggregations

âŒ Small datasets (< 1M rows)
âŒ Complex branching logic (if/else chains)
âŒ String parsing with complex regex
âŒ Operations already fast on CPU

================================================================================
LIMITATIONS & CONSIDERATIONS:
================================================================================

LIMITATIONS:
1. GPU Memory Constraints
   â€¢ GPUs have limited memory (16-80GB vs 100s of GB RAM)
   â€¢ Must batch data carefully
   â€¢ Use GPU only for compute-heavy parts

2. Data Transfer Overhead
   â€¢ Copying data CPU â†” GPU has latency
   â€¢ Only worth it for expensive operations
   â€¢ Keep data on GPU across operations when possible

3. Not All Operations Benefit
   â€¢ String regex, JSON parsing: minimal speedup
   â€¢ Simple filters: CPU is already fast
   â€¢ Complex conditional logic: doesn't parallelize well

4. Infrastructure Costs
   â€¢ GPU instances are 2-5x more expensive per hour
   â€¢ But complete jobs 10-50x faster â†’ net savings
   â€¢ Best for production workloads, not exploratory analysis

BEST PRACTICES:
â€¢ Profile first: Identify bottlenecks (use Spark UI)
â€¢ Batch operations: Do multiple ops on GPU before transferring back
â€¢ Use GPU for compute-intensive: ML training, inference, matrix ops
â€¢ Monitor GPU utilization: Ensure GPUs aren't idle (target 80%+)
â€¢ Handle failures: GPUs can fail, implement retry logic

================================================================================
USAGE:
================================================================================

This script demonstrates GPU acceleration patterns:

    python 05_gpu_acceleration.py

Examples included:
1. RAPIDS cuDF integration (GPU DataFrames)
2. PyTorch inference on Spark (batch predictions)
3. GPU-accelerated UDFs (custom CUDA kernels)
4. Performance benchmarking (CPU vs GPU)
5. Configuration examples

NOTE: This script demonstrates the concepts. To actually run GPU code,
you need:
â€¢ GPU-enabled hardware (NVIDIA GPU with CUDA support)
â€¢ CUDA Toolkit installed
â€¢ RAPIDS / PyTorch with CUDA support
â€¢ Cluster configuration for GPU executors

================================================================================
RELATED RESOURCES:
================================================================================

RAPIDS:
  https://rapids.ai/
  https://docs.rapids.ai/api/cudf/stable/

Spark + RAPIDS:
  https://nvidia.github.io/spark-rapids/

PyTorch + Spark:
  https://pytorch.org/docs/stable/distributed.html

Numba CUDA:
  https://numba.readthedocs.io/en/stable/cuda/index.html

AWS GPU Instances:
  https://aws.amazon.com/ec2/instance-types/p4/

Databricks GPU Clusters:
  https://docs.databricks.com/clusters/gpu.html

Related files in this project:
  â€¢ ../cluster_computing/06_gpu_accelerated_udfs.py - Detailed UDF examples
  â€¢ ../pyspark_pytorch/ - PyTorch integration examples
  â€¢ 01_dag_visualization.py - Understanding execution plans

AUTHOR: PySpark Education Project
LICENSE: Educational Use - MIT License
VERSION: 1.0.0 - Comprehensive GPU Acceleration Guide
CREATED: 2024
================================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, pandas_udf, PandasUDFType
from pyspark.sql.types import DoubleType, ArrayType
import pandas as pd
import numpy as np
import time

# ================================================================================
# GPU AVAILABILITY CHECK
# ================================================================================

def check_gpu_availability():
    """
    Check if GPU is available on this system.
    
    This function detects:
    â€¢ CUDA availability
    â€¢ Number of GPUs
    â€¢ GPU model and memory
    â€¢ cuDF installation status
    """
    print("=" * 80)
    print("GPU AVAILABILITY CHECK")
    print("=" * 80)
    
    # ========================================================================
    # Check CUDA (NVIDIA's parallel computing platform)
    # ========================================================================
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        
        if cuda_available:
            print(f"âœ… CUDA is available")
            print(f"   CUDA version: {torch.version.cuda}")
            print(f"   Number of GPUs: {torch.cuda.device_count()}")
            
            # Get GPU details for each device
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âŒ CUDA not available - running on CPU only")
            print("   To enable GPU:")
            print("   1. Install NVIDIA GPU drivers")
            print("   2. Install CUDA Toolkit")
            print("   3. Install PyTorch with CUDA: pip install torch --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        print("âš ï¸  PyTorch not installed - cannot check CUDA")
        print("   Install: pip install torch")
    
    # ========================================================================
    # Check RAPIDS cuDF (GPU DataFrame library)
    # ========================================================================
    try:
        import cudf
        print(f"\nâœ… RAPIDS cuDF is installed (version {cudf.__version__})")
        print("   GPU-accelerated DataFrames available!")
    except ImportError:
        print("\nâŒ RAPIDS cuDF not installed")
        print("   Install: pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com")
    
    # ========================================================================
    # Check Numba (for custom CUDA kernels)
    # ========================================================================
    try:
        from numba import cuda
        print(f"\nâœ… Numba CUDA is available")
        print("   Custom GPU kernels can be written!")
    except ImportError:
        print("\nâŒ Numba not installed")
        print("   Install: pip install numba")
    
    print("\n" + "=" * 80)


# ================================================================================
# APPROACH 1: CPU-ONLY BASELINE (FOR COMPARISON)
# ================================================================================

def cpu_baseline_example():
    """
    Demonstrate CPU-only processing as baseline for performance comparison.
    
    This establishes a baseline to measure GPU speedup against.
    """
    print("\n" + "=" * 80)
    print("APPROACH 1: CPU-ONLY BASELINE")
    print("=" * 80)
    
    # ========================================================================
    # Create Spark session (CPU-only configuration)
    # ========================================================================
    spark = SparkSession.builder \
        .appName("CPU_Baseline") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    
    print("\nğŸ”¹ Creating large dataset (10M rows)...")
    
    # Create sample data: sensor readings with timestamp
    # This simulates IoT or time series data
    df = spark.range(0, 10_000_000).toDF("id") \
        .withColumn("sensor_value", (col("id") % 100) / 10.0) \
        .withColumn("timestamp", col("id") * 1000)
    
    print(f"   Dataset: {df.count():,} rows")
    
    # ========================================================================
    # CPU Processing: Feature Engineering
    # ========================================================================
    print("\nğŸ”¹ CPU Processing: Feature engineering...")
    
    start_time = time.time()
    
    # Apply multiple transformations (typical feature engineering)
    result = df \
        .withColumn("squared", col("sensor_value") ** 2) \
        .withColumn("cubed", col("sensor_value") ** 3) \
        .withColumn("sqrt", col("sensor_value") ** 0.5) \
        .withColumn("log", col("sensor_value") + 1)  # log(x+1) to avoid log(0)
    
    # Trigger computation with action
    row_count = result.count()
    
    cpu_time = time.time() - start_time
    
    print(f"   âœ… Processed {row_count:,} rows")
    print(f"   â±ï¸  CPU Time: {cpu_time:.2f} seconds")
    print(f"   ğŸ“Š Throughput: {row_count / cpu_time:,.0f} rows/sec")
    
    print("\nğŸ’¡ This is our baseline. Let's see how GPU compares!")
    
    spark.stop()
    return cpu_time


# ================================================================================
# APPROACH 2: RAPIDS cuDF (GPU DataFrames)
# ================================================================================

def rapids_cudf_example():
    """
    Demonstrate RAPIDS cuDF for GPU-accelerated DataFrame operations.
    
    cuDF provides a pandas-like API that runs entirely on GPU, offering
    10-50x speedup for many operations.
    
    NOTE: Requires RAPIDS cuDF installed and NVIDIA GPU with CUDA support.
    """
    print("\n" + "=" * 80)
    print("APPROACH 2: RAPIDS cuDF (GPU DataFrames)")
    print("=" * 80)
    
    try:
        import cudf
        
        print("\nğŸ”¹ Processing with GPU-accelerated cuDF...")
        
        start_time = time.time()
        
        # ====================================================================
        # Create cuDF DataFrame ON GPU
        # ====================================================================
        # Data is created directly in GPU memory
        gpu_df = cudf.DataFrame({
            'id': range(10_000_000),
            'sensor_value': [(i % 100) / 10.0 for i in range(10_000_000)]
        })
        
        # ====================================================================
        # GPU Operations (all run on GPU - no CPU transfer!)
        # ====================================================================
        # These operations run on 1000s of CUDA cores in parallel
        gpu_df['squared'] = gpu_df['sensor_value'] ** 2
        gpu_df['cubed'] = gpu_df['sensor_value'] ** 3
        gpu_df['sqrt'] = gpu_df['sensor_value'] ** 0.5
        gpu_df['log'] = gpu_df['sensor_value'] + 1
        
        # Force computation (cuDF is also lazy)
        row_count = len(gpu_df)
        
        gpu_time = time.time() - start_time
        
        print(f"   âœ… Processed {row_count:,} rows on GPU")
        print(f"   â±ï¸  GPU Time: {gpu_time:.2f} seconds")
        print(f"   ğŸ“Š Throughput: {row_count / gpu_time:,.0f} rows/sec")
        
        print("\nğŸš€ cuDF Operation Speedups (typical):")
        print("   â€¢ Element-wise operations: 10-50x faster")
        print("   â€¢ GroupBy aggregations: 5-20x faster")
        print("   â€¢ Joins: 2-10x faster")
        print("   â€¢ Sorts: 5-15x faster")
        
        print("\nğŸ’¡ Integration with Spark:")
        print("   Use pandas_udf to convert Spark â†’ Pandas â†’ cuDF â†’ GPU")
        print("   Process each partition on GPU, return to Spark")
        
    except ImportError:
        print("\nâš ï¸  RAPIDS cuDF not installed")
        print("   Install: conda install -c rapidsai -c conda-forge cudf")
        print("   Or: pip install cudf-cu11")


# ================================================================================
# APPROACH 3: PyTorch Inference on Spark
# ================================================================================

def pytorch_inference_example():
    """
    Demonstrate PyTorch model inference using GPUs across Spark partitions.
    
    This pattern is common for:
    â€¢ Image classification on millions of images
    â€¢ Text embeddings (BERT, GPT) on documents
    â€¢ Any deep learning inference at scale
    
    Each Spark partition is processed on a GPU, enabling massive parallelism.
    """
    print("\n" + "=" * 80)
    print("APPROACH 3: PyTorch GPU Inference on Spark")
    print("=" * 80)
    
    spark = SparkSession.builder \
        .appName("PyTorch_GPU_Inference") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    
    print("\nğŸ”¹ Pattern: Broadcast model â†’ Partition inference â†’ Parallel GPUs")
    
    # ========================================================================
    # Step 1: Create sample data (image embeddings)
    # ========================================================================
    print("\n1ï¸âƒ£  Creating sample data (simulating image embeddings)...")
    
    # In real scenario: images loaded from S3/HDFS
    # Each row represents an image as a 512-dim embedding vector
    df = spark.range(0, 100000).toDF("image_id") \
        .withColumn("pixel_data", col("image_id") % 256)  # Simplified
    
    print(f"   Dataset: {df.count():,} images")
    
    # ========================================================================
    # Step 2: Define pandas_udf that runs PyTorch on GPU
    # ========================================================================
    print("\n2ï¸âƒ£  Defining GPU inference UDF...")
    
    @pandas_udf(DoubleType())
    def predict_on_gpu(pixel_data: pd.Series) -> pd.Series:
        """
        Run PyTorch model inference on GPU.
        
        This function is called once per Spark partition.
        Each partition is processed on a GPU in parallel!
        
        HOW IT WORKS:
        1. Function receives a batch of rows (1 partition)
        2. Loads PyTorch model onto GPU
        3. Converts data to GPU tensor
        4. Runs model inference on GPU
        5. Returns predictions to Spark
        
        In production:
        â€¢ Broadcast model to avoid loading per partition
        â€¢ Use larger batch sizes (1000s-10000s per partition)
        â€¢ Handle GPU OOM errors gracefully
        """
        try:
            import torch
            
            # Check if GPU is available
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # In production: Load pre-trained model
            # model = torch.load('model.pth').to(device)
            # For demo: Simple computation simulating inference
            
            # Convert pandas Series to PyTorch tensor ON GPU
            data_tensor = torch.tensor(pixel_data.values, dtype=torch.float32).to(device)
            
            # Simulate model inference (actual model.forward() in production)
            # Example: prediction = model(data_tensor)
            predictions = torch.sigmoid(data_tensor / 256.0)  # Dummy prediction
            
            # Move results back to CPU and return as pandas Series
            return pd.Series(predictions.cpu().numpy())
            
        except Exception as e:
            # Fallback to CPU if GPU unavailable
            return pd.Series([0.5] * len(pixel_data))
    
    # ========================================================================
    # Step 3: Apply UDF to DataFrame (triggers GPU inference)
    # ========================================================================
    print("\n3ï¸âƒ£  Running inference on Spark partitions...")
    
    start_time = time.time()
    
    # Apply GPU inference UDF
    # Spark will call predict_on_gpu() once per partition
    # If you have 10 partitions and 10 GPUs â†’ 10 parallel inferences!
    result = df.withColumn("prediction", predict_on_gpu(col("pixel_data")))
    
    # Trigger computation
    predictions_count = result.count()
    
    inference_time = time.time() - start_time
    
    print(f"   âœ… Generated {predictions_count:,} predictions")
    print(f"   â±ï¸  Time: {inference_time:.2f} seconds")
    print(f"   ğŸ“Š Throughput: {predictions_count / inference_time:,.0f} predictions/sec")
    
    # Show sample results
    print("\nğŸ“Š Sample predictions:")
    result.show(5, truncate=False)
    
    print("\nğŸ’¡ In Production with 10 GPUs:")
    print("   â€¢ 10 executors Ã— 1 GPU each = 10 GPUs")
    print("   â€¢ Each GPU processes its partitions in parallel")
    print("   â€¢ 10x throughput vs single GPU!")
    print("   â€¢ Can process millions of images per minute")
    
    spark.stop()


# ================================================================================
# APPROACH 4: GPU-Accelerated UDFs (Custom CUDA Kernels)
# ================================================================================

def gpu_accelerated_udf_example():
    """
    Demonstrate custom GPU kernels using Numba CUDA.
    
    When built-in operations aren't enough, write custom CUDA kernels
    that run directly on GPU hardware for maximum performance.
    
    USE CASES:
    â€¢ Custom mathematical operations
    â€¢ Specialized algorithms
    â€¢ When cuDF/PyTorch don't provide what you need
    """
    print("\n" + "=" * 80)
    print("APPROACH 4: GPU-Accelerated UDFs (Custom CUDA Kernels)")
    print("=" * 80)
    
    try:
        from numba import cuda
        import math
        
        print("\nğŸ”¹ Writing custom CUDA kernel for element-wise operations...")
        
        # ====================================================================
        # Define CUDA kernel (runs on GPU)
        # ====================================================================
        @cuda.jit
        def complex_computation_kernel(input_array, output_array):
            """
            Custom CUDA kernel that runs on GPU.
            
            CUDA EXECUTION MODEL:
            â€¢ Launched with 1000s of threads
            â€¢ Each thread processes one element
            â€¢ All threads execute in parallel on CUDA cores
            
            cuda.grid(1): Get unique thread ID (which element to process)
            """
            # Get the thread's unique ID (which array element to process)
            idx = cuda.grid(1)
            
            # Bounds check (don't access beyond array)
            if idx < input_array.size:
                # Custom computation (runs on GPU)
                x = input_array[idx]
                output_array[idx] = (x ** 2 + math.sqrt(abs(x)) + math.sin(x)) / 10.0
        
        print("   âœ… CUDA kernel defined")
        
        # ====================================================================
        # Test kernel with sample data
        # ====================================================================
        print("\nğŸ”¹ Testing CUDA kernel on GPU...")
        
        # Create sample data
        n = 1_000_000
        input_data = np.random.randn(n).astype(np.float32)
        output_data = np.zeros(n, dtype=np.float32)
        
        # Copy data to GPU
        d_input = cuda.to_device(input_data)
        d_output = cuda.to_device(output_data)
        
        # Configure kernel launch
        # threads_per_block: How many threads per block (typical: 256-1024)
        # blocks_per_grid: How many blocks needed to cover all data
        threads_per_block = 256
        blocks_per_grid = (n + threads_per_block - 1) // threads_per_block
        
        print(f"   Launching kernel:")
        print(f"   â€¢ {blocks_per_grid:,} blocks")
        print(f"   â€¢ {threads_per_block} threads per block")
        print(f"   â€¢ Total: {blocks_per_grid * threads_per_block:,} threads")
        
        # Launch kernel
        start_time = time.time()
        complex_computation_kernel[blocks_per_grid, threads_per_block](d_input, d_output)
        cuda.synchronize()  # Wait for kernel to finish
        kernel_time = time.time() - start_time
        
        # Copy result back to CPU
        result = d_output.copy_to_host()
        
        print(f"\n   âœ… Processed {n:,} elements")
        print(f"   â±ï¸  GPU Time: {kernel_time:.4f} seconds")
        print(f"   ğŸ“Š Throughput: {n / kernel_time:,.0f} elements/sec")
        
        print("\nğŸ’¡ Using in Spark:")
        print("   â€¢ Write CUDA kernel")
        print("   â€¢ Wrap in pandas_udf")
        print("   â€¢ Apply to Spark DataFrame")
        print("   â€¢ Each partition processed on GPU")
        
    except ImportError:
        print("\nâš ï¸  Numba not installed")
        print("   Install: pip install numba")
    except Exception as e:
        print(f"\nâš ï¸  CUDA not available: {e}")
        print("   Requires NVIDIA GPU with CUDA support")


# ================================================================================
# PERFORMANCE BENCHMARKING
# ================================================================================

def benchmark_cpu_vs_gpu():
    """
    Compare CPU vs GPU performance on identical workload.
    
    This gives a realistic sense of potential speedups.
    """
    print("\n" + "=" * 80)
    print("PERFORMANCE BENCHMARK: CPU vs GPU")
    print("=" * 80)
    
    print("\nğŸ¯ Workload: Feature engineering on 10M rows")
    print("   Operations: square, cube, sqrt, log")
    
    # Run CPU baseline
    print("\n" + "-" * 80)
    print("Running CPU baseline...")
    print("-" * 80)
    cpu_time = cpu_baseline_example()
    
    print("\n" + "-" * 80)
    print("Benchmark Summary")
    print("-" * 80)
    print(f"CPU Time: {cpu_time:.2f} seconds")
    print(f"\nPotential GPU Speedup (typical): 10-50x")
    print(f"Estimated GPU Time: {cpu_time / 20:.2f} seconds (assuming 20x speedup)")
    
    print("\nğŸ’¡ Actual speedup depends on:")
    print("   â€¢ GPU model (A100 > V100 > T4)")
    print("   â€¢ Operation type (matrix ops faster than string ops)")
    print("   â€¢ Data size (larger = better GPU utilization)")
    print("   â€¢ Memory bandwidth (PCIe bottleneck?)")


# ================================================================================
# MAIN EXECUTION
# ================================================================================

def main():
    """
    Run all GPU acceleration examples.
    """
    print("\n" + "ğŸš€" * 40)
    print("GPU ACCELERATION IN PYSPARK - COMPLETE GUIDE")
    print("ğŸš€" * 40)
    
    # Check GPU availability first
    check_gpu_availability()
    
    # Run examples
    cpu_baseline_example()      # Baseline for comparison
    rapids_cudf_example()        # GPU DataFrames
    pytorch_inference_example()  # DL inference
    gpu_accelerated_udf_example()  # Custom kernels
    benchmark_cpu_vs_gpu()       # Performance comparison
    
    print("\n" + "=" * 80)
    print("âœ… GPU ACCELERATION GUIDE COMPLETE")
    print("=" * 80)
    
    print("\nğŸ“š Key Takeaways:")
    print("   1. GPUs offer 10-100x speedup for compute-intensive tasks")
    print("   2. Use RAPIDS for DataFrame operations (cuDF)")
    print("   3. Use PyTorch for deep learning inference")
    print("   4. Write custom CUDA kernels for specialized operations")
    print("   5. GPU clusters enable massive parallel processing")
    print("   6. Profile first - not all operations benefit from GPU")
    
    print("\nğŸ’° Cost-Benefit:")
    print("   â€¢ GPU instances: 2-5x more expensive per hour")
    print("   â€¢ GPU processing: 10-50x faster")
    print("   â€¢ Net result: 50-80% cost reduction for large workloads!")


if __name__ == "__main__":
    main()
