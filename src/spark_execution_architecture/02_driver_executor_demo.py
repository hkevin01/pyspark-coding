"""
02_driver_executor_demo.py
===========================

Driver and Executor Responsibilities Demo

Demonstrates:
- Driver responsibilities (planning, scheduling, coordination)
- Executor responsibilities (task execution, data storage)
- Communication patterns
- Resource allocation
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, sum as _sum
from pyspark.sql.types import StringType
import time


def create_spark():
    """Create Spark session with explicit configuration."""
    return SparkSession.builder \
        .appName("DriverExecutorDemo") \
        .master("local[4]") \
        .config("spark.executor.memory", "1g") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.cores", "2") \
        .config("spark.sql.shuffle.partitions", "8") \
        .getOrCreate()


def demonstrate_driver_responsibilities():
    """
    Show what the driver does.
    """
    print("\n" + "=" * 80)
    print("DRIVER RESPONSIBILITIES")
    print("=" * 80)
    
    spark = create_spark()
    
    print("\nğŸ¯ DRIVER TASKS:")
    print("=" * 80)
    
    # 1. Maintains SparkSession
    print("\n1ï¸âƒ£  Maintains SparkSession")
    print(f"   SparkSession created: {spark}")
    print(f"   App Name: {spark.sparkContext.appName}")
    print(f"   Master: {spark.sparkContext.master}")
    
    # 2. Builds execution plans
    print("\n2ï¸âƒ£  Builds Execution Plans (Logical â†’ Physical)")
    df = spark.range(0, 1000).toDF("id")
    result = df.filter(col("id") > 500).select("id")
    
    print("   Logical Plan built by driver:")
    result.explain(extended=True)
    
    # 3. Schedules jobs/stages/tasks
    print("\n3ï¸âƒ£  Schedules Jobs, Stages, and Tasks")
    print("   Driver breaks query into:")
    print("   â€¢ Jobs (one per action)")
    print("   â€¢ Stages (split at shuffles)")
    print("   â€¢ Tasks (one per partition)")
    
    # 4. Tracks executor status
    print("\n4ï¸âƒ£  Tracks Executor Status")
    print(f"   Active executors: {spark.sparkContext.defaultParallelism}")
    print(f"   Default parallelism: {spark.sparkContext.defaultParallelism}")
    
    # 5. Collects results
    print("\n5ï¸âƒ£  Collects Results from Executors")
    count = result.count()
    print(f"   Driver received: {count} rows from executors")
    
    # 6. Manages broadcasts
    print("\n6ï¸âƒ£  Manages Broadcast Variables")
    broadcast_var = spark.sparkContext.broadcast({"key": "value"})
    print(f"   Broadcast variable created: {broadcast_var}")
    print("   Driver sends broadcast to all executors")
    
    print("\nğŸ“Š DRIVER MEMORY USAGE:")
    print("   â€¢ SparkSession metadata")
    print("   â€¢ Execution plans (DAGs)")
    print("   â€¢ Job/stage/task tracking")
    print("   â€¢ Broadcast variables")
    print("   â€¢ Small results from collect()")


def demonstrate_executor_responsibilities():
    """
    Show what executors do.
    """
    print("\n" + "=" * 80)
    print("EXECUTOR RESPONSIBILITIES")
    print("=" * 80)
    
    spark = create_spark()
    
    print("\nğŸ”§ EXECUTOR TASKS:")
    print("=" * 80)
    
    # 1. Run tasks
    print("\n1ï¸âƒ£  Execute Tasks on Partitions")
    df = spark.range(0, 10000).repartition(4)
    
    # UDF to show which executor is running
    @udf(StringType())
    def get_partition_info(value):
        import os
        return f"PID:{os.getpid()}"
    
    result = df.withColumn("executor_pid", get_partition_info(col("id")))
    
    print("   Tasks distributed across executors:")
    result.select("executor_pid").distinct().show()
    
    # 2. Store data partitions
    print("\n2ï¸âƒ£  Store Data Partitions in Memory")
    cached_df = df.cache()
    cached_df.count()  # Materialize cache
    
    print("   Data cached on executors")
    print("   Check Storage tab in Spark UI")
    
    # 3. Execute computations
    print("\n3ï¸âƒ£  Execute Transformations")
    
    start = time.time()
    computed = df \
        .filter(col("id") > 1000) \
        .withColumn("squared", col("id") * col("id")) \
        .filter(col("squared") < 50000000)
    
    result_count = computed.count()
    exec_time = time.time() - start
    
    print(f"   Executors processed {result_count} rows")
    print(f"   Execution time: {exec_time:.4f}s")
    print("   All computation done on executors, not driver")
    
    # 4. Shuffle read/write
    print("\n4ï¸âƒ£  Handle Shuffle Operations")
    grouped = df.groupBy((col("id") % 10).alias("bucket")).count()
    grouped.show()
    
    print("   Executors wrote shuffle files")
    print("   Executors read shuffle files")
    print("   Check Stages tab for shuffle metrics")
    
    # 5. Report metrics
    print("\n5ï¸âƒ£  Report Metrics to Driver")
    print("   â€¢ Task completion status")
    print("   â€¢ Shuffle bytes written/read")
    print("   â€¢ Records processed")
    print("   â€¢ Execution time")
    print("   â€¢ Memory usage")
    
    print("\nğŸ“Š EXECUTOR MEMORY USAGE:")
    print("   â€¢ Cached data partitions")
    print("   â€¢ Task execution memory")
    print("   â€¢ Shuffle buffers")
    print("   â€¢ Broadcast variable copies")
    
    cached_df.unpersist()


def demonstrate_driver_executor_communication():
    """
    Show communication patterns between driver and executors.
    """
    print("\n" + "=" * 80)
    print("DRIVER â†” EXECUTOR COMMUNICATION")
    print("=" * 80)
    
    spark = create_spark()
    
    print("\nğŸ“¡ COMMUNICATION FLOW:")
    print("=" * 80)
    
    df = spark.range(0, 10000).repartition(4)
    
    print("\n1ï¸âƒ£  JOB SUBMISSION:")
    print("   Driver â†’ Executors: 'Execute these tasks'")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Driver  â”‚ ------> â”‚ Executor â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("              Task 0-3 (by partition)")
    
    print("\n2ï¸âƒ£  TASK EXECUTION:")
    result = df.filter(col("id") > 5000).count()
    
    print("   Executors: Running tasks in parallel")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Executor â”‚ Task 0: Process partition 0")
    print("   â”‚ Executor â”‚ Task 1: Process partition 1")
    print("   â”‚ Executor â”‚ Task 2: Process partition 2")
    print("   â”‚ Executor â”‚ Task 3: Process partition 3")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\n3ï¸âƒ£  RESULT COLLECTION:")
    print("   Executors â†’ Driver: 'Task complete, here are results'")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Executor â”‚ ------> â”‚ Driver  â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("           Task results: counts from each partition")
    
    print(f"\n   Driver aggregates: {result} rows total")
    
    print("\n4ï¸âƒ£  SHUFFLE COMMUNICATION:")
    grouped = df.groupBy((col("id") % 3).alias("key")).count()
    grouped.show()
    
    print("   Executors â†” Executors: Shuffle data exchange")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Exec 1   â”‚ <-----> â”‚ Exec 2   â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("        â†•                    â†•")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚ Exec 3   â”‚ <-----> â”‚ Exec 4   â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("   (via shuffle service or executor)")


def demonstrate_failure_handling():
    """
    Show how driver handles executor failures.
    """
    print("\n" + "=" * 80)
    print("FAILURE HANDLING")
    print("=" * 80)
    
    spark = create_spark()
    
    print("\nğŸ”„ FAULT TOLERANCE:")
    print("=" * 80)
    
    print("\n1ï¸âƒ£  Task Failure:")
    print("   â€¢ Executor fails during task")
    print("   â€¢ Driver detects failure")
    print("   â€¢ Driver reschedules task on different executor")
    print("   â€¢ Uses lineage (DAG) to recompute lost data")
    
    print("\n2ï¸âƒ£  Executor Failure:")
    print("   â€¢ Executor process crashes")
    print("   â€¢ Driver marks executor as lost")
    print("   â€¢ Driver requests new executor from cluster manager")
    print("   â€¢ Reschedules all tasks from failed executor")
    
    print("\n3ï¸âƒ£  Driver Failure:")
    print("   â€¢ âš ï¸  Driver crash = entire application fails")
    print("   â€¢ No automatic recovery (single point of failure)")
    print("   â€¢ Solution: Use cluster mode + checkpointing")
    
    print("\n4ï¸âƒ£  Shuffle Data Persistence:")
    df = spark.range(0, 10000).repartition(4)
    grouped = df.groupBy((col("id") % 5).alias("key")).count()
    grouped.show()
    
    print("   â€¢ Shuffle files written to disk")
    print("   â€¢ Survive executor failures")
    print("   â€¢ External shuffle service keeps data")


def demonstrate_resource_allocation():
    """
    Show resource allocation between driver and executors.
    """
    print("\n" + "=" * 80)
    print("RESOURCE ALLOCATION")
    print("=" * 80)
    
    spark = create_spark()
    
    print("\nğŸ’¾ TYPICAL CLUSTER CONFIGURATION:")
    print("=" * 80)
    
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    CLUSTER RESOURCES                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
    â”‚  â”‚   DRIVER     â”‚  (1 node)                             â”‚
    â”‚  â”‚              â”‚                                        â”‚
    â”‚  â”‚  Memory: 2GB â”‚  â€¢ Plans execution                    â”‚
    â”‚  â”‚  Cores: 2    â”‚  â€¢ Schedules tasks                    â”‚
    â”‚  â”‚              â”‚  â€¢ Collects results                   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
    â”‚         â†“                                                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚            EXECUTORS (Worker Nodes)              â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
    â”‚  â”‚  â”‚ Executor 1 â”‚  â”‚ Executor 2 â”‚  â”‚ Executor 3 â”‚ â”‚  â”‚
    â”‚  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚ â”‚  â”‚
    â”‚  â”‚  â”‚ Memory: 8GBâ”‚  â”‚ Memory: 8GBâ”‚  â”‚ Memory: 8GBâ”‚ â”‚  â”‚
    â”‚  â”‚  â”‚ Cores: 4   â”‚  â”‚ Cores: 4   â”‚  â”‚ Cores: 4   â”‚ â”‚  â”‚
    â”‚  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚ â”‚  â”‚
    â”‚  â”‚  â”‚ â€¢ Run tasksâ”‚  â”‚ â€¢ Run tasksâ”‚  â”‚ â€¢ Run tasksâ”‚ â”‚  â”‚
    â”‚  â”‚  â”‚ â€¢ Store dataâ”‚ â”‚ â€¢ Store dataâ”‚ â”‚ â€¢ Store dataâ”‚ â”‚  â”‚
    â”‚  â”‚  â”‚ â€¢ Shuffle  â”‚  â”‚ â€¢ Shuffle  â”‚  â”‚ â€¢ Shuffle  â”‚ â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Total Cluster:
    â€¢ Driver: 1 node Ã— 2GB = 2GB
    â€¢ Executors: 3 nodes Ã— 8GB = 24GB
    â€¢ Total: 26GB cluster memory
    â€¢ Parallelism: 3 executors Ã— 4 cores = 12 concurrent tasks
    """)
    
    print("\nâš™ï¸  CONFIGURATION PARAMETERS:")
    print("   --driver-memory 2g")
    print("   --executor-memory 8g")
    print("   --executor-cores 4")
    print("   --num-executors 3")


def main():
    """
    Main execution function.
    """
    print("\n" + "ğŸ¯" * 40)
    print("DRIVER AND EXECUTOR RESPONSIBILITIES")
    print("ğŸ¯" * 40)
    
    demonstrate_driver_responsibilities()
    demonstrate_executor_responsibilities()
    demonstrate_driver_executor_communication()
    demonstrate_failure_handling()
    demonstrate_resource_allocation()
    
    print("\n" + "=" * 80)
    print("âœ… DRIVER/EXECUTOR DEMO COMPLETE")
    print("=" * 80)
    
    print("\nğŸ“š Summary:")
    print("   DRIVER:")
    print("   âœ… Maintains SparkSession")
    print("   âœ… Builds execution plans")
    print("   âœ… Schedules jobs/stages/tasks")
    print("   âœ… Collects results")
    print("   âœ… Manages broadcasts")
    
    print("\n   EXECUTORS:")
    print("   âœ… Execute tasks on partitions")
    print("   âœ… Store cached data")
    print("   âœ… Perform computations")
    print("   âœ… Handle shuffles")
    print("   âœ… Report metrics")
    
    spark = SparkSession.builder.getOrCreate()
    spark.stop()


if __name__ == "__main__":
    main()
