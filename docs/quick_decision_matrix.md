## Quick Decision Matrix

```
Data Size Decision:
â”œâ”€ < 10 GB       â†’ Pandas
â”œâ”€ 10 GB - 100 GB â†’ Pandas (if RAM allows) or PySpark
â”œâ”€ 100 GB - 10 TB â†’ PySpark
â””â”€ > 10 TB       â†’ PySpark definitely

Processing Type:
â”œâ”€ Batch         â†’ PySpark, Hadoop MapReduce
â”œâ”€ Interactive   â†’ PySpark, Presto/Trino
â”œâ”€ Streaming     â†’ PySpark (micro-batch), Flink (real-time)
â”œâ”€ ML/Analytics  â†’ PySpark
â””â”€ Transactions  â†’ PostgreSQL, MySQL

Latency Requirements:
â”œâ”€ Minutes-Hours â†’ PySpark, Hadoop
â”œâ”€ Seconds       â†’ PySpark Streaming
â”œâ”€ Milliseconds  â†’ Apache Flink, Kafka Streams
â””â”€ Microseconds  â†’ In-memory DB, Redis
```
### ğŸ“Š Enhanced Decision Tree

```
Need to process data?
â”‚
â”œâ”€ Data < 10GB?
â”‚  â”œâ”€ YES â†’ Use Pandas âœ…
â”‚  â”‚  â”œâ”€ Need SQL queries?
â”‚  â”‚  â”‚  â””â”€ YES â†’ Use Pandas + SQLite
â”‚  â”‚  â”œâ”€ Need ML inference?
â”‚  â”‚  â”‚  â””â”€ YES â†’ Use Pandas + PyTorch/sklearn
â”‚  â”‚  â””â”€ Need visualizations?
â”‚  â”‚     â””â”€ YES â†’ Add Matplotlib/Plotly
â”‚  
â””â”€ Data > 10GB?
   â””â”€ YES â†’ Use PySpark âœ…
      â”‚
      â”œâ”€â”€â”€ Processing Mode?
      â”‚    â”‚
      â”‚    â”œâ”€ Batch Processing?
      â”‚    â”‚  â””â”€ YES â†’ PySpark DataFrame API
      â”‚    â”‚     â”œâ”€ Complex transformations? â†’ Use UDFs
      â”‚    â”‚     â”œâ”€ SQL-heavy logic? â†’ Use Spark SQL
      â”‚    â”‚     â””â”€ Need checkpointing? â†’ Enable `.checkpoint()`
      â”‚    â”‚
      â”‚    â””â”€ Stream Processing?
      â”‚       â””â”€ YES â†’ PySpark Structured Streaming âœ…
      â”‚          â”‚
      â”‚          â”œâ”€â”€â”€ Data Source?
      â”‚          â”‚    â”œâ”€ Kafka â†’ Use `.format("kafka")`
      â”‚          â”‚    â”œâ”€ File system â†’ Use `.format("csv/json/parquet")`
      â”‚          â”‚    â”œâ”€ Socket â†’ Use `.format("socket")`
      â”‚          â”‚    â””â”€ Rate source (testing) â†’ Use `.format("rate")`
      â”‚          â”‚
      â”‚          â”œâ”€â”€â”€ Processing Requirements?
      â”‚          â”‚    â”œâ”€ Stateless transformations â†’ Use `.select()`, `.filter()`
      â”‚          â”‚    â”œâ”€ Stateful operations â†’ Use `.groupBy().agg()`
      â”‚          â”‚    â”œâ”€ Windowed aggregations â†’ Use `.window()` or `.groupBy(window())`
      â”‚          â”‚    â”œâ”€ Watermarking needed? â†’ Use `.withWatermark()`
      â”‚          â”‚    â””â”€ Join streams? â†’ Use `.join()` with watermarks
      â”‚          â”‚
      â”‚          â”œâ”€â”€â”€ Output Sink?
      â”‚          â”‚    â”œâ”€ Console (debug) â†’ `.writeStream.format("console")`
      â”‚          â”‚    â”œâ”€ File system â†’ `.format("parquet/csv/json")`
      â”‚          â”‚    â”œâ”€ Kafka â†’ `.format("kafka")`
      â”‚          â”‚    â”œâ”€ Database â†’ `.foreach()` or `.foreachBatch()`
      â”‚          â”‚    â””â”€ Memory (testing) â†’ `.format("memory")`
      â”‚          â”‚
      â”‚          â””â”€â”€â”€ Delivery Semantics?
      â”‚               â”œâ”€ At-least-once â†’ Default (no idempotency)
      â”‚               â”œâ”€ Exactly-once â†’ Enable checkpointing + idempotent writes
      â”‚               â””â”€ At-most-once â†’ No checkpointing (not recommended)
      â”‚
      â”œâ”€â”€â”€ Need ML Predictions?
      â”‚    â””â”€ YES â†’ Add PyTorch/TensorFlow âœ…
      â”‚       â”‚
      â”‚       â”œâ”€â”€â”€ Deployment Strategy?
      â”‚       â”‚    â”‚
      â”‚       â”‚    â”œâ”€ In-Database Inference?
      â”‚       â”‚    â”‚  â””â”€ YES â†’ Use UDFs for Inference âœ…
      â”‚       â”‚    â”‚     â”‚
      â”‚       â”‚    â”‚     â”œâ”€ PySpark â†’ Pandas UDF with model broadcasting
      â”‚       â”‚    â”‚     â”œâ”€ PostgreSQL â†’ PL/Python UDF
      â”‚       â”‚    â”‚     â”œâ”€ BigQuery â†’ Remote Functions + Cloud Functions
      â”‚       â”‚    â”‚     â””â”€ Snowflake â†’ Python UDF with packages
      â”‚       â”‚    â”‚
      â”‚       â”‚    â”œâ”€ Batch Scoring?
      â”‚       â”‚    â”‚  â””â”€ Use `.mapPartitions()` with model loading
      â”‚       â”‚    â”‚
      â”‚       â”‚    â””â”€ Real-time Inference?
      â”‚       â”‚       â””â”€ Use UDF in Streaming query
      â”‚       â”‚
      â”‚       â””â”€â”€â”€ Model Optimization?
      â”‚            â”œâ”€ Broadcasting â†’ `sc.broadcast(model)` (critical!)
      â”‚            â”œâ”€ Batch processing â†’ Process partitions, not rows
      â”‚            â”œâ”€ GPU support â†’ Use `.cuda()` in UDF
      â”‚            â””â”€ Model caching â†’ Load once per executor
      â”‚
      â”œâ”€â”€â”€ Data Format?
      â”‚    â”œâ”€ CSV â†’ `.read.csv()`
      â”‚    â”œâ”€ JSON â†’ `.read.json()`
      â”‚    â”œâ”€ Parquet â†’ `.read.parquet()` (recommended for big data)
      â”‚    â”œâ”€ Avro â†’ `.read.format("avro")`
      â”‚    â”œâ”€ Delta Lake â†’ `.read.format("delta")`
      â”‚    â””â”€ Database â†’ `.read.jdbc()` or UDFs
      â”‚
      â”œâ”€â”€â”€ Performance Optimization?
      â”‚    â”œâ”€ Partitioning â†’ `.repartition()` or `.coalesce()`
      â”‚    â”œâ”€ Caching â†’ `.cache()` or `.persist()`
      â”‚    â”œâ”€ Broadcasting â†’ `broadcast()` for small tables in joins
      â”‚    â”œâ”€ Columnar storage â†’ Use Parquet
      â”‚    â””â”€ Predicate pushdown â†’ Filter early with `.filter()`
      â”‚
      â””â”€â”€â”€ Fault Tolerance?
           â”œâ”€ Checkpointing â†’ `.checkpoint()` for lineage truncation
           â”œâ”€ Write-ahead logs â†’ Enable for streaming
           â”œâ”€ Retry logic â†’ Configure `spark.task.maxFailures`
           â””â”€ Graceful shutdown â†’ Handle SIGTERM signals

```

### ğŸ¯ Streaming-Specific Decision Path

```
Need PySpark Streaming?
â”‚
â”œâ”€â”€â”€ Step 1: Choose Trigger Mode
â”‚    â”œâ”€ ProcessingTime â†’ `.trigger(processingTime='10 seconds')`
â”‚    â”œâ”€ Once â†’ `.trigger(once=True)` (batch-like)
â”‚    â”œâ”€ Continuous â†’ `.trigger(continuous='1 second')` (experimental)
â”‚    â””â”€ AvailableNow â†’ `.trigger(availableNow=True)` (Spark 3.3+)
â”‚
â”œâ”€â”€â”€ Step 2: Configure Checkpointing (Required!)
â”‚    â””â”€ `.option("checkpointLocation", "s3://bucket/checkpoints")`
â”‚       â”œâ”€ HDFS â†’ High reliability
â”‚       â”œâ”€ S3 â†’ Cloud storage
â”‚       â””â”€ Local â†’ Testing only (not for production)
â”‚
â”œâ”€â”€â”€ Step 3: Handle State Management
â”‚    â”œâ”€ Stateless â†’ Direct transformations (map, filter)
â”‚    â”œâ”€ Stateful â†’ Aggregations, joins, windowing
â”‚    â”‚  â”œâ”€ State timeout â†’ `.withWatermark("timestamp", "10 minutes")`
â”‚    â”‚  â””â”€ State store â†’ Automatic, but monitor size
â”‚    â””â”€ Custom state â†’ `.mapGroupsWithState()` or `.flatMapGroupsWithState()`
â”‚
â”œâ”€â”€â”€ Step 4: Implement Watermarking (for Late Data)
â”‚    â””â”€ `.withWatermark("event_time", "10 minutes")`
â”‚       â”œâ”€ Defines max lateness allowed
â”‚       â”œâ”€ Enables state cleanup
â”‚       â””â”€ Required for outer joins
â”‚
â”œâ”€â”€â”€ Step 5: Configure Output Mode
â”‚    â”œâ”€ Append â†’ Only new rows (default for stateless)
â”‚    â”œâ”€ Complete â†’ Entire result table (aggregations only)
â”‚    â””â”€ Update â†’ Only changed rows (stateful operations)
â”‚
â””â”€â”€â”€ Step 6: Monitor & Scale
     â”œâ”€ Monitor lag â†’ Check input vs processing rate
     â”œâ”€ Scale executors â†’ Increase based on throughput
     â”œâ”€ Tune batch size â†’ Balance latency vs throughput
     â””â”€ Track state size â†’ Prevent memory overflow

```

### ğŸš€ UDF Decision Matrix

```
Need to extend PySpark functionality?
â”‚
â”œâ”€â”€â”€ Basic Transformations?
â”‚    â””â”€ NO UDF needed â†’ Use built-in functions (faster)
â”‚
â”œâ”€â”€â”€ Complex Python Logic?
â”‚    â””â”€ YES â†’ Choose UDF Type:
â”‚       â”‚
â”‚       â”œâ”€ Row-by-row processing?
â”‚       â”‚  â””â”€ Standard UDF â†’ `@udf(returnType)`
â”‚       â”‚     âš ï¸ Slow: Serialization overhead
â”‚       â”‚
â”‚       â”œâ”€ Vectorized operations?
â”‚       â”‚  â””â”€ Pandas UDF â†’ `@pandas_udf(returnType)`
â”‚       â”‚     âœ… Fast: Columnar processing with Arrow
â”‚       â”‚     â”‚
â”‚       â”‚     â”œâ”€ SCALAR â†’ Transform columns
â”‚       â”‚     â”œâ”€ GROUPED_MAP â†’ Custom aggregations per group
â”‚       â”‚     â”œâ”€ GROUPED_AGG â†’ Aggregate with Pandas
â”‚       â”‚     â””â”€ SCALAR_ITER â†’ Process batches (ML inference)
â”‚       â”‚
â”‚       â””â”€ ML Model Inference?
â”‚          â””â”€ YES â†’ Use Pandas UDF with Broadcasting âœ…
â”‚             â”‚
â”‚             â”œâ”€ Step 1: Broadcast model
â”‚             â”‚  â””â”€ `broadcast_model = sc.broadcast(model)`
â”‚             â”‚
â”‚             â”œâ”€ Step 2: Create Pandas UDF
â”‚             â”‚  â””â”€ `@pandas_udf(FloatType())`
â”‚             â”‚     def predict(features: pd.Series) -> pd.Series:
â”‚             â”‚         model = broadcast_model.value
â”‚             â”‚         return model.predict(features)
â”‚             â”‚
â”‚             â”œâ”€ Step 3: Apply to DataFrame
â”‚             â”‚  â””â”€ `df.withColumn("prediction", predict(col("features")))`
â”‚             â”‚
â”‚             â””â”€ Performance Tips:
â”‚                â”œâ”€ Use SCALAR_ITER for large models
â”‚                â”œâ”€ Batch predictions for GPU efficiency
â”‚                â”œâ”€ Repartition data for balanced load
â”‚                â””â”€ Monitor executor memory usage

```

### ğŸ’¾ Data Sink Decision Tree

```
Where to write results?
â”‚
â”œâ”€â”€â”€ Development/Testing?
â”‚    â”œâ”€ Console â†’ `.writeStream.format("console").start()`
â”‚    â””â”€ Memory â†’ `.writeStream.format("memory").queryName("table").start()`
â”‚
â”œâ”€â”€â”€ Production Storage?
â”‚    â”œâ”€ File System (HDFS/S3)?
â”‚    â”‚  â”œâ”€ Parquet â†’ `.format("parquet")` (recommended)
â”‚    â”‚  â”œâ”€ JSON â†’ `.format("json")`
â”‚    â”‚  â”œâ”€ CSV â†’ `.format("csv")`
â”‚    â”‚  â””â”€ Delta Lake â†’ `.format("delta")` (ACID transactions)
â”‚    â”‚
â”‚    â”œâ”€ Message Queue?
â”‚    â”‚  â””â”€ Kafka â†’ `.format("kafka").option("topic", "output")`
â”‚    â”‚
â”‚    â”œâ”€ Database?
â”‚    â”‚  â”œâ”€ JDBC â†’ Use `.foreachBatch()` with JDBC write
â”‚    â”‚  â”œâ”€ NoSQL â†’ Use `.foreach()` with custom writer
â”‚    â”‚  â””â”€ UDF in DB â†’ Create table, then use SQL UDF for inference
â”‚    â”‚
â”‚    â””â”€ Custom Logic?
â”‚       â”œâ”€ `.foreach()` â†’ Row-by-row (slow)
â”‚       â””â”€ `.foreachBatch()` â†’ Batch processing (recommended)
â”‚          â””â”€ def write_batch(df, epoch_id):
â”‚              # Custom write logic (JDBC, API calls, etc.)
â”‚              df.write.jdbc(...)

```

### âš¡ Quick Reference: When to Use What

| Scenario | Tool/Pattern | Why |
|----------|-------------|-----|
| **Data < 10GB** | Pandas | Faster for small data, simpler API |
| **Data 10GB - 1TB** | PySpark | Distributed processing, handles OOM |
| **Data > 1TB** | PySpark + Parquet | Columnar storage, predicate pushdown |
| **ML Inference in PySpark** | Pandas UDF + Broadcasting | Vectorized, efficient model reuse |
| **ML Inference in PostgreSQL** | PL/Python UDF | In-database processing, no data export |
| **ML Inference in BigQuery** | Remote Functions | Serverless, scalable to petabytes |
| **Real-time Streaming** | PySpark Streaming + Kafka | Micro-batch, exactly-once semantics |
| **Sub-second Latency** | Flink or Kafka Streams | True real-time, event-by-event |
| **Complex Aggregations** | Window functions + Watermark | Handle late data, state management |
| **Large Model Inference** | SCALAR_ITER Pandas UDF | Batch processing, GPU support |
| **Join Streams** | Stream-Stream Join + Watermark | Required for bounded state |
| **Deduplication** | `.dropDuplicates()` + Watermark | Stateful, memory-efficient |
| **Checkpointing** | HDFS/S3 location | Fault tolerance, exactly-once |
| **Testing Streaming** | Memory sink + rate source | Fast iteration, no external deps |

---

## ğŸ“Š Visual Decision Flow Diagrams

### Main Decision Flow

```mermaid
flowchart TD
    Start([Need to Process Data?]) --> DataSize{Data Size?}
    
    DataSize -->|< 10GB| Pandas[Use Pandas]
    DataSize -->|> 10GB| PySpark[Use PySpark]
    
    Pandas --> PandasML{Need ML?}
    PandasML -->|Yes| PandasTorch[Pandas + PyTorch/sklearn]
    PandasML -->|No| PandasSQL{Need SQL?}
    PandasSQL -->|Yes| SQLite[Pandas + SQLite]
    PandasSQL -->|No| PandasDone[Done]
    
    PySpark --> ProcessMode{Processing Mode?}
    
    ProcessMode -->|Batch| BatchAPI[PySpark DataFrame API]
    ProcessMode -->|Stream| StreamAPI[PySpark Structured Streaming]
    
    BatchAPI --> BatchML{Need ML?}
    BatchML -->|Yes| UDFInference[Use Pandas UDF + Broadcasting]
    BatchML -->|No| BatchDone[Done]
    
    StreamAPI --> StreamSource{Data Source?}
    StreamSource -->|Kafka| KafkaConfig[Configure Kafka Source]
    StreamSource -->|Files| FileConfig[Configure File Source]
    StreamSource -->|Socket| SocketConfig[Configure Socket Source]
    
    KafkaConfig --> StreamML{Need ML?}
    FileConfig --> StreamML
    SocketConfig --> StreamML
    
    StreamML -->|Yes| StreamUDF[Use UDF in Stream Query]
    StreamML -->|No| StreamSink{Output Sink?}
    
    StreamUDF --> StreamSink
    StreamSink -->|Kafka| KafkaSink[Write to Kafka]
    StreamSink -->|Files| FileSink[Write to Files]
    StreamSink -->|Database| DBSink[Write to Database]
    StreamSink -->|Console| ConsoleSink[Console Output]
    
    UDFInference --> Checkpoint1{Need Fault Tolerance?}
    Checkpoint1 -->|Yes| EnableCheckpoint[Enable Checkpointing]
    Checkpoint1 -->|No| BatchDone
    
    KafkaSink --> Checkpoint2{Need Exactly-Once?}
    FileSink --> Checkpoint2
    DBSink --> Checkpoint2
    ConsoleSink --> StreamDone[Done]
    
    Checkpoint2 -->|Yes| ExactlyOnce[Enable Checkpointing + Idempotent Writes]
    Checkpoint2 -->|No| StreamDone
    
    ExactlyOnce --> StreamDone
    EnableCheckpoint --> BatchDone
    
    style Start fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style Pandas fill:#2d3748,stroke:#48bb78,stroke-width:2px,color:#fff
    style PySpark fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
    style UDFInference fill:#2d3748,stroke:#ed8936,stroke-width:2px,color:#fff
    style StreamUDF fill:#2d3748,stroke:#ed8936,stroke-width:2px,color:#fff
    style BatchAPI fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
    style StreamAPI fill:#2d3748,stroke:#9f7aea,stroke-width:2px,color:#fff
    style ExactlyOnce fill:#2d3748,stroke:#48bb78,stroke-width:2px,color:#fff
```

### PySpark Streaming Architecture Flow

```mermaid
flowchart LR
    subgraph Sources["ğŸ“¥ Data Sources"]
        Kafka[Kafka Topics]
        Files[File System]
        Socket[Socket Stream]
        Rate[Rate Source]
    end
    
    subgraph Processing["âš™ï¸ Stream Processing"]
        Read[readStream API]
        Transform[Transformations]
        UDF[Pandas UDF<br/>ML Inference]
        Window[Window Functions]
        Watermark[Watermarking]
        State[State Management]
    end
    
    subgraph Sinks["ğŸ“¤ Output Sinks"]
        KafkaOut[Kafka Topics]
        FilesOut[Parquet/JSON/CSV]
        DB[(Database)]
        Console[Console Debug]
        Memory[Memory Table]
    end
    
    subgraph FaultTolerance["ğŸ›¡ï¸ Fault Tolerance"]
        Checkpoint[Checkpointing]
        WAL[Write-Ahead Log]
        Idempotent[Idempotent Writes]
    end
    
    Kafka --> Read
    Files --> Read
    Socket --> Read
    Rate --> Read
    
    Read --> Transform
    Transform --> UDF
    Transform --> Window
    Window --> Watermark
    Watermark --> State
    UDF --> State
    
    State --> KafkaOut
    State --> FilesOut
    State --> DB
    State --> Console
    State --> Memory
    
    KafkaOut --> Checkpoint
    FilesOut --> Checkpoint
    DB --> Checkpoint
    
    Checkpoint --> WAL
    WAL --> Idempotent
    
    style Kafka fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
    style Files fill:#2d3748,stroke:#48bb78,stroke-width:2px,color:#fff
    style Socket fill:#2d3748,stroke:#9f7aea,stroke-width:2px,color:#fff
    style UDF fill:#2d3748,stroke:#ed8936,stroke-width:2px,color:#fff
    style Checkpoint fill:#2d3748,stroke:#f56565,stroke-width:2px,color:#fff
    style DB fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
```

### UDF Inference Decision Flow

```mermaid
flowchart TD
    Start([Need ML Inference?]) --> Deploy{Deployment Strategy?}
    
    Deploy -->|In-Database| DBInfer[UDF for Inference]
    Deploy -->|Batch Scoring| BatchScore[mapPartitions]
    Deploy -->|Real-time Stream| StreamInfer[UDF in Stream Query]
    
    DBInfer --> Platform{Platform?}
    Platform -->|PySpark| PySparkUDF[Pandas UDF + Broadcasting]
    Platform -->|PostgreSQL| PostgresUDF[PL/Python UDF]
    Platform -->|BigQuery| BQRemote[Remote Functions]
    Platform -->|Snowflake| SnowflakeUDF[Python UDF]
    
    PySparkUDF --> Broadcast[1. Broadcast Model]
    Broadcast --> CreateUDF[2. Create Pandas UDF]
    CreateUDF --> Apply[3. Apply to DataFrame]
    Apply --> Optimize{Optimize?}
    
    Optimize -->|Large Model| ScalarIter[Use SCALAR_ITER]
    Optimize -->|GPU| GPUBatch[Batch + .cuda]
    Optimize -->|Load Balance| Repartition[Repartition Data]
    
    PostgresUDF --> PLPython[Define PL/Python Function]
    PLPython --> SQLQuery[Use in SQL Query]
    
    BQRemote --> CloudFunc[Deploy Cloud Function]
    CloudFunc --> BQConnect[Create Remote Function]
    BQConnect --> SQLQuery
    
    SnowflakeUDF --> SnowPython[Define Python UDF]
    SnowPython --> SnowPackages[Specify Packages]
    SnowPackages --> SQLQuery
    
    BatchScore --> LoadModel[Load Model per Partition]
    LoadModel --> ProcessBatch[Process Entire Partition]
    
    StreamInfer --> StreamBroadcast[Broadcast Model]
    StreamBroadcast --> StreamUDF[Apply UDF in Stream]
    StreamUDF --> Monitor[Monitor Lag & Throughput]
    
    SQLQuery --> Done[Done]
    ScalarIter --> Done
    GPUBatch --> Done
    Repartition --> Done
    ProcessBatch --> Done
    Monitor --> Done
    
    style Start fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style DBInfer fill:#2d3748,stroke:#ed8936,stroke-width:2px,color:#fff
    style PySparkUDF fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
    style Broadcast fill:#2d3748,stroke:#48bb78,stroke-width:2px,color:#fff
    style CloudFunc fill:#2d3748,stroke:#9f7aea,stroke-width:2px,color:#fff
    style Monitor fill:#2d3748,stroke:#f56565,stroke-width:2px,color:#fff
```

### Streaming Trigger & Checkpoint Configuration

```mermaid
flowchart TD
    Start([Configure Streaming Query]) --> Trigger{Choose Trigger Mode}
    
    Trigger -->|ProcessingTime| PT[trigger processingTime='10s']
    Trigger -->|Once| Once[trigger once=True]
    Trigger -->|Continuous| Cont[trigger continuous='1s']
    Trigger -->|AvailableNow| AN[trigger availableNow=True]
    
    PT --> Checkpoint[Configure Checkpointing]
    Once --> Checkpoint
    Cont --> Checkpoint
    AN --> Checkpoint
    
    Checkpoint --> Location{Checkpoint Location?}
    Location -->|HDFS| HDFS[checkpointLocation: hdfs://]
    Location -->|S3| S3[checkpointLocation: s3://]
    Location -->|Local| Local[checkpointLocation: file://]
    
    HDFS --> State{State Management?}
    S3 --> State
    Local --> State
    
    State -->|Stateless| Stateless[Direct Transformations]
    State -->|Stateful| Stateful[Aggregations/Joins]
    
    Stateful --> Watermark[Configure Watermark]
    Watermark --> WMTime[withWatermark event_time 10 minutes]
    
    WMTime --> OutputMode{Output Mode?}
    Stateless --> OutputMode
    
    OutputMode -->|Append| Append[Append: New Rows Only]
    OutputMode -->|Complete| Complete[Complete: Full Table]
    OutputMode -->|Update| Update[Update: Changed Rows]
    
    Append --> Sink[Configure Output Sink]
    Complete --> Sink
    Update --> Sink
    
    Sink --> Monitor[Monitor & Scale]
    Monitor --> Metrics{Check Metrics}
    
    Metrics -->|High Lag| Scale[Scale Executors]
    Metrics -->|Memory Issues| StateSize[Check State Size]
    Metrics -->|Low Throughput| Tune[Tune Batch Size]
    
    Scale --> Done[Start Streaming Query]
    StateSize --> Done
    Tune --> Done
    
    style Start fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style Checkpoint fill:#2d3748,stroke:#ed8936,stroke-width:2px,color:#fff
    style Watermark fill:#2d3748,stroke:#4299e1,stroke-width:2px,color:#fff
    style Monitor fill:#2d3748,stroke:#48bb78,stroke-width:2px,color:#fff
    style Done fill:#2d3748,stroke:#9f7aea,stroke-width:2px,color:#fff
```

---

## ğŸ”§ Configuration Cheat Sheet

### PySpark Streaming Minimal Setup

```python
# 1. Read from Kafka
stream_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "input-topic") \
    .load()

# 2. Transform (with ML inference)
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType

# Broadcast model
broadcast_model = spark.sparkContext.broadcast(trained_model)

@pandas_udf(FloatType())
def predict_udf(features: pd.Series) -> pd.Series:
    model = broadcast_model.value
    return pd.Series(model.predict(features.values.reshape(-1, 1)))

# Apply transformations
result_df = stream_df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*") \
    .withColumn("prediction", predict_udf(col("features")))

# 3. Write to Kafka with checkpointing
query = result_df \
    .selectExpr("CAST(id AS STRING) AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output-topic") \
    .option("checkpointLocation", "/path/to/checkpoint") \
    .trigger(processingTime='10 seconds') \
    .start()

query.awaitTermination()
```

### Watermarking Example

```python
# Handle late data with watermarking
windowed_df = stream_df \
    .withWatermark("event_time", "10 minutes") \
    .groupBy(
        window(col("event_time"), "5 minutes"),
        col("device_id")
    ) \
    .agg(
        avg("temperature").alias("avg_temp"),
        max("temperature").alias("max_temp"),
        count("*").alias("record_count")
    )
```

### Stream-Stream Join with Watermark

```python
# Join two streams with watermarks
stream1 = stream1_df.withWatermark("timestamp", "10 minutes")
stream2 = stream2_df.withWatermark("timestamp", "15 minutes")

joined = stream1.join(
    stream2,
    expr("""
        stream1.id = stream2.id AND
        stream1.timestamp >= stream2.timestamp AND
        stream1.timestamp <= stream2.timestamp + interval 5 minutes
    """)
)
```

---

## ğŸ“ˆ Performance Comparison

| Operation | Standard UDF | Pandas UDF | Pandas UDF + Broadcasting | Built-in Functions |
|-----------|-------------|------------|--------------------------|-------------------|
| **Throughput** | 1x (baseline) | 10-50x | 50-100x | 100-1000x |
| **Serialization** | Row-by-row | Columnar (Arrow) | Columnar (Arrow) | Native |
| **Model Loading** | Per row | Per partition | Once per executor | N/A |
| **Best For** | Simple logic | Vectorized ops | ML inference | SQL operations |
| **GPU Support** | âŒ | âœ… | âœ… | âŒ |

### Streaming Latency Targets

| Trigger Mode | Latency | Use Case |
|--------------|---------|----------|
| **ProcessingTime='1s'** | ~1-2 seconds | Near real-time dashboards |
| **ProcessingTime='10s'** | ~10-15 seconds | Standard streaming ETL |
| **ProcessingTime='1m'** | ~1-2 minutes | Aggregated analytics |
| **Once** | Batch-like | Catch-up processing |
| **Continuous** | < 1 second | Experimental, low latency |

---

## ğŸ¯ Best Practices Summary

### âœ… DO

- **Use Pandas UDF** for ML inference (50-100x faster than standard UDF)
- **Broadcast models** to executors (critical for performance)
- **Enable checkpointing** for streaming (fault tolerance, exactly-once)
- **Use watermarking** for stateful operations (prevents unbounded state)
- **Repartition data** for load balancing
- **Monitor lag** in streaming queries
- **Use Parquet** for storage (columnar, compressed)
- **Filter early** with predicate pushdown
- **Test with memory sink** before production

### âŒ DON'T

- **Don't use standard UDF** for ML inference (too slow)
- **Don't load model per row** (use broadcasting)
- **Don't skip checkpointing** in production streaming
- **Don't ignore late data** (configure watermarks)
- **Don't use CSV** for big data (use Parquet)
- **Don't forget to `.stop()`** streaming queries
- **Don't over-partition** (more partitions = more overhead)
- **Don't cache everything** (monitor memory usage)