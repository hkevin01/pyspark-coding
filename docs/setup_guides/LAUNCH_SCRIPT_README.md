# ğŸš€ Launch Script Quick Reference

## What It Does

The `launch_monitoring.sh` script is a **one-command solution** that:

1. âœ… **Checks prerequisites** (Docker, npm, Node.js, Chrome)
2. âœ… **Verifies file structure** (all required files exist)
3. âœ… **Checks port availability** (detects conflicts)
4. âœ… **Installs npm dependencies** (if not already installed)
5. âœ… **Starts Docker services** (Spark cluster + Prometheus + Dashboard)
6. âœ… **Waits for services** (ensures everything is ready)
7. âœ… **Verifies health** (checks each service status)
8. âœ… **Opens Chrome** (launches dashboard at localhost:3000)
9. âœ… **Shows live logs** (streams Docker logs)

---

## Usage

### Quick Start (One Command)
```bash
cd /home/kevin/Projects/pyspark-coding
./launch_monitoring.sh
```

That's it! The script handles everything automatically.

---

## What You'll See

### Step-by-Step Output
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ PySpark Monitoring Dashboard Launch Script
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Step 1: Checking Prerequisites
âœ… Docker is installed
âœ… Docker Compose is installed
âœ… npm is installed (10.2.3)
âœ… Node.js is installed (v20.10.0)
âœ… Chrome/Chromium is installed

ğŸ“ Step 2: Verifying File Structure
âœ… Found: docker-compose.monitoring.yml
âœ… Found: package.json
âœ… Found: next.config.js
âœ… Found: index.tsx
âœ… Found: MONITORING_SETUP_GUIDE.md

ğŸ”Œ Step 3: Checking Port Availability
âœ… Port 3000 (Dashboard) is available
âœ… Port 9080 (Spark Master) is available
âœ… Port 9090 (Prometheus) is available
... (checking all ports)

ğŸ“¦ Step 4: Installing npm Dependencies
â„¹ï¸  node_modules already exists, checking for updates...
âœ… Dependencies are up to date

ï¿½ï¿½ Step 5: Starting Docker Services
â–¶ Starting Spark cluster, Prometheus, and monitoring dashboard...
âœ… Docker services started

â³ Step 6: Waiting for Services to Initialize
â–¶ Waiting for Spark Master to be ready...
âœ… Spark Master is ready!
â–¶ Waiting for Prometheus to be ready...
âœ… Prometheus is ready!
â–¶ Waiting for Monitoring Dashboard to be ready...
âœ… Monitoring Dashboard is ready!

âœ… Step 7: Verifying Services
â–¶ Checking Spark Master...
âœ… Spark Master is running
â–¶ Checking Prometheus...
âœ… Prometheus is healthy
â–¶ Checking Workers...
â„¹ï¸  Found 3 worker(s) running

ğŸŒ Step 8: Service URLs
Dashboard:        http://localhost:3000
Spark Master UI:  http://localhost:9080
Prometheus:       http://localhost:9090
Worker 1 UI:      http://localhost:8081
Worker 2 UI:      http://localhost:8082
Worker 3 UI:      http://localhost:8083
App UI:           http://localhost:4040 (when job running)

ğŸŒ Step 9: Launching Dashboard in Chrome
â–¶ Opening http://localhost:3000 in Chrome...
âœ… Dashboard opened in Chrome

ğŸ’¡ Quick Tips
Dashboard Features:
  â€¢ Toggle auto-refresh in the header
  â€¢ Choose refresh interval: 2s, 5s, 10s, or 30s
  â€¢ View cluster overview with 4 metric cards
  â€¢ Monitor workers in real-time
  â€¢ Track active applications
  â€¢ Visualize performance trends

Test the Dashboard:
  docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/long_running_demo.py

ğŸ‰ SUCCESS! Monitoring Dashboard is Ready!
```

---

## Troubleshooting

### Port Conflicts
If ports are in use, the script will automatically attempt cleanup:
```bash
âš ï¸  Port 3000 (Dashboard) is already in use
âš ï¸  Some ports are in use. Attempting to clean up...
```

### Missing Dependencies
If prerequisites are missing:
```bash
âŒ Docker is not installed
âŒ Please install missing dependencies and try again
```

Install missing tools:
```bash
# Ubuntu/Debian
sudo apt-get install docker.io docker-compose nodejs npm

# macOS
brew install docker docker-compose node
```

### Services Not Starting
If a service fails to start within 60 seconds (30 attempts Ã— 2s):
```bash
âŒ Spark Master failed to start after 30 attempts
```

Check logs:
```bash
docker-compose -f docker-compose.monitoring.yml logs spark-master
```

---

## Manual Control

### Stop All Services
```bash
cd /home/kevin/Projects/pyspark-coding
docker-compose -f docker-compose.monitoring.yml down
```

### Restart Services
```bash
docker-compose -f docker-compose.monitoring.yml restart
```

### View Logs
```bash
# All services
docker-compose -f docker-compose.monitoring.yml logs -f

# Specific service
docker-compose -f docker-compose.monitoring.yml logs -f monitoring-dashboard
```

### Check Service Status
```bash
docker-compose -f docker-compose.monitoring.yml ps
```

---

## Script Features

### âœ… Color-Coded Output
- ğŸŸ¢ **Green**: Success messages
- ğŸ”´ **Red**: Error messages
- ğŸŸ¡ **Yellow**: Warnings
- ğŸ”µ **Blue**: Info messages
- ğŸŸ£ **Purple**: Step indicators
- ğŸ”· **Cyan**: Headers

### âœ… Automatic Port Detection
Checks all required ports before starting:
- 3000: Dashboard
- 9080: Spark Master
- 9090: Prometheus
- 8081-8083: Workers
- 4040: Application UI
- 7077: Spark cluster

### âœ… Health Checks
Verifies each service is responding before proceeding:
- HTTP health checks with retries
- 30 attempts Ã— 2 seconds = 60 second timeout
- Graceful failure with clear error messages

### âœ… Browser Auto-Launch
Detects available browsers in order:
1. `google-chrome`
2. `google-chrome-stable`
3. `chromium-browser`
4. `chromium`
5. `xdg-open` (fallback)

---

## Testing the Dashboard

After the script launches, test with a sample job:

```bash
# Submit a long-running demo job
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/long_running_demo.py

# Watch the dashboard update in real-time:
# âœ… Active apps counter increases
# âœ… CPU/Memory usage increases
# âœ… Job appears in "Live Applications"
# âœ… Charts update with new data points
```

---

## What Happens Behind the Scenes

1. **Prerequisite Check**: Validates all tools are installed
2. **File Verification**: Ensures project structure is complete
3. **Port Check**: Detects conflicts and attempts cleanup
4. **Dependency Install**: Runs `npm install` if needed
5. **Docker Compose Up**: Starts all 6 services in detached mode
6. **Service Wait**: Polls each service until healthy
7. **Health Verification**: Confirms Spark Master and Prometheus
8. **Worker Count**: Checks number of active workers
9. **Browser Launch**: Opens Chrome to localhost:3000
10. **Log Streaming**: Shows live Docker logs (Ctrl+C to exit)

---

## Quick Commands Reference

```bash
# Launch everything (recommended)
./launch_monitoring.sh

# Stop everything
docker-compose -f docker-compose.monitoring.yml down

# Restart without full check
docker-compose -f docker-compose.monitoring.yml restart

# View specific service logs
docker-compose -f docker-compose.monitoring.yml logs -f spark-master
docker-compose -f docker-compose.monitoring.yml logs -f prometheus
docker-compose -f docker-compose.monitoring.yml logs -f monitoring-dashboard

# Check service health
curl http://localhost:9080  # Spark Master
curl http://localhost:9090/-/healthy  # Prometheus
curl http://localhost:3000  # Dashboard

# Run demo job
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/long_running_demo.py
```

---

## File Locations

- **Launch Script**: `/home/kevin/Projects/pyspark-coding/launch_monitoring.sh`
- **Docker Compose**: `/home/kevin/Projects/pyspark-coding/docker-compose.monitoring.yml`
- **Frontend Code**: `/home/kevin/Projects/pyspark-coding/src/monitoring_frontend/`
- **Setup Guide**: `/home/kevin/Projects/pyspark-coding/MONITORING_SETUP_GUIDE.md`

---

## Success Indicators

When everything works correctly, you should see:

âœ… All prerequisite checks pass
âœ… All required files found
âœ… All ports available (or conflicts resolved)
âœ… npm dependencies installed
âœ… 6 Docker containers running
âœ… All services respond to health checks
âœ… 3 workers detected
âœ… Chrome opens to dashboard
âœ… Dashboard shows cluster metrics

---

## Support

For detailed documentation, see:
- `MONITORING_SETUP_GUIDE.md` - Complete setup guide
- `src/monitoring_frontend/README.md` - Dashboard documentation
- `docker/spark-cluster/README.md` - Cluster setup

For issues, check:
1. Docker is running: `docker ps`
2. Ports are free: `lsof -i :3000`
3. Services are healthy: `docker-compose ps`
4. Logs for errors: `docker-compose logs`

---

**ğŸ‰ Enjoy your automated monitoring solution!**
